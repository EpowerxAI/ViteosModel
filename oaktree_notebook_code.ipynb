{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START \n",
      "\tModule : ViteosMongoDB \n",
      "\tFunction Class : ViteosMongoDB_Class \n",
      "\tFunction Name : __init__\n",
      "STOP \n",
      "\tModule : ViteosMongoDB \n",
      "\tFunction Class : ViteosMongoDB_Class \n",
      "\tFunction Name : __init__\n",
      "Time Taken : 0.01 sec\n",
      "START \n",
      "\tModule : ViteosMongoDB \n",
      "\tFunction Class : ViteosMongoDB_Class \n",
      "\tFunction Name : connect_with_or_without_ssh\n",
      "START \n",
      "\tModule : ViteosMongoDB \n",
      "\tFunction Class : ViteosMongoDB_Class \n",
      "\tFunction Name : connect_without_ssh\n",
      "Connecting to 10.1.15.137\n",
      "\n",
      "\n",
      "Mongo Client without ssh created\n",
      "\n",
      "\n",
      "\tDatabases present in server 10.1.15.137\n",
      "\t\t\n",
      "09_12_2020\n",
      "\t\t15_10_Backup\n",
      "\t\tBACKUP_SOROS_22_09\n",
      "\t\tBACKUP_SOROS_2309\n",
      "\t\tBackUp_Soros_22BD_Latest\n",
      "\t\tReconDB_ML\n",
      "\t\tReconDB_ML_Scheduler\n",
      "\t\tReconDB_ML_Test\n",
      "\t\tReconDB_PROD_Parallel\n",
      "\t\tReconDB_PROD_Soros_Parallel\n",
      "\t\tReconDB_Soros_ML\n",
      "\t\tReconDB_Soros_ML_Test\n",
      "\t\tTestDB\n",
      "\t\tadmin\n",
      "\t\tconfig\n",
      "\t\thh\n",
      "\t\tlocal\n",
      "\t\tt1\n",
      "\t\ttest\n",
      "STOP \n",
      "\tModule : ViteosMongoDB \n",
      "\tFunction Class : ViteosMongoDB_Class \n",
      "\tFunction Name : connect_without_ssh\n",
      "Time Taken : 0.04 sec\n",
      "STOP \n",
      "\tModule : ViteosMongoDB \n",
      "\tFunction Class : ViteosMongoDB_Class \n",
      "\tFunction Name : connect_with_or_without_ssh\n",
      "Time Taken : 0.04 sec\n",
      "866\n",
      "meo size\n",
      "844\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Sep  5 20:36:16 2020\n",
    "\n",
    "@author: consultant138\n",
    "\"\"\"\n",
    "import os\n",
    "os.chdir('D:\\\\ViteosModel')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import datetime as dt\n",
    "import sys\n",
    "from ViteosMongoDB import  ViteosMongoDB_Class as mngdb\n",
    "from datetime import datetime,date,timedelta\n",
    "from pandas.io.json import json_normalize\n",
    "import dateutil.parser\n",
    "from difflib import SequenceMatcher\n",
    "import pprint\n",
    "import json\n",
    "from pandas import merge\n",
    "import re\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import glob\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from dateutil.parser import parse\n",
    "import operator\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "import random\n",
    "import decimal\n",
    "\n",
    "cols = ['Currency','Account Type','Accounting Net Amount',\n",
    "#'Accounting Net Amount Difference','Accounting Net Amount Difference Absolute ',\n",
    "'Task ID', 'Source Combination Code',\n",
    "'Activity Code','Age','Age WK',\n",
    "'Asset Type Category','Base Currency','Base Net Amount','Bloomberg_Yellow_Key',\n",
    "'B-P Net Amount',\n",
    "#'B-P Net Amount Difference','B-P Net Amount Difference Absolute',\n",
    "'BreakID',\n",
    "'Business Date','Cancel Amount','Cancel Flag','CUSIP','Custodian',\n",
    "'Custodian Account',\n",
    "'Derived Source','Description','Department','ExpiryDate','ExternalComment1','ExternalComment2',\n",
    "'ExternalComment3','Fund','FX Rate','Interest Amount','InternalComment1','InternalComment2',\n",
    "'InternalComment3','Investment Type','Is Combined Data','ISIN','Keys',\n",
    "'Mapped Custodian Account','Net Amount Difference','Net Amount Difference Absolute','Non Trade Description',\n",
    "'OTE Custodian Account',\n",
    "#'Predicted Action','Predicted Status','Prediction Details',\n",
    "'Price','Prime Broker',\n",
    "'Quantity','SEDOL','Settle Date','SPM ID','Status','Strike Price',\n",
    "'System Comments','Ticker','Trade Date','Trade Expenses','Transaction Category','Transaction ID','Transaction Type',\n",
    "'Underlying Cusip','Underlying Investment ID','Underlying ISIN','Underlying Sedol','Underlying Ticker','Source Combination','_ID']\n",
    "#'UnMapped']\n",
    "\n",
    "add = ['ViewData.Side0_UniqueIds', 'ViewData.Side1_UniqueIds',\n",
    "      # 'MetaData.0._RecordID','MetaData.1._RecordID',\n",
    "       'ViewData.Task Business Date']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_cols = ['ViewData.' + x for x in cols] + add\n",
    "\n",
    "common_cols = ['ViewData.Accounting Net Amount', 'ViewData.Age',\n",
    "'ViewData.Age WK', 'ViewData.Asset Type Category',\n",
    "'ViewData.B-P Net Amount', 'ViewData.Base Net Amount','ViewData.CUSIP', \n",
    " 'ViewData.Cancel Amount',\n",
    "       'ViewData.Cancel Flag',\n",
    "#'ViewData.Commission',\n",
    "        'ViewData.Currency', 'ViewData.Custodian',\n",
    "       'ViewData.Custodian Account',\n",
    "       'ViewData.Description','ViewData.Department', 'ViewData.ExpiryDate', 'ViewData.Fund',\n",
    "       'ViewData.ISIN',\n",
    "       'ViewData.Investment Type',\n",
    "      # 'ViewData.Keys',\n",
    "       'ViewData.Mapped Custodian Account',\n",
    "       'ViewData.Net Amount Difference',\n",
    "       'ViewData.Net Amount Difference Absolute',\n",
    "        #'ViewData.OTE Ticker',\n",
    "        'ViewData.Price',\n",
    "       'ViewData.Prime Broker', 'ViewData.Quantity',\n",
    "       'ViewData.SEDOL', 'ViewData.SPM ID', 'ViewData.Settle Date',\n",
    "       \n",
    "  #  'ViewData.Strike Price',\n",
    "               'Date',\n",
    "       'ViewData.Ticker', 'ViewData.Trade Date',\n",
    "       'ViewData.Transaction Category',\n",
    "       'ViewData.Transaction Type', 'ViewData.Underlying Cusip',\n",
    "       'ViewData.Underlying ISIN',\n",
    "       'ViewData.Underlying Sedol','filter_key','ViewData.Status','ViewData.BreakID',\n",
    "              'ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData._ID']\n",
    "\n",
    "model_cols = [\n",
    "            'SideA.ViewData.B-P Net Amount', \n",
    "              #'SideA.ViewData.Cancel Flag', \n",
    "              #'SideA.new_desc_cat',\n",
    "             # 'SideA.ViewData.Description',\n",
    "             # 'SideA.ViewData.Department',\n",
    "   \n",
    "    \n",
    "              \n",
    "             # 'SideA.ViewData.Price',\n",
    "             # 'SideA.ViewData.Quantity',\n",
    "             #'SideA.ViewData.Investment Type', \n",
    "              #'SideA.ViewData.Asset Type Category', \n",
    "              'SideB.ViewData.Accounting Net Amount', \n",
    "              #'SideB.ViewData.Cancel Flag', \n",
    "             # 'SideB.ViewData.Description',\n",
    "              # 'SideB.ViewData.Department',\n",
    "              \n",
    "             # 'SideB.ViewData.Price',\n",
    "             # 'SideB.ViewData.Quantity',\n",
    "             # 'SideB.new_desc_cat',\n",
    "             # 'SideB.ViewData.Investment Type', \n",
    "              #'SideB.ViewData.Asset Type Category', \n",
    "              'Trade_Date_match', 'Settle_Date_match', \n",
    "                'Amount_diff_2', \n",
    "              'Trade_date_diff', 'Settle_date_diff', 'SideA.ISIN_NA', 'SideB.ISIN_NA', \n",
    "             # 'ViewData.Combined Fund',\n",
    "              'ViewData.Combined Transaction Type', 'Combined_Desc','Combined_TType',\n",
    "             # 'SideA.TType', 'SideB.TType', \n",
    "              'abs_amount_flag',\n",
    "    'tt_map_flag', \n",
    "              'All_key_nan','new_key_match', 'new_pb1',\n",
    "              'SideB.Date','SideA.ViewData.Settle Date','SideB.ViewData.Settle Date',\n",
    "            'SideA.ViewData._ID', 'SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds', 'SideA.ViewData.Side1_UniqueIds',\n",
    "              'SideB.ViewData.Status', 'SideB.ViewData.BreakID_B_side',\n",
    "              'SideA.ViewData.Status', 'SideA.ViewData.BreakID_A_side'] \n",
    "              #'label']\n",
    "\n",
    "model_cols_2 =[#'SideA.ViewData.B-P Net Amount', \n",
    "              #'SideA.ViewData.Cancel Flag', \n",
    "              #'SideA.new_desc_cat',\n",
    "             # 'SideA.ViewData.Description',\n",
    "             # 'SideA.ViewData.Department',\n",
    "   \n",
    "    \n",
    "              \n",
    "             # 'SideA.ViewData.Price',\n",
    "             # 'SideA.ViewData.Quantity',\n",
    "             #'SideA.ViewData.Investment Type', \n",
    "              #'SideA.ViewData.Asset Type Category', \n",
    "              #'SideB.ViewData.Accounting Net Amount', \n",
    "              #'SideB.ViewData.Cancel Flag', \n",
    "             # 'SideB.ViewData.Description',\n",
    "              # 'SideB.ViewData.Department',\n",
    "              \n",
    "             # 'SideB.ViewData.Price',\n",
    "             # 'SideB.ViewData.Quantity',\n",
    "             # 'SideB.new_desc_cat',\n",
    "             # 'SideB.ViewData.Investment Type', \n",
    "              #'SideB.ViewData.Asset Type Category', \n",
    "              'Trade_Date_match', 'Settle_Date_match', \n",
    "              #  'Amount_diff_2', \n",
    "              'Trade_date_diff', 'Settle_date_diff', 'SideA.ISIN_NA', 'SideB.ISIN_NA', \n",
    "             # 'ViewData.Combined Fund',\n",
    "              'ViewData.Combined Transaction Type', 'Combined_Desc','Combined_TType',\n",
    "             # 'SideA.TType', 'SideB.TType', \n",
    "              'abs_amount_flag',\n",
    "    'tt_map_flag', \n",
    "              'All_key_nan','new_key_match', 'new_pb1',\n",
    "              'SideB.Date','SideA.ViewData.Settle Date','SideB.ViewData.Settle Date',\n",
    "            'SideA.ViewData._ID', 'SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds', 'SideA.ViewData.Side1_UniqueIds',\n",
    "              'SideB.ViewData.Status', 'SideB.ViewData.BreakID_B_side',\n",
    "              'SideA.ViewData.Status', 'SideA.ViewData.BreakID_A_side'] \n",
    "              #'label']\n",
    "\n",
    "#### Closed break functions - Begin #### \n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def dictionary_exclude_keys(fun_dict, fun_keys_to_exclude):\n",
    "    return {x: fun_dict[x] for x in fun_dict if x not in fun_keys_to_exclude}\n",
    "\n",
    "def write_dict_at_top(fun_filename, fun_dict_to_add):\n",
    "    with open(fun_filename, 'r+') as f:\n",
    "        fun_existing_content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(json.dumps(fun_dict_to_add, indent = 4))\n",
    "        f.write('\\n')\n",
    "        f.write(fun_existing_content)\n",
    "\n",
    "def normalize_bp_acct_col_names(fun_df):\n",
    "    bp_acct_col_names_mapping_dict = {\n",
    "                                      'ViewData.Cust Net Amount' : 'ViewData.B-P Net Amount',\n",
    "                                      'ViewData.Cust Net Amount Difference' : 'ViewData.B-P Net Amount Difference',\n",
    "                                      'ViewData.Cust Net Amount Difference Absolute' : 'ViewData.B-P Net Amount Difference Absolute',\n",
    "                                      'ViewData.CP Net Amount' : 'ViewData.B-P Net Amount',\n",
    "                                      'ViewData.CP Net Amount Difference' : 'ViewData.B-P Net Amount Difference',\n",
    "                                      'ViewData.CP Net Amount Difference Absolute' : 'ViewData.B-P Net Amount Difference Absolute',\n",
    "                                      'ViewData.PMSVendor Net Amount' : 'ViewData.Accounting Net Amount'\n",
    "                                        }\n",
    "    fun_df.rename(columns = bp_acct_col_names_mapping_dict, inplace = True)\n",
    "    return(fun_df)\n",
    "\n",
    "\n",
    "\n",
    "# M X M and N X N architecture for closed break prediction\n",
    "def closed_cols():\n",
    "    cols_for_closed_list = ['Status','Source Combination','Mapped Custodian Account',\n",
    "                   'Accounting Currency','B-P Currency', \n",
    "                   'Transaction ID','Transaction Type','Description','Investment ID',\n",
    "                   'Accounting Net Amount','B-P Net Amount', \n",
    "                   'InternalComment2','Custodian','Fund']\n",
    "    cols_for_closed_list = ['ViewData.' + x for x in cols_for_closed_list]\n",
    "    cols_for_closed_x_list = [x + '_x' for x in cols_for_closed_list] + ['ViewData.Side0_UniqueIds_x','ViewData.Side1_UniqueIds_x']\n",
    "    cols_for_closed_y_list = [x + '_y' for x in cols_for_closed_list] + ['ViewData.Side0_UniqueIds_y','ViewData.Side1_UniqueIds_y']\n",
    "    cols_for_closed_x_y_list = cols_for_closed_x_list + cols_for_closed_y_list\n",
    "    return({\n",
    "            'cols_for_closed' : cols_for_closed_list,\n",
    "            'cols_for_closed_x' : cols_for_closed_x_list,\n",
    "            'cols_for_closed_y' : cols_for_closed_y_list,\n",
    "            'cols_for_closed_x_y' : cols_for_closed_x_y_list\n",
    "            })\n",
    "\n",
    "def cleaned_meo(#fun_filepath_meo, \n",
    "                fun_meo_df):\n",
    "#    meo = pd.read_csv(fun_filepath_meo)           .drop_duplicates()           .reset_index()           .drop('index',1)\n",
    "    meo = fun_meo_df\n",
    "    meo = normalize_bp_acct_col_names(fun_df = meo)\n",
    "    \n",
    "#    Commened out below line on 26-11-2020 to exclude SPM from closed coverage, and added the line below the commened line\n",
    "#    meo = meo[~meo['ViewData.Status'].isin(['SMT','HST', 'OC', 'CT', 'Archive','SMR'])]\n",
    "    meo = meo[~meo['ViewData.Status'].isin(['SPM','SMT','HST', 'OC', 'CT', 'Archive','SMR'])] \n",
    "    meo = meo[~meo['ViewData.Status'].isnull()]           .reset_index()           .drop('index',1)\n",
    "    \n",
    "    meo['Date'] = pd.to_datetime(meo['ViewData.Task Business Date'])\n",
    "    meo = meo[~meo['Date'].isnull()]           .reset_index()           .drop('index',1)\n",
    "    meo['Date'] = pd.to_datetime(meo['Date']).dt.date\n",
    "    meo['Date'] = meo['Date'].astype(str)\n",
    "\n",
    "    meo['ViewData.Side0_UniqueIds'] = meo['ViewData.Side0_UniqueIds'].astype(str)\n",
    "    meo['ViewData.Side1_UniqueIds'] = meo['ViewData.Side1_UniqueIds'].astype(str)\n",
    "\n",
    "    meo['flag_side0'] = meo.apply(lambda x: len(x['ViewData.Side0_UniqueIds'].split(',')), axis=1)\n",
    "    meo['flag_side1'] = meo.apply(lambda x: len(x['ViewData.Side1_UniqueIds'].split(',')), axis=1)\n",
    "\n",
    "    meo.loc[meo['ViewData.Side0_UniqueIds']=='nan','flag_side0'] = 0\n",
    "    meo.loc[meo['ViewData.Side1_UniqueIds']=='nan','flag_side1'] = 0\n",
    "\n",
    "    meo.loc[meo['ViewData.Side0_UniqueIds']=='None','flag_side0'] = 0\n",
    "    meo.loc[meo['ViewData.Side1_UniqueIds']=='None','flag_side1'] = 0\n",
    "   \n",
    "    meo['ViewData.BreakID'] = meo['ViewData.BreakID'].astype(int)\n",
    "    meo = meo[meo['ViewData.BreakID']!=-1]           .reset_index()           .drop('index',1)\n",
    "          \n",
    "    meo['Side_0_1_UniqueIds'] = meo['ViewData.Side0_UniqueIds'].astype(str) +                                 meo['ViewData.Side1_UniqueIds'].astype(str)\n",
    "                                \n",
    "    meo = meo.sort_values(by=['ViewData.Transaction ID','ViewData.Transaction Type'],ascending = False)\n",
    "    return(meo)\n",
    "    \n",
    "def cleaned_aua(fun_filepath_aua):\n",
    "    aua = pd.read_csv(fun_filepath_aua)       .drop_duplicates()       .reset_index()       .drop('index',1)       .sort_values(by=['ViewData.Transaction ID','ViewData.Transaction Type'],ascending = False)\n",
    "\n",
    "    aua = normalize_bp_acct_col_names(fun_df = aua)\n",
    "\n",
    "    \n",
    "    aua['Side_0_1_UniqueIds'] = aua['ViewData.Side0_UniqueIds'].astype(str) +                                 aua['ViewData.Side1_UniqueIds'].astype(str)\n",
    "    \n",
    "    return(aua)\n",
    "\n",
    "def Acct_MEO_combination_file(fun_side, fun_cleaned_meo_df):\n",
    "    if(fun_side == 'PB' or fun_side == 'BP' or fun_side == 'B-P' or fun_side == 'Prime Broker'):\n",
    "        side_meo = fun_cleaned_meo_df[(fun_cleaned_meo_df['flag_side1'] >= 1) & (fun_cleaned_meo_df['flag_side0'] == 0)]\n",
    "#        Currency_col_name = 'ViewData.B-P Currency'\n",
    "    elif(fun_side == 'Acct' or fun_side == 'Accounting'):\n",
    "        side_meo = fun_cleaned_meo_df[(fun_cleaned_meo_df['flag_side1'] == 0) & (fun_cleaned_meo_df['flag_side0'] >= 1)]\n",
    "#        Currency_col_name = 'ViewData.Accounting Currency'\n",
    "    else:\n",
    "        print('The only options for side are on of the following : ')\n",
    "        print('For Prime Broker side, the options are PB or BP or B-P or Prime Broker')\n",
    "        print('For Accounting side, the options are Acct or Accounting')\n",
    "        raise ValueError('Exiting function because fun_side argument was not from the accepted set of parameter values')\n",
    "    \n",
    "    side_meo['filter_key'] = side_meo['ViewData.Source Combination'].astype(str) +                          side_meo['ViewData.Mapped Custodian Account'].astype(str) +                          side_meo['ViewData.Currency'].astype(str)\n",
    "        \n",
    "    side_meo_training_df =[]\n",
    "    for key in (list(np.unique(np.array(list(side_meo['filter_key'].values))))):\n",
    "        side_meo_filter_slice = side_meo[side_meo['filter_key']==key]\n",
    "        if side_meo_filter_slice.empty == False:\n",
    "    \n",
    "            side_meo_filter_slice = side_meo_filter_slice.reset_index()\n",
    "            side_meo_filter_slice = side_meo_filter_slice.drop('index', 1)\n",
    "    \n",
    "            side_meo_filter_joined = pd.merge(side_meo_filter_slice, side_meo_filter_slice, on='filter_key')\n",
    "            side_meo_training_df.append(side_meo_filter_joined)\n",
    "    return(pd.concat(side_meo_training_df))\n",
    "    \n",
    "def identifying_closed_breaks_from_Trans_type(fun_side, fun_transaction_type_list, fun_side_meo_combination_df, fun_setup_code_crucial):\n",
    "    if(fun_side == 'PB' or fun_side == 'BP' or fun_side == 'B-P' or fun_side == 'Prime Broker'):\n",
    "        Net_amount_col_name_list = ['ViewData.B-P Net Amount_' + x for x in ['x','y']]\n",
    "        Side_0_1_UniqueIds_col_name_list = ['ViewData.Side1_UniqueIds_' + x for x in ['x','y']]\n",
    "    elif(fun_side == 'Acct' or fun_side == 'Accounting'):\n",
    "        Net_amount_col_name_list = ['ViewData.Accounting Net Amount_' + x for x in ['x','y']]\n",
    "        Side_0_1_UniqueIds_col_name_list = ['ViewData.Side0_UniqueIds_' + x for x in ['x','y']]\n",
    "    else:\n",
    "        print('The only options for side are on of the following : ')\n",
    "        print('For Prime Broker side, the options are PB or BP or B-P or Prime Broker')\n",
    "        print('For Accounting side, the options are Acct or Accounting')\n",
    "        raise ValueError('Exiting function because fun_side argument was not from the accepted set of parameter values')        \n",
    "    \n",
    "    if(fun_setup_code_crucial == '379'):\n",
    "        Transaction_type_closed_break_df =             fun_side_meo_combination_df[                     (fun_side_meo_combination_df['ViewData.Transaction Type_x'].astype(str).isin(fun_transaction_type_list)) &                     (fun_side_meo_combination_df['ViewData.Transaction Type_y'].astype(str).isin(fun_transaction_type_list)) &                     (abs(fun_side_meo_combination_df[Net_amount_col_name_list[0]]).astype(str) == abs(fun_side_meo_combination_df[Net_amount_col_name_list[1]]).astype(str)) &                     (fun_side_meo_combination_df[Side_0_1_UniqueIds_col_name_list[0]].astype(str) != fun_side_meo_combination_df[Side_0_1_UniqueIds_col_name_list[1]].astype(str))                     ]\n",
    "    return(set(\n",
    "                Transaction_type_closed_break_df['ViewData.Side0_UniqueIds_x'].astype(str) + \\\n",
    "                Transaction_type_closed_break_df['ViewData.Side1_UniqueIds_x'].astype(str)\n",
    "               ))\n",
    "\n",
    "def closed_breaks_captured_mode(fun_aua_df, fun_transaction_type, fun_captured_closed_breaks_set, fun_mode):\n",
    "    if(fun_transaction_type != 'All_Closed_Breaks'):\n",
    "        aua_df = fun_aua_df[(fun_aua_df['ViewData.Status'] == 'UCB') &                             (fun_aua_df['ViewData.Transaction Type'] == fun_transaction_type)]\n",
    "    else:\n",
    "        aua_df = fun_aua_df[(fun_aua_df['ViewData.Status'] == 'UCB')]\n",
    "        \n",
    "    aua_side_0_1_UniqueIds_set = set(aua_df['ViewData.Side0_UniqueIds'].astype(str) +                                  aua_df['ViewData.Side1_UniqueIds'].astype(str))\n",
    "    if(fun_mode == 'Correctly_Captured_In_AUA'):\n",
    "        list_to_return = list(aua_side_0_1_UniqueIds_set & fun_captured_closed_breaks_set)\n",
    "    elif(fun_mode == 'Not_Captured_In_AUA'):\n",
    "        list_to_return = list(aua_side_0_1_UniqueIds_set - fun_captured_closed_breaks_set)\n",
    "    elif(fun_mode == 'Over_Captured_In_AUA'):\n",
    "        list_to_return = list(fun_captured_closed_breaks_set - aua_side_0_1_UniqueIds_set)\n",
    "    return(list_to_return)\n",
    "\n",
    "def update_dict_to_output_breakids_number_pct(fun_dict, fun_aua_df, fun_loop_transaction_type, fun_count, fun_Side_0_1_UniqueIds_list):\n",
    "    mode_type_list = ['Correctly_Captured_In_AUA','Not_Captured_In_AUA','Over_Captured_In_AUA']\n",
    "    for mode_type in mode_type_list:\n",
    "#    if(fun_loop_transaction_type != 'All_Closed_Breaks'):\n",
    "        fun_dict[fun_loop_transaction_type][mode_type + '_BreakIDs_in_AUA'] = list(set(            fun_aua_df[fun_aua_df['Side_0_1_UniqueIds'].isin(                     closed_breaks_captured_mode(fun_aua_df = fun_aua_df,                                         fun_transaction_type = fun_loop_transaction_type,                                         fun_captured_closed_breaks_set = set(fun_Side_0_1_UniqueIds_list),                                         fun_mode = mode_type))]                    ['ViewData.BreakID']))\n",
    "    \n",
    "        fun_total_number = len(                             fun_dict[fun_loop_transaction_type][mode_type + '_BreakIDs_in_AUA'])\n",
    "        \n",
    "        fun_dict[fun_loop_transaction_type][mode_type + '_Total_Number'] = len(                             fun_dict[fun_loop_transaction_type][mode_type + '_BreakIDs_in_AUA'])\n",
    "        \n",
    "        if(fun_count != 0):\n",
    "            \n",
    "            fun_dict[fun_loop_transaction_type][mode_type + '_Percentage'] = fun_total_number/fun_count#\\\n",
    "#                                 fun_dict[fun_loop_transaction_type][mode_type + '_Total_Number']/fun_count\n",
    "        \n",
    "        else:\n",
    "            fun_dict[fun_loop_transaction_type][mode_type + '_Percentage'] = fun_loop_transaction_type + ' not found in Closed breaks of AUA'\n",
    "    return(fun_dict)\n",
    "\n",
    "def closed_daily_run(fun_setup_code, \n",
    "                     fun_date, \n",
    "                     fun_meo_df_daily_run#,\n",
    "#                     fun_main_filepath_meo, \n",
    "#                     fun_main_filepath_aua\n",
    "                     ):\n",
    "    setup_val = fun_setup_code\n",
    "    main_meo = cleaned_meo(fun_meo_df = fun_meo_df_daily_run)#, fun_filepath_meo = fun_main_filepath_meo\n",
    "    \n",
    "    BP_meo_training_df = Acct_MEO_combination_file(fun_side = 'PB', \\\n",
    "                                                   fun_cleaned_meo_df = main_meo)\n",
    "    \n",
    "    Acct_meo_training_df = Acct_MEO_combination_file(fun_side = 'Acct', \\\n",
    "                                                     fun_cleaned_meo_df = main_meo)\n",
    "\n",
    "#    main_aua = cleaned_aua(fun_filepath_aua = fun_main_filepath_aua)\n",
    "    \n",
    "    if(fun_setup_code == '379'):\n",
    "        Transaction_Type_dict = {\n",
    "                                'Interest BP_side' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['Interest'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df},\n",
    "                                'Interest Acct_side' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['Interest'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'STIF Interest BP_side' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['STIF Interest'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df},\n",
    "                                'STIF Interest Acct_side' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['STIF Interest'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'Buy BP_side' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['Buy'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df},\n",
    "                                'Buy Acct_side' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['Buy'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'Sell BP_side' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['Sell'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df},\n",
    "                                'Sell Acct_side' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['Sell'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'ForwardFX BP_side' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['ForwardFX'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df},\n",
    "                                'ForwardFX Acct_side' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['ForwardFX'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'Internal Trans' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['Internal Trans'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df},\n",
    "                                'Withdraw' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['Withdraw'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'Deposit' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['Deposit'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'Redemption' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['Redemption'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df},\n",
    "                                'Subscription' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['Redemption'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df},\n",
    "                                'Incoming Wire' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['Incoming Wire'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df},\n",
    "                                'Transfer' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['Transfer'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'Withdrawal BP_side' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['Withdrawal'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df},\n",
    "                                'Withdrawal Acct_side' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['Withdrawal'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'Revenue' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['Revenue'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'Pay Down' : {'side' : 'Acct',\n",
    "                                           'Transaction_Type' : ['Pay Down'],\n",
    "                                           'Side_meo_training_df' : Acct_meo_training_df},\n",
    "                                'Over & Short' : {'side' : 'PB',\n",
    "                                           'Transaction_Type' : ['Over & Short'],\n",
    "                                           'Side_meo_training_df' : BP_meo_training_df}\n",
    "                                }\n",
    "\n",
    "    print(os.getcwd())\n",
    "    os.chdir('D:\\\\ViteosModel\\\\Closed')\n",
    "    print(os.getcwd())\n",
    "    \n",
    "    filepath_stdout = fun_setup_code + '_closed_run_date_' + str(fun_date) + '_timestamp_' + str(datetime.now().strftime(\"%d_%m_%Y_%H_%M\")) + '.txt'\n",
    "    orig_stdout = sys.stdout\n",
    "    f = open(filepath_stdout, 'w')\n",
    "    sys.stdout = f\n",
    "    \n",
    "    Side_0_1_UniqueIds_closed_all_list = []\n",
    "    for Transaction_type in Transaction_Type_dict:\n",
    "\n",
    "        Side_0_1_UniqueIds_for_Transaction_type = identifying_closed_breaks_from_Trans_type(fun_side = Transaction_Type_dict.get(Transaction_type).get('side'), \\\n",
    "                                                                                            fun_transaction_type_list = Transaction_Type_dict.get(Transaction_type).get('Transaction_Type'), \\\n",
    "                                                                                            fun_side_meo_combination_df = Transaction_Type_dict.get(Transaction_type).get('Side_meo_training_df'), \\\n",
    "                                                                                            fun_setup_code_crucial = setup_val)\n",
    "\n",
    "#        count_closed_breaks_for_transaction_type = len(set(main_aua[(main_aua['ViewData.Status'] == 'UCB') & \\\n",
    "#                                                                    (main_aua['ViewData.Transaction Type'] == Transaction_type)]['Side_0_1_UniqueIds']))\n",
    "#        \n",
    "#        Transaction_Type_dict = update_dict_to_output_breakids_number_pct(fun_dict = Transaction_Type_dict, \\\n",
    "#                                                                          fun_aua_df = main_aua, \\\n",
    "#                                                                          fun_loop_transaction_type = Transaction_type, \\\n",
    "#                                                                          fun_count = count_closed_breaks_for_transaction_type, \\\n",
    "#                                                                          fun_Side_0_1_UniqueIds_list = Side_0_1_UniqueIds_for_Transaction_type)\n",
    "            \n",
    "        \n",
    "        Side_0_1_UniqueIds_closed_all_list.extend(Side_0_1_UniqueIds_for_Transaction_type)\n",
    "        print('\\n' + Transaction_type + '\\n')\n",
    "#        pprint.pprint(dictionary_exclude_keys(fun_dict = Transaction_Type_dict.get(Transaction_type),                                      fun_keys_to_exclude = {'side','Transaction_Type','Side_meo_training_df'}),                      width = 4)\n",
    "    \n",
    "    sys.stdout = orig_stdout\n",
    "    f.close()\n",
    "    \n",
    "#    count_all_closed_breaks = len(set(main_aua[(main_aua['ViewData.Status'] == 'UCB')]                                               ['Side_0_1_UniqueIds']))\n",
    "    \n",
    "#    aua_closed_dict = {'All_Closed_Breaks' : {}}\n",
    "#    aua_closed_dict = update_dict_to_output_breakids_number_pct(fun_dict = aua_closed_dict,\\\n",
    "#                                                                fun_aua_df = main_aua, \\\n",
    "#                                                                fun_loop_transaction_type = 'All_Closed_Breaks', \\\n",
    "#                                                                fun_count = count_all_closed_breaks, \\\n",
    "#                                                                fun_Side_0_1_UniqueIds_list = Side_0_1_UniqueIds_closed_all_list)\n",
    "    \n",
    "#    write_dict_at_top(fun_filename = filepath_stdout, \\\n",
    "#                      fun_dict_to_add = aua_closed_dict)\n",
    "    \n",
    "    return(Side_0_1_UniqueIds_closed_all_list)\n",
    "\n",
    "#### Closed break functions - End #### \n",
    "\n",
    "#### Break Prediction functions - Begin #### \n",
    "\n",
    "def equals_fun(a,b):\n",
    "    if a == b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "vec_equals_fun = np.vectorize(equals_fun)\n",
    "\n",
    "\n",
    "def descclean(com,cat_list):\n",
    "    cat_all1 = []\n",
    "    list1 = cat_list\n",
    "    m = 0\n",
    "    if (type(com) == str):\n",
    "        com = com.lower()\n",
    "        com1 =  re.split(\"[,/. \\-!?:]+\", com)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for item in list1:\n",
    "            if (type(item) == str):\n",
    "                item = item.lower()\n",
    "                item1 = item.split(' ')\n",
    "                lst3 = [value for value in item1 if value in com1] \n",
    "                if len(lst3) == len(item1):\n",
    "                    cat_all1.append(item)\n",
    "                    m = m+1\n",
    "            \n",
    "                else:\n",
    "                    m = m\n",
    "            else:\n",
    "                    m = 0\n",
    "    else:\n",
    "        m = 0\n",
    "    \n",
    "\n",
    "            \n",
    "    if m >0 :\n",
    "        return list(set(cat_all1))\n",
    "    else:\n",
    "        if ((type(com)==str)):\n",
    "            if (len(com1)<4):\n",
    "                if ((len(com1)==1) & com1[0].startswith('20')== True):\n",
    "                    return 'swap id'\n",
    "                else:\n",
    "                    return com\n",
    "            else:\n",
    "                return 'NA'\n",
    "        else:\n",
    "            return 'NA'\n",
    "\n",
    "def currcln(x):\n",
    "    if (type(x)==list):\n",
    "        return x\n",
    "      \n",
    "    else:\n",
    "       \n",
    "        \n",
    "        if x == 'NA':\n",
    "            return \"NA\"\n",
    "        elif (('dollar' in x) | ('dollars' in x )):\n",
    "            return 'dollar'\n",
    "        elif (('pound' in x) | ('pounds' in x)):\n",
    "            return 'pound'\n",
    "        elif ('yen' in x):\n",
    "            return 'yen'\n",
    "        elif ('euro' in x) :\n",
    "            return 'euro'\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "def catcln1(cat,df):\n",
    "    ret = []\n",
    "    if (type(cat)==list):\n",
    "        \n",
    "        if 'equity swap settlement' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'equity swap' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'swap settlement' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'swap unwind' in cat:\n",
    "            ret.append('swap unwind')\n",
    "        #return 'swap unwind'\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "        else:\n",
    "        \n",
    "       \n",
    "            for item in cat:\n",
    "            \n",
    "                a = df[df['Pairing']==item]['replace'].values[0]\n",
    "                if a not in ret:\n",
    "                    ret.append(a)\n",
    "        return list(set(ret))\n",
    "      \n",
    "    else:\n",
    "        return cat\n",
    "\n",
    "def desccat(x):\n",
    "    if isinstance(x, list):\n",
    "        \n",
    "        if 'equity swap settlement' in x:\n",
    "            return 'swap settlement'\n",
    "        elif 'collateral transfer' in x:\n",
    "            return 'collateral transfer'\n",
    "        elif 'dividend' in x:\n",
    "            return 'dividend'\n",
    "        elif (('loan' in x) & ('option' in x)):\n",
    "            return 'option loan'\n",
    "        \n",
    "        elif (('interest' in x) & ('corp' in x) ):\n",
    "            return 'corp loan'\n",
    "        elif (('interest' in x) & ('loan' in x) ):\n",
    "            return 'interest'\n",
    "        else:\n",
    "            return x[0]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def new_pf_mapping(x):\n",
    "    if x=='GSIL':\n",
    "        return 'GS'\n",
    "    elif x == 'CITIGM':\n",
    "        return 'CITI'\n",
    "    elif x == 'JPMNA':\n",
    "        return 'JPM'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def mhreplaced(item):\n",
    "    word1 = []\n",
    "    word2 = []\n",
    "    if (type(item) == str):\n",
    "    \n",
    "        for items in item.split(' '):\n",
    "            if (type(items) == str):\n",
    "                items = items.lower()\n",
    "                if items.isdigit() == False:\n",
    "                    word1.append(items)\n",
    "        \n",
    "            \n",
    "                for c in word1:\n",
    "                    if c.endswith('MH')==False:\n",
    "                        word2.append(c)\n",
    "    \n",
    "                words = ' '.join(word2)\n",
    "                return words\n",
    "    else:\n",
    "        return item\n",
    "    \n",
    "\n",
    "def fundmatch(item):\n",
    "    items = item.lower()\n",
    "    items = item.replace(' ','') \n",
    "    return items\n",
    "\n",
    "def is_num(item):\n",
    "    try:\n",
    "        float(item)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_date_format(item):\n",
    "    try:\n",
    "        parse(item, fuzzy=False)\n",
    "        return True\n",
    "    \n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def date_edge_cases(item):\n",
    "    if len(item) == 5 and item[2] =='/' and is_num(item[:2]) and is_num(item[3:]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def nan_fun(x):\n",
    "    if x=='nan':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def a_keymatch(a_cusip, a_isin):\n",
    "    \n",
    "    pb_nan = 0\n",
    "    a_common_key = 'NA' \n",
    "    if a_cusip=='nan' and a_isin =='nan':\n",
    "        pb_nan =1\n",
    "    elif(a_cusip!='nan' and a_isin == 'nan'):\n",
    "        a_common_key = a_cusip\n",
    "    elif(a_cusip =='nan' and a_isin !='nan'):\n",
    "        a_common_key = a_isin\n",
    "    else:\n",
    "        a_common_key = a_isin\n",
    "        \n",
    "    return (pb_nan, a_common_key)\n",
    "\n",
    "def b_keymatch(b_cusip, b_isin):\n",
    "    accounting_nan = 0\n",
    "    b_common_key = 'NA'\n",
    "    if b_cusip =='nan' and b_isin =='nan':\n",
    "        accounting_nan =1\n",
    "    elif (b_cusip!='nan' and b_isin == 'nan'):\n",
    "        b_common_key = b_cusip\n",
    "    elif(b_cusip =='nan' and b_isin !='nan'):\n",
    "        b_common_key = b_isin\n",
    "    else:\n",
    "        b_common_key = b_isin\n",
    "    return (accounting_nan, b_common_key)\n",
    "\n",
    "\n",
    "def nan_equals_fun(a,b):\n",
    "    if a==1 and b==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def new_key_match_fun(a,b,c):\n",
    "    if a==b and c==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def  clean_text(df, text_field, new_text_field_name):\n",
    "    df[text_field] = df[text_field].astype(str)\n",
    "    df[new_text_field_name] = df[text_field].str.lower()\n",
    "    \n",
    "    \n",
    "    \n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda x: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", x))  \n",
    "    # remove numbers\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda x: re.sub(r\"\\d+\", \"\", x))\n",
    "    df[new_text_field_name] = df[new_text_field_name].str.replace('usd','')\n",
    "    df[new_text_field_name] = df[new_text_field_name].str.replace('eur0','')\n",
    "    df[new_text_field_name] = df[new_text_field_name].str.replace(' usd','')\n",
    "    df[new_text_field_name] = df[new_text_field_name].str.replace(' euro','')\n",
    "\n",
    "    df[new_text_field_name] = df[new_text_field_name].str.replace(' eur','')\n",
    "    df[new_text_field_name] = df[new_text_field_name].str.replace('eur','')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def umr_seg(X_test):\n",
    "    b_count = X_test.groupby(['SideB.ViewData.Side0_UniqueIds'])['Predicted_action'].value_counts().reset_index(name='count')\n",
    "    b_unique = X_test.groupby(['SideB.ViewData.Side0_UniqueIds'])['Predicted_action'].unique().reset_index()\n",
    "    \n",
    "    b_unique['len'] = b_unique['Predicted_action'].str.len()\n",
    "    b_count2 = pd.merge(b_count, b_unique.drop('Predicted_action',1), on='SideB.ViewData.Side0_UniqueIds', how='left')\n",
    "    umr_table = b_count2[(b_count2['Predicted_action']=='UMR_One_to_One') & (b_count2['count']==1) & (b_count2['len']<=2)]\n",
    "    return umr_table['SideB.ViewData.Side0_UniqueIds'].values\n",
    "\n",
    "def normalize_final_no_pair_table_col_names(fun_final_no_pair_table):\n",
    "    final_no_pair_table_col_names_mapping_dict = {\n",
    "                                      'SideA.ViewData.Side1_UniqueIds' : 'ViewData.Side1_UniqueIds',\n",
    "                                      'SideB.ViewData.Side0_UniqueIds' : 'ViewData.Side0_UniqueIds',\n",
    "                                      'SideA.ViewData.BreakID_A_side' : 'ViewData.BreakID_Side1', \n",
    "                                      'SideB.ViewData.BreakID_B_side' : 'ViewData.BreakID_Side0'\n",
    "                                      }\n",
    "    fun_final_no_pair_table.rename(columns = final_no_pair_table_col_names_mapping_dict, inplace = True)\n",
    "    return(fun_final_no_pair_table)\n",
    "\n",
    "def no_pair_seg(X_test):\n",
    "    \n",
    "    b_side_agg = X_test.groupby(['SideB.ViewData.Side0_UniqueIds'])['Predicted_action_2'].unique().reset_index()\n",
    "    a_side_agg = X_test.groupby(['SideA.ViewData.Side1_UniqueIds'])['Predicted_action_2'].unique().reset_index()\n",
    "    \n",
    "    b_side_agg['len'] = b_side_agg['Predicted_action_2'].str.len()\n",
    "    b_side_agg['No_Pair_flag'] = b_side_agg['Predicted_action_2'].apply(lambda x: 1 if 'No-Pair' in x else 0)\n",
    "\n",
    "    a_side_agg['len'] = a_side_agg['Predicted_action_2'].str.len()\n",
    "    a_side_agg['No_Pair_flag'] = a_side_agg['Predicted_action_2'].apply(lambda x: 1 if 'No-Pair' in x else 0)\n",
    "    \n",
    "    no_pair_ids_b_side = b_side_agg[(b_side_agg['len']==1) & (b_side_agg['No_Pair_flag']==1)]['SideB.ViewData.Side0_UniqueIds'].values\n",
    "\n",
    "    no_pair_ids_a_side = a_side_agg[(a_side_agg['len']==1) & (a_side_agg['No_Pair_flag']==1)]['SideA.ViewData.Side1_UniqueIds'].values\n",
    "    \n",
    "    return no_pair_ids_b_side, no_pair_ids_a_side\n",
    " \n",
    "def subSum(numbers,total):\n",
    "    for length in range(1, 3):\n",
    "        if len(numbers) < length or length < 1:\n",
    "            return []\n",
    "        for index,number in enumerate(numbers):\n",
    "            if length == 1 and np.isclose(number, total,atol=0.25).any():\n",
    "                return [number]\n",
    "            subset = subSum(numbers[index+1:],total-number)\n",
    "            if subset: \n",
    "                return [number] + subset\n",
    "        return []\n",
    "\n",
    "def one_to_one_umb(data):\n",
    "    \n",
    "    count = data['SideB.ViewData.Side0_UniqueIds'].value_counts().reset_index(name='count0')\n",
    "    id0s = count[count['count0']==1]['index'].unique()\n",
    "    id1s = data[data['SideB.ViewData.Side0_UniqueIds'].isin(id0s)]['SideA.ViewData.Side1_UniqueIds']\n",
    "    \n",
    "    count1 = data['SideA.ViewData.Side1_UniqueIds'].value_counts().reset_index(name='count1')\n",
    "    final_ids = count1[(count1['count1']==1) & (count1['index'].isin(id1s))]['index'].unique()\n",
    "    return final_ids\n",
    "\n",
    "def no_pair_seg2(X_test):\n",
    "    \n",
    "    b_side_agg = X_test.groupby(['SideB.ViewData.Side0_UniqueIds'])['Predicted_action'].unique().reset_index()\n",
    "    a_side_agg = X_test.groupby(['SideA.ViewData.Side1_UniqueIds'])['Predicted_action'].unique().reset_index()\n",
    "    \n",
    "    b_side_agg['len'] = b_side_agg['Predicted_action'].str.len()\n",
    "    b_side_agg['No_Pair_flag'] = b_side_agg['Predicted_action'].apply(lambda x: 1 if 'No-Pair' in x else 0)\n",
    "\n",
    "    a_side_agg['len'] = a_side_agg['Predicted_action'].str.len()\n",
    "    a_side_agg['No_Pair_flag'] = a_side_agg['Predicted_action'].apply(lambda x: 1 if 'No-Pair' in x else 0)\n",
    "    \n",
    "    no_pair_ids_b_side = b_side_agg[(b_side_agg['len']==1) & (b_side_agg['No_Pair_flag']==1)]['SideB.ViewData.Side0_UniqueIds'].values\n",
    "\n",
    "    no_pair_ids_a_side = a_side_agg[(a_side_agg['len']==1) & (a_side_agg['No_Pair_flag']==1)]['SideA.ViewData.Side1_UniqueIds'].values\n",
    "    \n",
    "    return no_pair_ids_b_side, no_pair_ids_a_side\n",
    "\n",
    "def return_int_list(list_x):\n",
    "    return [int(i) for i in list_x]\n",
    "    \n",
    "def normalize_bp_acct_col_names(fun_df):\n",
    "    bp_acct_col_names_mapping_dict = {\n",
    "                                      'ViewData.Cust Net Amount' : 'ViewData.B-P Net Amount',\n",
    "                                      'ViewData.Cust Net Amount Difference' : 'ViewData.B-P Net Amount Difference',\n",
    "                                      'ViewData.Cust Net Amount Difference Absolute' : 'ViewData.B-P Net Amount Difference Absolute',\n",
    "                                      'ViewData.CP Net Amount' : 'ViewData.B-P Net Amount',\n",
    "                                      'ViewData.CP Net Amount Difference' : 'ViewData.B-P Net Amount Difference',\n",
    "                                      'ViewData.CP Net Amount Difference Absolute' : 'ViewData.B-P Net Amount Difference Absolute',\n",
    "                                      'ViewData.PMSVendor Net Amount' : 'ViewData.Accounting Net Amount'\n",
    "                                        }\n",
    "    fun_df.rename(columns = bp_acct_col_names_mapping_dict, inplace = True)\n",
    "    return(fun_df)\n",
    "\n",
    "def find_BreakID_and_other_cols_in_meo_for_Side_0_1_UniqueIds_value(fun_string_value_of_Side_0_1_UniqueIds, fun_meo_df, fun_side, fun_other_cols_list = None):\n",
    "    if fun_other_cols_list is None:\n",
    "        all_cols_to_find = ['ViewData.BreakID']\n",
    "    else:\n",
    "        all_cols_to_find = fun_other_cols_list + ['ViewData.BreakID']\n",
    "    if(fun_side == 0):\n",
    "        return(fun_meo_df[fun_meo_df['ViewData.Side0_UniqueIds'] == fun_string_value_of_Side_0_1_UniqueIds][all_cols_to_find])\n",
    "    elif(fun_side == 1):\n",
    "        return(fun_meo_df[fun_meo_df['ViewData.Side1_UniqueIds'] == fun_string_value_of_Side_0_1_UniqueIds][all_cols_to_find])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def contains_multiple_values_in_either_Side_0_or_1_UniqueIds_for_expected_single_sided_status(fun_row):\n",
    "    \n",
    "    if(',' in str(fun_row['ViewData.Side0_UniqueIds'])):\n",
    "        Side_0_contains_comma = 1\n",
    "    else:\n",
    "        Side_0_contains_comma = 0\n",
    "\n",
    "    if(',' in str(fun_row['ViewData.Side1_UniqueIds'])):\n",
    "        Side_1_contains_comma = 1\n",
    "    else:\n",
    "        Side_1_contains_comma = 0\n",
    "    \n",
    "    if((str(fun_row['ViewData.Status']) in ['OB','SDB','UOB','CNF','CMF']) and ((Side_0_contains_comma == 1) or (Side_1_contains_comma == 1))):\n",
    "        return('remove')\n",
    "    else:\n",
    "        return('keep')\n",
    "\n",
    "def find_Side_0_1_UniqueIds_and_other_cols_in_meo_for_BreakID_value(fun_string_value_of_BreakID,fun_meo_df,fun_other_cols_list = None):\n",
    "    if fun_other_cols_list is None:\n",
    "        all_cols_to_find = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.Status']\n",
    "    else:\n",
    "        all_cols_to_find = fun_other_cols_list + ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.Status']\n",
    "    fun_meo_df['ViewData.BreakID'] = fun_meo_df['ViewData.BreakID'].astype(str)\n",
    "    return(fun_meo_df[fun_meo_df['ViewData.BreakID'] == fun_string_value_of_BreakID][all_cols_to_find])\n",
    "\n",
    "def make_Side0_Side1_columns_for_final_smb_ob_table_row_apply(row, fun_side):\n",
    "#    print(row)\n",
    "\n",
    "    if(fun_side == 0):\n",
    "        if(row['Side0_UniqueIds_OB'] == ''):\n",
    "            return(row['Side0_UniqueIds_SMB'])\n",
    "        else:\n",
    "            return(row['Side0_UniqueIds_OB'] + ',' + row['Side0_UniqueIds_SMB'])\n",
    "    elif(fun_side == 1):\n",
    "        if(row['Side1_UniqueIds_OB'] == ''):\n",
    "            return(row['Side1_UniqueIds_SMB'])\n",
    "        else:\n",
    "            return(row['Side1_UniqueIds_OB'] + ',' + row['Side1_UniqueIds_SMB'])\n",
    "    \n",
    "def make_Side0_Side1_columns_for_final_smb_ob_table(fun_final_smb_ob_table, fun_meo_df):\n",
    "    fun_final_smb_ob_table = pd.merge(fun_final_smb_ob_table,fun_meo_df[['ViewData.BreakID','ViewData.Side0_UniqueIds']], left_on = 'BreakID_OB', right_on = 'ViewData.BreakID')\n",
    "    fun_final_smb_ob_table.drop('ViewData.BreakID', axis = 1, inplace = True)\n",
    "    fun_final_smb_ob_table.rename(columns = {'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds_OB'}, inplace = True) \n",
    "\n",
    "    fun_final_smb_ob_table = pd.merge(fun_final_smb_ob_table,fun_meo_df[['ViewData.BreakID','ViewData.Side1_UniqueIds']], left_on = 'BreakID_OB', right_on = 'ViewData.BreakID')\n",
    "    fun_final_smb_ob_table.drop('ViewData.BreakID', axis = 1, inplace = True)\n",
    "    fun_final_smb_ob_table.rename(columns = {'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds_OB'}, inplace = True) \n",
    "\n",
    "    fun_final_smb_ob_table = pd.merge(fun_final_smb_ob_table,fun_meo_df[['ViewData.BreakID','ViewData.Side0_UniqueIds']], left_on = 'BreakID_SMB', right_on = 'ViewData.BreakID')\n",
    "    fun_final_smb_ob_table.drop('ViewData.BreakID', axis = 1, inplace = True)\n",
    "    fun_final_smb_ob_table.rename(columns = {'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds_SMB'}, inplace = True) \n",
    "\n",
    "    fun_final_smb_ob_table = pd.merge(fun_final_smb_ob_table,fun_meo_df[['ViewData.BreakID','ViewData.Side1_UniqueIds']], left_on = 'BreakID_SMB', right_on = 'ViewData.BreakID')\n",
    "    fun_final_smb_ob_table.drop('ViewData.BreakID', axis = 1, inplace = True)\n",
    "    fun_final_smb_ob_table.rename(columns = {'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds_SMB'}, inplace = True) \n",
    "\n",
    "    fun_final_smb_ob_table['Side0_UniqueIds_OB'] = fun_final_smb_ob_table['Side0_UniqueIds_OB'].astype(str)            \n",
    "    fun_final_smb_ob_table['Side1_UniqueIds_OB'] = fun_final_smb_ob_table['Side1_UniqueIds_OB'].astype(str)            \n",
    "    fun_final_smb_ob_table['Side0_UniqueIds_SMB'] = fun_final_smb_ob_table['Side0_UniqueIds_SMB'].astype(str)            \n",
    "    fun_final_smb_ob_table['Side1_UniqueIds_SMB'] = fun_final_smb_ob_table['Side1_UniqueIds_SMB'].astype(str)            \n",
    "\n",
    "    fun_final_smb_ob_table['Side0_UniqueIds_OB'] = fun_final_smb_ob_table['Side0_UniqueIds_OB'].replace('None','')            \n",
    "    fun_final_smb_ob_table['Side1_UniqueIds_OB'] = fun_final_smb_ob_table['Side1_UniqueIds_OB'].replace('None','')            \n",
    "    fun_final_smb_ob_table['Side0_UniqueIds_SMB'] = fun_final_smb_ob_table['Side0_UniqueIds_SMB'].replace('None','')            \n",
    "    fun_final_smb_ob_table['Side1_UniqueIds_SMB'] = fun_final_smb_ob_table['Side1_UniqueIds_SMB'].replace('None','')            \n",
    "\n",
    "    fun_final_smb_ob_table['Side0_UniqueIds_OB'] = fun_final_smb_ob_table['Side0_UniqueIds_OB'].replace('nan','')            \n",
    "    fun_final_smb_ob_table['Side1_UniqueIds_OB'] = fun_final_smb_ob_table['Side1_UniqueIds_OB'].replace('nan','')\n",
    "    fun_final_smb_ob_table['Side0_UniqueIds_SMB'] = fun_final_smb_ob_table['Side0_UniqueIds_SMB'].replace('nan','') \n",
    "    fun_final_smb_ob_table['Side1_UniqueIds_SMB'] = fun_final_smb_ob_table['Side1_UniqueIds_SMB'].replace('nan','')\n",
    "\n",
    "    fun_final_smb_ob_table['Side0_UniqueIds'] = fun_final_smb_ob_table.apply(lambda row : make_Side0_Side1_columns_for_final_smb_ob_table_row_apply(row, fun_side = 0),axis = 1,result_type=\"expand\")\n",
    "    fun_final_smb_ob_table['Side1_UniqueIds'] = fun_final_smb_ob_table.apply(lambda row : make_Side0_Side1_columns_for_final_smb_ob_table_row_apply(row, fun_side = 1),axis = 1,result_type=\"expand\")\n",
    "#    fun_final_smb_ob_table.iloc[fun_final_smb_ob_table['Side0_UniqueIds_OB'] == '', 'Side0_UniqueIds'] = fun_final_smb_ob_table['Side0_UniqueIds_SMB']\n",
    "#    fun_final_smb_ob_table.iloc[fun_final_smb_ob_table['Side0_UniqueIds_OB'] != '', 'Side0_UniqueIds'] = fun_final_smb_ob_table['Side0_UniqueIds_OB'] + fun_final_smb_ob_table['Side0_UniqueIds_SMB']\n",
    "#    fun_final_smb_ob_table.iloc[fun_final_smb_ob_table['Side1_UniqueIds_OB'] == '', 'Side1_UniqueIds'] = fun_final_smb_ob_table['Side1_UniqueIds_SMB']\n",
    "#    fun_final_smb_ob_table.iloc[fun_final_smb_ob_table['Side1_UniqueIds_OB'] != '', 'Side1_UniqueIds'] = fun_final_smb_ob_table['Side1_UniqueIds_OB'] + fun_final_smb_ob_table['Side1_UniqueIds_SMB']\n",
    "\n",
    "    fun_final_smb_ob_table.drop(['Side0_UniqueIds_OB','Side1_UniqueIds_OB','Side0_UniqueIds_SMB','Side1_UniqueIds_SMB'], axis = 1, inplace = True)\n",
    "\n",
    "    return(fun_final_smb_ob_table)\n",
    "\n",
    "\n",
    "def make_Side0_Side1_columns_for_final_smb_ob_or_umb_ob_table_row_apply(row, fun_side, fun_umb_or_smb_flag):\n",
    "#    print(row)\n",
    "    if(fun_umb_or_smb_flag == 'SMB'):\n",
    "        Side0_UniqueIds_col_name = 'Side0_UniqueIds_SMB'\n",
    "        Side1_UniqueIds_col_name = 'Side1_UniqueIds_SMB'\n",
    "    elif(fun_umb_or_smb_flag == 'UMB'):\n",
    "        Side0_UniqueIds_col_name = 'Side0_UniqueIds_UMB'\n",
    "        Side1_UniqueIds_col_name = 'Side1_UniqueIds_UMB'\n",
    "        \n",
    "    if(fun_side == 0):\n",
    "        if(row['Side0_UniqueIds_OB'] == ''):\n",
    "            return(row[Side0_UniqueIds_col_name])\n",
    "        else:\n",
    "            return(row['Side0_UniqueIds_OB'] + ',' + row[Side0_UniqueIds_col_name])\n",
    "    elif(fun_side == 1):\n",
    "        if(row['Side1_UniqueIds_OB'] == ''):\n",
    "            return(row[Side1_UniqueIds_col_name])\n",
    "        else:\n",
    "            return(row['Side1_UniqueIds_OB'] + ',' + row[Side1_UniqueIds_col_name])\n",
    "    \n",
    "def make_Side0_Side1_columns_for_final_smb_or_umb_ob_table(fun_final_smb_or_umb_ob_table, fun_meo_df, fun_umb_or_smb_flag):\n",
    "    flag_value = fun_umb_or_smb_flag\n",
    "    if(fun_umb_or_smb_flag == 'SMB'):\n",
    "        Side0_UniqueIds_col_name = 'Side0_UniqueIds_SMB'\n",
    "        Side1_UniqueIds_col_name = 'Side1_UniqueIds_SMB'\n",
    "        BreakID_smb_umb_col_name = 'BreakID_SMB'\n",
    "    elif(fun_umb_or_smb_flag == 'UMB'):\n",
    "        Side0_UniqueIds_col_name = 'Side0_UniqueIds_UMB'\n",
    "        Side1_UniqueIds_col_name = 'Side1_UniqueIds_UMB'\n",
    "        BreakID_smb_umb_col_name = 'BreakID_UMB'\n",
    "\n",
    "    fun_final_smb_or_umb_ob_table = pd.merge(fun_final_smb_or_umb_ob_table,fun_meo_df[['ViewData.BreakID','ViewData.Side0_UniqueIds']], left_on = 'BreakID_OB', right_on = 'ViewData.BreakID')\n",
    "    fun_final_smb_or_umb_ob_table.drop('ViewData.BreakID', axis = 1, inplace = True)\n",
    "    fun_final_smb_or_umb_ob_table.rename(columns = {'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds_OB'}, inplace = True) \n",
    "\n",
    "    fun_final_smb_or_umb_ob_table = pd.merge(fun_final_smb_or_umb_ob_table,fun_meo_df[['ViewData.BreakID','ViewData.Side1_UniqueIds']], left_on = 'BreakID_OB', right_on = 'ViewData.BreakID')\n",
    "    fun_final_smb_or_umb_ob_table.drop('ViewData.BreakID', axis = 1, inplace = True)\n",
    "    fun_final_smb_or_umb_ob_table.rename(columns = {'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds_OB'}, inplace = True) \n",
    "\n",
    "    fun_final_smb_or_umb_ob_table = pd.merge(fun_final_smb_or_umb_ob_table,fun_meo_df[['ViewData.BreakID','ViewData.Side0_UniqueIds']], left_on = BreakID_smb_umb_col_name, right_on = 'ViewData.BreakID')\n",
    "    fun_final_smb_or_umb_ob_table.drop('ViewData.BreakID', axis = 1, inplace = True)\n",
    "    fun_final_smb_or_umb_ob_table.rename(columns = {'ViewData.Side0_UniqueIds' : Side0_UniqueIds_col_name}, inplace = True) \n",
    "\n",
    "    fun_final_smb_or_umb_ob_table = pd.merge(fun_final_smb_or_umb_ob_table,fun_meo_df[['ViewData.BreakID','ViewData.Side1_UniqueIds']], left_on = BreakID_smb_umb_col_name, right_on = 'ViewData.BreakID')\n",
    "    fun_final_smb_or_umb_ob_table.drop('ViewData.BreakID', axis = 1, inplace = True)\n",
    "    fun_final_smb_or_umb_ob_table.rename(columns = {'ViewData.Side1_UniqueIds' : Side1_UniqueIds_col_name}, inplace = True) \n",
    "\n",
    "    fun_final_smb_or_umb_ob_table['Side0_UniqueIds_OB'] = fun_final_smb_or_umb_ob_table['Side0_UniqueIds_OB'].astype(str)            \n",
    "    fun_final_smb_or_umb_ob_table['Side1_UniqueIds_OB'] = fun_final_smb_or_umb_ob_table['Side1_UniqueIds_OB'].astype(str)            \n",
    "    fun_final_smb_or_umb_ob_table[Side0_UniqueIds_col_name] = fun_final_smb_or_umb_ob_table[Side0_UniqueIds_col_name].astype(str)            \n",
    "    fun_final_smb_or_umb_ob_table[Side1_UniqueIds_col_name] = fun_final_smb_or_umb_ob_table[Side1_UniqueIds_col_name].astype(str)            \n",
    "\n",
    "    fun_final_smb_or_umb_ob_table['Side0_UniqueIds_OB'] = fun_final_smb_or_umb_ob_table['Side0_UniqueIds_OB'].replace('None','')            \n",
    "    fun_final_smb_or_umb_ob_table['Side1_UniqueIds_OB'] = fun_final_smb_or_umb_ob_table['Side1_UniqueIds_OB'].replace('None','')            \n",
    "    fun_final_smb_or_umb_ob_table[Side0_UniqueIds_col_name] = fun_final_smb_or_umb_ob_table[Side0_UniqueIds_col_name].replace('None','')            \n",
    "    fun_final_smb_or_umb_ob_table[Side1_UniqueIds_col_name] = fun_final_smb_or_umb_ob_table[Side1_UniqueIds_col_name].replace('None','')            \n",
    "\n",
    "    fun_final_smb_or_umb_ob_table['Side0_UniqueIds_OB'] = fun_final_smb_or_umb_ob_table['Side0_UniqueIds_OB'].replace('nan','')            \n",
    "    fun_final_smb_or_umb_ob_table['Side1_UniqueIds_OB'] = fun_final_smb_or_umb_ob_table['Side1_UniqueIds_OB'].replace('nan','')\n",
    "    fun_final_smb_or_umb_ob_table[Side0_UniqueIds_col_name] = fun_final_smb_or_umb_ob_table[Side0_UniqueIds_col_name].replace('nan','') \n",
    "    fun_final_smb_or_umb_ob_table[Side1_UniqueIds_col_name] = fun_final_smb_or_umb_ob_table[Side1_UniqueIds_col_name].replace('nan','')\n",
    "\n",
    "    fun_final_smb_or_umb_ob_table['Side0_UniqueIds'] = fun_final_smb_or_umb_ob_table.apply(lambda row : make_Side0_Side1_columns_for_final_smb_ob_or_umb_ob_table_row_apply(row, fun_side = 0, fun_umb_or_smb_flag = flag_value),axis = 1,result_type=\"expand\")\n",
    "    fun_final_smb_or_umb_ob_table['Side1_UniqueIds'] = fun_final_smb_or_umb_ob_table.apply(lambda row : make_Side0_Side1_columns_for_final_smb_ob_or_umb_ob_table_row_apply(row, fun_side = 1, fun_umb_or_smb_flag = flag_value),axis = 1,result_type=\"expand\")\n",
    "#    fun_final_smb_or_umb_ob_table.iloc[fun_final_smb_or_umb_ob_table['Side0_UniqueIds_OB'] == '', 'Side0_UniqueIds'] = fun_final_smb_or_umb_ob_table[Side0_UniqueIds_col_name]\n",
    "#    fun_final_smb_or_umb_ob_table.iloc[fun_final_smb_or_umb_ob_table['Side0_UniqueIds_OB'] != '', 'Side0_UniqueIds'] = fun_final_smb_or_umb_ob_table['Side0_UniqueIds_OB'] + fun_final_smb_or_umb_ob_table[Side0_UniqueIds_col_name]\n",
    "#    fun_final_smb_or_umb_ob_table.iloc[fun_final_smb_or_umb_ob_table['Side1_UniqueIds_OB'] == '', 'Side1_UniqueIds'] = fun_final_smb_or_umb_ob_table[Side1_UniqueIds_col_name]\n",
    "#    fun_final_smb_or_umb_ob_table.iloc[fun_final_smb_or_umb_ob_table['Side1_UniqueIds_OB'] != '', 'Side1_UniqueIds'] = fun_final_smb_or_umb_ob_table['Side1_UniqueIds_OB'] + fun_final_smb_or_umb_ob_table[Side1_UniqueIds_col_name]\n",
    "\n",
    "    fun_final_smb_or_umb_ob_table.drop(['Side0_UniqueIds_OB','Side1_UniqueIds_OB',Side0_UniqueIds_col_name,Side1_UniqueIds_col_name], axis = 1, inplace = True)\n",
    "\n",
    "    return(fun_final_smb_or_umb_ob_table)\n",
    "\n",
    "\n",
    "date_numbers_list = [16]\n",
    "                     #2,3,4,\n",
    "                    # 7,8,9,10,11,\n",
    "                    # 14,15,16,17,18,\n",
    "                    # 21,22,23,24,25,\n",
    "                    # 28,29,30]\n",
    "\n",
    "client = 'OakTree'\n",
    "\n",
    "setup_code = '379'\n",
    "\n",
    "today = date.today()\n",
    "d1 = datetime.strptime(today.strftime(\"%Y-%m-%d\"),\"%Y-%m-%d\")\n",
    "desired_date = d1 - timedelta(days=4)\n",
    "desired_date_str = desired_date.strftime(\"%Y-%m-%d\")\n",
    "date_input = desired_date_str\n",
    "\n",
    "#filepaths_AUA = '//vitblrdevcons01/Raman  Strategy ML 2.0/All_Data/' + client + '/JuneData/AUA/AUACollections.AUA_HST_RecData_' + setup_code + '_' + str(date_input) + '.csv'\n",
    "#filepaths_MEO = '//vitblrdevcons01/Raman  Strategy ML 2.0/All_Data/' + client + '/JuneData/MEO/MeoCollections.MEO_HST_RecData_' + setup_code + '_' + str(date_input) + '.csv'\n",
    "filepaths_no_pair_id_data = '//vitblrdevcons01/Raman  Strategy ML 2.0/All_Data/' + client + '/UAT_Run/X_Test_' + setup_code + '/no_pair_ids_' + setup_code + '_' + str(date_input) + '.csv'\n",
    "filepaths_no_pair_id_no_data_warning = '//vitblrdevcons01/Raman  Strategy ML 2.0/All_Data/' + client + '/UAT_Run/X_Test_' + setup_code + '/WARNING_no_pair_ids_' + setup_code + str(date_input) + '.csv'\n",
    "\n",
    "\n",
    "mngdb_obj_1_for_reading_and_writing_in_uat_server = mngdb(param_without_ssh  = True, param_without_RabbitMQ_pipeline = True,\n",
    "                 param_SSH_HOST = None, param_SSH_PORT = None,\n",
    "                 param_SSH_USERNAME = None, param_SSH_PASSWORD = None,\n",
    "                 param_MONGO_HOST = '10.1.15.137', param_MONGO_PORT = 27017,\n",
    "                 param_MONGO_USERNAME = 'mongouseradmin', param_MONGO_PASSWORD = '@L0ck&Key')\n",
    "mngdb_obj_1_for_reading_and_writing_in_uat_server.connect_with_or_without_ssh()\n",
    "db_1_for_MEO_data = mngdb_obj_1_for_reading_and_writing_in_uat_server.client['ReconDB_ML']\n",
    "\n",
    "\n",
    "query_1_for_MEO_data = db_1_for_MEO_data['RecData_' + setup_code].find({ \n",
    "                                                                     \"LastPerformedAction\": 31\n",
    "                                                             },\n",
    "                                                             {\n",
    "                                                                     \"DataSides\" : 1,\n",
    "                                                                     \"BreakID\" : 1,\n",
    "                                                                     \"LastPerformedAction\" : 1,\n",
    "                                                                     \"TaskInstanceID\" : 1,\n",
    "                                                                     \"SourceCombinationCode\" : 1,\n",
    "                                                                     \"MetaData\" : 1, \n",
    "                                                                     \"ViewData\" : 1\n",
    "                                                             })\n",
    "list_of_dicts_query_result_1 = list(query_1_for_MEO_data)\n",
    "\n",
    "meo_df = json_normalize(list_of_dicts_query_result_1)\n",
    "meo_df = meo_df.loc[:,meo_df.columns.str.startswith('ViewData')]\n",
    "meo_df['ViewData.Task Business Date'] = meo_df['ViewData.Task Business Date'].apply(dt.datetime.isoformat) \n",
    "print(meo_df.shape[0])\n",
    "meo_df.drop_duplicates(keep=False, inplace = True)\n",
    "meo_df = normalize_bp_acct_col_names(fun_df = meo_df)\n",
    "\n",
    "#Change added on 17-12-2020 to remove records with multiple values of Side0 and Side1 UniqueIds for statuses like OB,UOB,SDB,CNF and CMF. Typically, these statuses should have single values in Side0 and Side1 UniqueIds. So records not following expected behviour are removed\n",
    "\n",
    "meo_df['remove_or_keep_for_multiple_uniqueids_in_ob_issue'] = meo_df.apply(lambda row : contains_multiple_values_in_either_Side_0_or_1_UniqueIds_for_expected_single_sided_status(fun_row = row), axis = 1,result_type=\"expand\")\n",
    "meo_df = meo_df[~(meo_df['remove_or_keep_for_multiple_uniqueids_in_ob_issue'] == 'remove')]\n",
    "\n",
    "meo = meo_df[new_cols]\n",
    "print('meo size')\n",
    "print(meo.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BreakID_from_list_of_Side_01_UniqueIds(fun_str_list_Side_01_UniqueIds, fun_meo_df, fun_side_0_or_1):\n",
    "    list_BreakID_corresponding_to_Side_01_UniqueIds = []\n",
    "    print(fun_str_list_Side_01_UniqueIds)\n",
    "    for str_element_Side_01_UniqueIds in fun_str_list_Side_01_UniqueIds:\n",
    "        if(fun_side_0_or_1 == 0):\n",
    "            element_BreakID_corresponding_to_Side_01_UniqueIds = fun_meo_df[fun_meo_df['ViewData.Side0_UniqueIds'].isin([str_element_Side_01_UniqueIds])]['ViewData.BreakID'].unique()\n",
    "            list_BreakID_corresponding_to_Side_01_UniqueIds.append(element_BreakID_corresponding_to_Side_01_UniqueIds[0])\n",
    "        elif(fun_side_0_or_1 == 1):\n",
    "            element_BreakID_corresponding_to_Side_01_UniqueIds = fun_meo_df[fun_meo_df['ViewData.Side1_UniqueIds'].isin([str_element_Side_01_UniqueIds])]['ViewData.BreakID'].unique()\n",
    "            list_BreakID_corresponding_to_Side_01_UniqueIds.append(element_BreakID_corresponding_to_Side_01_UniqueIds[0])\n",
    "    return(list_BreakID_corresponding_to_Side_01_UniqueIds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_non_null_value(string_of_values_separated_by_comma):\n",
    "    if(string_of_values_separated_by_comma != '' and string_of_values_separated_by_comma != 'nan' and string_of_values_separated_by_comma != 'None' ):\n",
    "        if(string_of_values_separated_by_comma.partition(',')[0] != '' and string_of_values_separated_by_comma.partition(',')[0] != 'nan' and string_of_values_separated_by_comma.partition(',')[0] != 'None'):\n",
    "            return(string_of_values_separated_by_comma.partition(',')[0])\n",
    "        else:\n",
    "            return(get_first_non_null_value(string_of_values_separated_by_comma.partition(',')[2]))\n",
    "    else:\n",
    "        return('Blank value')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meo_df_taskids = list(meo_df['ViewData.Task ID'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change made on 12-12-2020 as per Pratik to catch instances where a single SMB pairs off with a single OB. BreakIDs caught in this code piece will be removed from propogating down further. Also, these BreakIDs will be given the status of UMR with Predicted_action of UMR_One-Many_to_Many-One\n",
    "#Begin change code made on 12-12-2020\n",
    "meo2 = meo[meo['ViewData.Status'].isin(['OB','SMB','SPM','UMB'])]\n",
    "meo2 = meo2.reset_index().drop('index',1)\n",
    "\n",
    "meo2['ViewData.Net Amount Difference Absolute'] = np.round(meo2['ViewData.Net Amount Difference Absolute'],2)\n",
    "\n",
    "abs_amount_count = meo2['ViewData.Net Amount Difference Absolute'].value_counts().reset_index()\n",
    "\n",
    "duplicate_amount = abs_amount_count[abs_amount_count['ViewData.Net Amount Difference Absolute']==2]\n",
    "duplicate_amount.columns = ['ViewData.Net Amount Difference Absolute','count']\n",
    "duplicate_amount = duplicate_amount.reset_index().drop('index',1)\n",
    "\n",
    "if duplicate_amount.shape[0]>0:\n",
    "    meo3 = meo2[meo2['ViewData.Net Amount Difference Absolute'].isin(duplicate_amount['ViewData.Net Amount Difference Absolute'].unique())]\n",
    "    meo3 = meo3.reset_index().drop('index',1)\n",
    "    meo3 = meo3.sort_values(by='ViewData.Net Amount Difference Absolute')\n",
    "    meo3 = meo3.reset_index().drop('index',1)\n",
    "    \n",
    "    smb_amount = meo3[meo3['ViewData.Status'].isin(['SMB'])]['ViewData.Net Amount Difference Absolute'].unique()\n",
    "    umb_amount = meo3[meo3['ViewData.Status'].isin(['UMB'])]['ViewData.Net Amount Difference Absolute'].unique()\n",
    "    \n",
    "    smb_ob_table = meo3[meo3['ViewData.Net Amount Difference Absolute'].isin(smb_amount)]\n",
    "    umb_ob_table = meo3[meo3['ViewData.Net Amount Difference Absolute'].isin(umb_amount)]\n",
    "    \n",
    "    ob_breakid = []\n",
    "    smb_breakid = []\n",
    "    for amount in smb_amount:\n",
    "        ob = smb_ob_table[(smb_ob_table['ViewData.Net Amount Difference Absolute']==amount) & (smb_ob_table['ViewData.Status']=='OB')]\n",
    "        smb = smb_ob_table[(smb_ob_table['ViewData.Net Amount Difference Absolute']==amount) & (smb_ob_table['ViewData.Status']=='SMB')]\n",
    "#         if((ob.shape[0]==1) and (smb.shape[0]==1) and (ob['ViewData.Mapped Custodian Account'] == smb['ViewData.Mapped Custodian Account']) and (ob['ViewData.Currency'] == smb['ViewData.Currency']) and (ob['ViewData.Source Combination Code'] == smb['ViewData.Source Combination Code'])):\n",
    "\n",
    "        if ob.shape[0]==1 and smb.shape[0]==1 :\n",
    "#Change added on 17-12-2020 by Rohit to include filter on ob and smb. Below if statement is commented out and new if statement is included\n",
    "            if((ob['ViewData.Mapped Custodian Account'].iloc[0] == smb['ViewData.Mapped Custodian Account'].iloc[0]) and (ob['ViewData.Currency'].iloc[0] == smb['ViewData.Currency'].iloc[0]) and (ob['ViewData.Source Combination Code'].iloc[0] == smb['ViewData.Source Combination Code'].iloc[0])):\n",
    "\n",
    "                ob_breakid.append(ob['ViewData.BreakID'].values)\n",
    "                smb_breakid.append(smb['ViewData.BreakID'].values)\n",
    "            \n",
    "    if len(ob_breakid)>0:\n",
    "        final_smb_ob_table = pd.DataFrame(ob_breakid)\n",
    "        final_smb_ob_table.columns = ['BreakID_OB']\n",
    "        final_smb_ob_table['BreakID_SMB'] = smb_breakid\n",
    "        final_smb_ob_table['BreakID_SMB'] = final_smb_ob_table['BreakID_SMB'].apply(lambda x: str(x).replace(\"[\",''))\n",
    "        final_smb_ob_table['BreakID_SMB'] = final_smb_ob_table['BreakID_SMB'].apply(lambda x: str(x).replace(\"]\",''))\n",
    "        final_smb_ob_table['BreakID_SMB'] = final_smb_ob_table['BreakID_SMB'].astype(int)\n",
    "    else:\n",
    "        final_smb_ob_table = pd.DataFrame()\n",
    "else:\n",
    "    final_smb_ob_table = pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_smb_ob_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove BreakIDs caught in final_smb_ob_table if final_smb_ob_table is not null\n",
    "if(final_smb_ob_table.shape[0] != 0):\n",
    "    final_smb_ob_table['BreakID_SMB'] = final_smb_ob_table['BreakID_SMB'].astype(np.int64)\n",
    "    final_smb_ob_table['BreakID_OB'] = final_smb_ob_table['BreakID_OB'].astype(np.int64)\n",
    "    \n",
    "    final_smb_ob_table_BreakID_list =  list(final_smb_ob_table['BreakID_OB']) + list(final_smb_ob_table['BreakID_SMB'])\n",
    "    meo = meo[~meo['ViewData.BreakID'].isin(final_smb_ob_table_BreakID_list)]\n",
    "else:\n",
    "    final_smb_ob_table_BreakID_list = []\n",
    "#End change code made on 12-12-2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change made on 12-12-2020 to incorporate final_smb_ob_table. The BreakIDs in this table will be given the Predicted_Status of UMR and Predicted_action of UMR_One-Many_to_Many-One\n",
    "#Begin code change made on 12-12-2020 to incorporate final_smb_ob_table\n",
    "#final_smb_ob_table\n",
    "if(final_smb_ob_table.shape[0] != 0):\n",
    "\n",
    "    final_smb_ob_table_copy = pd.merge(final_smb_ob_table,meo_df[['ViewData.BreakID','ViewData.Task ID','ViewData.Task Business Date','ViewData.Source Combination Code']].drop_duplicates(), left_on = 'BreakID_OB',right_on = 'ViewData.BreakID', how='left')\n",
    "    final_smb_ob_table_copy.drop('ViewData.BreakID', axis = 1, inplace = True)\n",
    "    \n",
    "    final_smb_ob_table_copy['Predicted_Status'] = 'UMR'\n",
    "    final_smb_ob_table_copy['Predicted_action'] = 'UMR_One-Many_to_Many-One'\n",
    "    final_smb_ob_table_copy['ML_flag'] = 'ML'\n",
    "    final_smb_ob_table_copy['SetupID'] = setup_code \n",
    "    final_smb_ob_table_copy['ViewData.Task Business Date'] = pd.to_datetime(final_smb_ob_table_copy['ViewData.Task Business Date'])\n",
    "    final_smb_ob_table_copy['ViewData.Task Business Date'] = final_smb_ob_table_copy['ViewData.Task Business Date'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "    final_smb_ob_table_copy['ViewData.Task Business Date'] = pd.to_datetime(final_smb_ob_table_copy['ViewData.Task Business Date'])\n",
    "    final_smb_ob_table_copy = make_Side0_Side1_columns_for_final_smb_or_umb_ob_table(final_smb_ob_table_copy,meo_df, 'SMB')\n",
    "    final_smb_ob_table_copy['probability_No_pair'] = ''\n",
    "    final_smb_ob_table_copy['probability_UMB'] = ''\n",
    "    final_smb_ob_table_copy['probability_UMR'] = ''\n",
    "    final_smb_ob_table_copy['probability_UMT'] = ''\n",
    "    final_smb_ob_table_copy['PredictedComment'] = ''\n",
    "    final_smb_ob_table_copy['PredictedCategory'] = ''\n",
    "    columns_rename_for_smb_ob_table_dict = {'BreakID_OB' : 'BreakID',\n",
    "                                       'BreakID_SMB' : 'Final_predicted_break',\n",
    "                                       'ViewData.Task ID' : 'TaskID',\n",
    "                                       'ViewData.Task Business Date' : 'BusinessDate',\n",
    "                                       'ViewData.Source Combination Code' : 'SourceCombinationCode'\n",
    "                                       }\n",
    "    final_smb_ob_table_copy.rename(columns = columns_rename_for_smb_ob_table_dict, inplace = True)\n",
    "    filepaths_final_smb_ob_table_copy = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_smb_ob_table_copy_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "\n",
    "    final_smb_ob_table_copy.to_csv(filepaths_final_smb_ob_table_copy)\n",
    "\n",
    "\n",
    "else:\n",
    "    final_smb_ob_table_copy = pd.DataFrame()\n",
    "#End code change made on 12-12-2020 to incorporate final_smb_ob_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_breakid = []\n",
    "umb_breakid = []\n",
    "for amount in umb_amount:\n",
    "    ob = umb_ob_table[(umb_ob_table['ViewData.Net Amount Difference Absolute']==amount) & (umb_ob_table['ViewData.Status']=='OB')]\n",
    "    umb = umb_ob_table[(umb_ob_table['ViewData.Net Amount Difference Absolute']==amount) & (umb_ob_table['ViewData.Status']=='UMB')]\n",
    "#         if((ob.shape[0]==1) and (smb.shape[0]==1) and (ob['ViewData.Mapped Custodian Account'] == smb['ViewData.Mapped Custodian Account']) and (ob['ViewData.Currency'] == smb['ViewData.Currency']) and (ob['ViewData.Source Combination Code'] == smb['ViewData.Source Combination Code'])):\n",
    "\n",
    "    if ob.shape[0]==1 and umb.shape[0]==1 :\n",
    "#Change added on 17-12-2020 by Rohit to include filter on ob and smb. Below if statement is commented out and new if statement is included\n",
    "        if((ob['ViewData.Mapped Custodian Account'].iloc[0] == umb['ViewData.Mapped Custodian Account'].iloc[0]) and (ob['ViewData.Currency'].iloc[0] == umb['ViewData.Currency'].iloc[0]) and (ob['ViewData.Source Combination Code'].iloc[0] == umb['ViewData.Source Combination Code'].iloc[0])):\n",
    "\n",
    "            ob_breakid.append(ob['ViewData.BreakID'].values)\n",
    "            umb_breakid.append(umb['ViewData.BreakID'].values)\n",
    "            \n",
    "if len(ob_breakid)>0:\n",
    "    final_umb_ob_table = pd.DataFrame(ob_breakid)\n",
    "    final_umb_ob_table.columns = ['BreakID_OB']\n",
    "    final_umb_ob_table['BreakID_UMB'] = umb_breakid\n",
    "    final_umb_ob_table['BreakID_UMB'] = final_umb_ob_table['BreakID_UMB'].apply(lambda x: str(x).replace(\"[\",''))\n",
    "    final_umb_ob_table['BreakID_UMB'] = final_umb_ob_table['BreakID_UMB'].apply(lambda x: str(x).replace(\"]\",''))\n",
    "    final_umb_ob_table['BreakID_UMB'] = final_umb_ob_table['BreakID_UMB'].astype(int)\n",
    "else:\n",
    "    final_umb_ob_table = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BreakID_OB</th>\n",
       "      <th>BreakID_UMB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1672099615</td>\n",
       "      <td>1649155977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BreakID_OB  BreakID_UMB\n",
       "0  1672099615   1649155977"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_umb_ob_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove BreakIDs caught in final_umb_ob_table if final_umb_ob_table is not null\n",
    "if(final_umb_ob_table.shape[0] != 0):\n",
    "    final_umb_ob_table['BreakID_UMB'] = final_umb_ob_table['BreakID_UMB'].astype(np.int64)\n",
    "    final_umb_ob_table['BreakID_OB'] = final_umb_ob_table['BreakID_OB'].astype(np.int64)\n",
    "    \n",
    "    final_umb_ob_table_BreakID_list =  list(final_umb_ob_table['BreakID_OB']) + list(final_umb_ob_table['BreakID_UMB'])\n",
    "    meo = meo[~meo['ViewData.BreakID'].isin(final_umb_ob_table_BreakID_list)]\n",
    "else:\n",
    "    final_umb_ob_table_BreakID_list = []\n",
    "#End change code made on 12-12-2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change made on 20-12-2020 to incorporate final_umb_ob_table. The BreakIDs in this table will be given the Predicted_Status of UMR and Predicted_action of UMR_One-Many_to_Many-One\n",
    "#Begin code change made on 20-12-2020 to incorporate final_umb_ob_table\n",
    "#final_umb_ob_table\n",
    "if(final_umb_ob_table.shape[0] != 0):\n",
    "\n",
    "    final_umb_ob_table_copy = pd.merge(final_umb_ob_table,meo_df[['ViewData.BreakID','ViewData.Task ID','ViewData.Task Business Date','ViewData.Source Combination Code']].drop_duplicates(), left_on = 'BreakID_OB',right_on = 'ViewData.BreakID', how='left')\n",
    "    final_umb_ob_table_copy.drop('ViewData.BreakID', axis = 1, inplace = True)\n",
    "    \n",
    "    final_umb_ob_table_copy['Predicted_Status'] = 'UMR'\n",
    "    final_umb_ob_table_copy['Predicted_action'] = 'UMR_One-Many_to_Many-One'\n",
    "    final_umb_ob_table_copy['ML_flag'] = 'ML'\n",
    "    final_umb_ob_table_copy['SetupID'] = setup_code \n",
    "    final_umb_ob_table_copy['ViewData.Task Business Date'] = pd.to_datetime(final_umb_ob_table_copy['ViewData.Task Business Date'])\n",
    "    final_umb_ob_table_copy['ViewData.Task Business Date'] = final_umb_ob_table_copy['ViewData.Task Business Date'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "    final_umb_ob_table_copy['ViewData.Task Business Date'] = pd.to_datetime(final_umb_ob_table_copy['ViewData.Task Business Date'])\n",
    "    final_umb_ob_table_copy = make_Side0_Side1_columns_for_final_smb_or_umb_ob_table(final_umb_ob_table_copy,meo_df,'UMB')\n",
    "    final_umb_ob_table_copy['probability_No_pair'] = ''\n",
    "    final_umb_ob_table_copy['probability_UMB'] = ''\n",
    "    final_umb_ob_table_copy['probability_UMR'] = ''\n",
    "    final_umb_ob_table_copy['probability_UMT'] = ''\n",
    "    final_umb_ob_table_copy['PredictedComment'] = ''\n",
    "    final_umb_ob_table_copy['PredictedCategory'] = ''\n",
    "    columns_rename_for_umb_ob_table_dict = {'BreakID_OB' : 'BreakID',\n",
    "                                       'BreakID_UMB' : 'Final_predicted_break',\n",
    "                                       'ViewData.Task ID' : 'TaskID',\n",
    "                                       'ViewData.Task Business Date' : 'BusinessDate',\n",
    "                                       'ViewData.Source Combination Code' : 'SourceCombinationCode'\n",
    "                                       }\n",
    "    final_umb_ob_table_copy.rename(columns = columns_rename_for_umb_ob_table_dict, inplace = True)\n",
    "    filepaths_final_umb_ob_table_copy = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_umb_ob_table_copy_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "\n",
    "    final_umb_ob_table_copy.to_csv(filepaths_final_umb_ob_table_copy)\n",
    "\n",
    "\n",
    "else:\n",
    "    final_umb_ob_table_copy = pd.DataFrame()\n",
    "#End code change made on 12-12-2020 to incorporate final_smb_ob_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BreakID</th>\n",
       "      <th>Final_predicted_break</th>\n",
       "      <th>TaskID</th>\n",
       "      <th>BusinessDate</th>\n",
       "      <th>SourceCombinationCode</th>\n",
       "      <th>Predicted_Status</th>\n",
       "      <th>Predicted_action</th>\n",
       "      <th>ML_flag</th>\n",
       "      <th>SetupID</th>\n",
       "      <th>Side0_UniqueIds</th>\n",
       "      <th>Side1_UniqueIds</th>\n",
       "      <th>probability_No_pair</th>\n",
       "      <th>probability_UMB</th>\n",
       "      <th>probability_UMR</th>\n",
       "      <th>probability_UMT</th>\n",
       "      <th>PredictedComment</th>\n",
       "      <th>PredictedCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1672099615</td>\n",
       "      <td>1649155977</td>\n",
       "      <td>3791153165</td>\n",
       "      <td>2020-12-17 00:00:00+00:00</td>\n",
       "      <td>108916259</td>\n",
       "      <td>UMR</td>\n",
       "      <td>UMR_One-Many_to_Many-One</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "      <td>93_3791153165_Advent Geneva,89_3791125414_Adve...</td>\n",
       "      <td>213_3791135409_State Street</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      BreakID  Final_predicted_break      TaskID              BusinessDate  \\\n",
       "0  1672099615             1649155977  3791153165 2020-12-17 00:00:00+00:00   \n",
       "\n",
       "  SourceCombinationCode Predicted_Status          Predicted_action ML_flag  \\\n",
       "0             108916259              UMR  UMR_One-Many_to_Many-One      ML   \n",
       "\n",
       "  SetupID                                    Side0_UniqueIds  \\\n",
       "0     379  93_3791153165_Advent Geneva,89_3791125414_Adve...   \n",
       "\n",
       "               Side1_UniqueIds probability_No_pair probability_UMB  \\\n",
       "0  213_3791135409_State Street                                       \n",
       "\n",
       "  probability_UMR probability_UMT PredictedComment PredictedCategory  \n",
       "0                                                                     "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_umb_ob_table_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViewData.B-P Net Amount</th>\n",
       "      <th>ViewData.Accounting Net Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>NaN</td>\n",
       "      <td>963.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ViewData.B-P Net Amount  ViewData.Accounting Net Amount\n",
       "655                      NaN                          963.33"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meo_df[meo_df['ViewData.BreakID'] == 1672099615][['ViewData.B-P Net Amount','ViewData.Accounting Net Amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViewData.B-P Net Amount</th>\n",
       "      <th>ViewData.Accounting Net Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1477.47</td>\n",
       "      <td>514.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ViewData.B-P Net Amount  ViewData.Accounting Net Amount\n",
       "280                  1477.47                          514.14"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meo_df[meo_df['ViewData.BreakID'] == 1649155977][['ViewData.B-P Net Amount','ViewData.Accounting Net Amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "963.33"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1477.47 - 514.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViewData.Task ID</th>\n",
       "      <th>ViewData.Source Combination Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>3791153165</td>\n",
       "      <td>108916259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ViewData.Task ID ViewData.Source Combination Code\n",
       "655        3791153165                        108916259"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meo_df[meo_df['ViewData.BreakID'] == 1672099615][['ViewData.Task ID','ViewData.Source Combination Code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change made on 20-12-2020. UMB_Carry_forward df used to be taken out from meo_df, but as per Pratik, it will now be taken out from meo where smb-ob and umb-ob break ids have already been taken out from meo\n",
    "umb_carry_forward_df = meo[meo['ViewData.Status'] == 'UMB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Side_0_1_UniqueIds_closed_all_dates_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:293: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ViteosModel\n",
      "D:\\ViteosModel\\Closed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 0\n",
    "for i in range(0,len(date_numbers_list)):\n",
    "\n",
    "    Side_0_1_UniqueIds_closed_all_dates_list.append(\n",
    "            closed_daily_run(fun_setup_code=setup_code,\\\n",
    "                             fun_date = i,\\\n",
    "                             fun_meo_df_daily_run = meo)\n",
    "#                             fun_main_filepath_meo= filepaths_MEO[i],\\\n",
    "#                             fun_main_filepath_aua = filepaths_AUA[i])\n",
    "            )\n",
    "\n",
    "new_closed_keys = [i.replace('nan','') for i in Side_0_1_UniqueIds_closed_all_dates_list[0]]\n",
    "new_closed_keys = [i.replace('None','') for i in new_closed_keys]\n",
    "\n",
    "\n",
    "df1 = meo[~meo['ViewData.Status'].isin(['SMT','HST', 'OC', 'CT', 'Archive','SMR'])]\n",
    "#df = df[df['MatchStatus'] != 21]\n",
    "df1 = df1[~df1['ViewData.Status'].isnull()]\n",
    "df1 = df1.reset_index()\n",
    "df1 = df1.drop('index',1)\n",
    "\n",
    "## Output for Closed breaks\n",
    "\n",
    "closed_df_side1 = df1[df1['ViewData.Side1_UniqueIds'].isin(new_closed_keys)]\n",
    "closed_df_side0 = df1[df1['ViewData.Side0_UniqueIds'].isin(new_closed_keys)]\n",
    "closed_df = closed_df_side1.append(closed_df_side0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Date value count is:\n",
      "2020-12-17    747\n",
      "2019-04-22      1\n",
      "2020-03-04      1\n",
      "Name: Date, dtype: int64\n",
      "Choosing the date : 2020-12-17\n"
     ]
    }
   ],
   "source": [
    "df2 = df1[~((df1['ViewData.Side1_UniqueIds'].isin(new_closed_keys)) | (df1['ViewData.Side0_UniqueIds'].isin(new_closed_keys)))]\n",
    "\n",
    "df = df2.copy()\n",
    "df = df.reset_index()\n",
    "df = df.drop('index',1)\n",
    "df['Date'] = pd.to_datetime(df['ViewData.Task Business Date'])\n",
    "df = df[~df['Date'].isnull()]\n",
    "df = df.reset_index()\n",
    "df = df.drop('index',1)\n",
    "\n",
    "pd.to_datetime(df['Date'])\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "\n",
    "df['Date'] = df['Date'].astype(str)\n",
    "\n",
    "df = df[df['ViewData.Status'].isin(['OB','SDB','UOB','UDB','CMF','CNF','SMB','SPM'])]\n",
    "df = df.reset_index()\n",
    "df = df.drop('index',1)\n",
    "df['ViewData.Side0_UniqueIds'] = df['ViewData.Side0_UniqueIds'].astype(str)\n",
    "df['ViewData.Side1_UniqueIds'] = df['ViewData.Side1_UniqueIds'].astype(str)\n",
    "df['flag_side0'] = df.apply(lambda x: len(x['ViewData.Side0_UniqueIds'].split(',')), axis=1)\n",
    "df['flag_side1'] = df.apply(lambda x: len(x['ViewData.Side1_UniqueIds'].split(',')), axis=1)\n",
    "df = df.rename(columns= {'ViewData.Cust Net Amount':'ViewData.B-P Net Amount'})\n",
    "\n",
    "print('The Date value count is:')\n",
    "print(df['Date'].value_counts())\n",
    "\n",
    "date_i = df['Date'].mode()[0]\n",
    "\n",
    "print('Choosing the date : ' + date_i)\n",
    "\n",
    "sample = df[df['Date'] == date_i]\n",
    "sample = sample.reset_index()\n",
    "sample = sample.drop('index',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "smb = sample[sample['ViewData.Status']=='SMB'].reset_index()\n",
    "smb = smb.drop('index',1)\n",
    "smb_pb = smb.copy()\n",
    "smb_acc = smb.copy()\n",
    "smb_pb['ViewData.Accounting Net Amount'] = np.nan\n",
    "smb_pb['ViewData.Side0_UniqueIds'] = np.nan\n",
    "smb_pb['ViewData.Status'] ='SMB-OB'\n",
    "\n",
    "smb_acc['ViewData.B-P Net Amount'] = np.nan\n",
    "smb_acc['ViewData.Side1_UniqueIds'] = np.nan\n",
    "smb_acc['ViewData.Status'] ='SMB-OB'\n",
    "\n",
    "sample = sample[sample['ViewData.Status']!='SMB']\n",
    "sample = sample.reset_index()\n",
    "sample = sample.drop('index',1)\n",
    "\n",
    "sample = pd.concat([sample,smb_pb,smb_acc],axis=0)\n",
    "sample = sample.reset_index()\n",
    "sample = sample.drop('index',1)\n",
    "\n",
    "sample['ViewData.Side0_UniqueIds'] = sample['ViewData.Side0_UniqueIds'].astype(str)\n",
    "sample['ViewData.Side1_UniqueIds'] = sample['ViewData.Side1_UniqueIds'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 70)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[sample['ViewData.Side1_UniqueIds']=='nan'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373, 70)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[sample['ViewData.Side0_UniqueIds']=='None'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 70)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[sample['ViewData.Side0_UniqueIds']=='NaN'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 70)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[sample['ViewData.Side0_UniqueIds']==''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.loc[sample['ViewData.Side0_UniqueIds']=='nan','flag_side0'] = 0\n",
    "sample.loc[sample['ViewData.Side1_UniqueIds']=='nan','flag_side1'] = 0\n",
    "\n",
    "sample.loc[sample['ViewData.Side0_UniqueIds']=='None','flag_side0'] = 0\n",
    "sample.loc[sample['ViewData.Side1_UniqueIds']=='None','flag_side1'] = 0\n",
    "\n",
    "\n",
    "sample.loc[sample['ViewData.Side0_UniqueIds']=='','flag_side0'] = 0\n",
    "sample.loc[sample['ViewData.Side1_UniqueIds']=='','flag_side1'] = 0\n",
    "\n",
    "sample.loc[sample['ViewData.Side1_UniqueIds']=='nan','Trans_side'] = 'B_side'\n",
    "sample.loc[sample['ViewData.Side0_UniqueIds']=='nan','Trans_side'] = 'A_side'\n",
    "\n",
    "sample.loc[sample['ViewData.Side1_UniqueIds']=='None','Trans_side'] = 'B_side'\n",
    "sample.loc[sample['ViewData.Side0_UniqueIds']=='None','Trans_side'] = 'A_side'\n",
    "\n",
    "sample.loc[sample['ViewData.Side1_UniqueIds']=='','Trans_side'] = 'B_side'\n",
    "sample.loc[sample['ViewData.Side0_UniqueIds']=='','Trans_side'] = 'A_side'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "sample.loc[sample['Trans_side']=='A_side','ViewData.B-P Currency'] = sample.loc[sample['Trans_side']=='A_side','ViewData.Currency']\n",
    "sample.loc[sample['Trans_side']=='B_side','ViewData.Accounting Currency'] = sample.loc[sample['Trans_side']=='B_side','ViewData.Currency'] \n",
    "\n",
    "sample['ViewData.B-P Currency'] = sample['ViewData.B-P Currency'].astype(str)\n",
    "sample['ViewData.Accounting Currency'] = sample['ViewData.Accounting Currency'].astype(str)\n",
    "sample['ViewData.Mapped Custodian Account'] = sample['ViewData.Mapped Custodian Account'].astype(str)\n",
    "sample['filter_key'] = sample.apply(lambda x: x['ViewData.Mapped Custodian Account'] + x['ViewData.B-P Currency'] if x['Trans_side']=='A_side' else x['ViewData.Mapped Custodian Account'] + x['ViewData.Accounting Currency'], axis=1)\n",
    "\n",
    "sample1 = sample[(sample['flag_side0']<=1) & (sample['flag_side1']<=1) & (sample['ViewData.Status'].isin(['OB','SPM','SDB','UDB','UOB','SMB-OB','CNF','CMF']))]\n",
    "\n",
    "sample1 = sample1.reset_index()\n",
    "sample1 = sample1.drop('index', 1)\n",
    "\n",
    "sample1['ViewData.BreakID'] = sample1['ViewData.BreakID'].astype(int)\n",
    "\n",
    "sample1 = sample1[sample1['ViewData.BreakID']!=-1]\n",
    "sample1 = sample1.reset_index()\n",
    "sample1 = sample1.drop('index',1)\n",
    "\n",
    "sample1 = sample1.sort_values(['ViewData.BreakID','Date'], ascending =[True, False])\n",
    "sample1 = sample1.reset_index()\n",
    "sample1 = sample1.drop('index',1)\n",
    "\n",
    "aa = sample1[sample1['Trans_side']=='A_side']\n",
    "bb = sample1[sample1['Trans_side']=='B_side']\n",
    "\n",
    "aa['filter_key'] = aa['ViewData.Source Combination'].astype(str) + aa['ViewData.Mapped Custodian Account'].astype(str) + aa['ViewData.B-P Currency'].astype(str)\n",
    "\n",
    "bb['filter_key'] = bb['ViewData.Source Combination'].astype(str) + bb['ViewData.Mapped Custodian Account'].astype(str) + bb['ViewData.Accounting Currency'].astype(str)\n",
    "\n",
    "aa = aa.reset_index()\n",
    "aa = aa.drop('index', 1)\n",
    "bb = bb.reset_index()\n",
    "bb = bb.drop('index', 1)\n",
    "\n",
    "bb = bb[~bb['ViewData.Accounting Net Amount'].isnull()]\n",
    "bb = bb.reset_index()\n",
    "bb = bb.drop('index',1)\n",
    "# Change made on 20-12-2020 as asked by Pratik. For this particular code piece, on a given settle date, some breaks in many_to_many category knock off and will be removed from further down the line\n",
    "# Begin change on 20-12-2020\n",
    "cc = pd.concat([aa, bb], axis=0)\n",
    "\n",
    "cc = cc.reset_index().drop('index',1)\n",
    "\n",
    "cc['ViewData.Transaction Type'] = cc['ViewData.Transaction Type'].astype(str)\n",
    "cc['ViewData.Settle Date'] = pd.to_datetime(cc['ViewData.Settle Date'])\n",
    "cc['filter_key_with_sd'] = cc['filter_key'].astype(str) + cc['ViewData.Settle Date'].astype(str)\n",
    "\n",
    "\n",
    "##########################\n",
    "cc7 = cc.copy()\n",
    "\n",
    "cc_new = cc7[cc7['ViewData.Status']!='SPM']\n",
    "cc_new = cc_new.reset_index().drop('index',1)\n",
    "\n",
    "#cc_new = cc_new[~((cc_new['ViewData.Side0_UniqueIds'].isin(final_umr_table['SideB.ViewData.Side0_UniqueIds'])) | (cc_new['ViewData.Side1_UniqueIds'].isin(final_umr_table['SideA.ViewData.Side1_UniqueIds'])))]\n",
    "\n",
    "#cc_new = cc_new[~((cc_new['ViewData.Side0_UniqueIds'].isin(final_umt_table['SideB.ViewData.Side0_UniqueIds'])) | (cc_new['ViewData.Side1_UniqueIds'].isin(final_umt_table['SideA.ViewData.Side1_UniqueIds'])))]\n",
    "\n",
    "cc_new = cc_new.reset_index().drop('index',1)\n",
    "\n",
    "filter_key_umt_umb_sd = []\n",
    "diff_in_amount_sd = []\n",
    "diff_in_amount_key_sd = []\n",
    "for key in cc_new['filter_key_with_sd'].unique():    \n",
    "    cc_dummy = cc_new[cc_new['filter_key_with_sd']==key]\n",
    "    if (-0.25<= cc_dummy['ViewData.Net Amount Difference'].sum() <=0.25) & (cc_dummy.shape[0]>2) & (cc_dummy['Trans_side'].nunique()>1):\n",
    "        #print(cc2_dummy.shape[0])\n",
    "        #print(key)\n",
    "        filter_key_umt_umb_sd.append(key)\n",
    "    else:\n",
    "        if (cc_dummy.shape[0]>2) & (cc_dummy['Trans_side'].nunique()>1):\n",
    "            diff = cc_dummy['ViewData.Net Amount Difference'].sum()\n",
    "            diff_in_amount_sd.append(diff)\n",
    "            diff_in_amount_key_sd.append(key)\n",
    "\n",
    "## Equity Swap Many to many\n",
    "\n",
    "sd_mtm_1_ids = []\n",
    "sd_mtm_0_ids = []\n",
    "\n",
    "for key in filter_key_umt_umb_sd:\n",
    "    one_side = cc_new[cc_new['filter_key_with_sd']== key]['ViewData.Side1_UniqueIds'].unique()\n",
    "    zero_side = cc_new[cc_new['filter_key_with_sd']== key]['ViewData.Side0_UniqueIds'].unique()\n",
    "    one_side = [i for i in one_side if i not in ['nan','None','']]\n",
    "    zero_side = [i for i in zero_side if i not in ['nan','None','']]\n",
    "    sd_mtm_1_ids.append(one_side)\n",
    "    sd_mtm_0_ids.append(zero_side)\n",
    "\n",
    "if sd_mtm_1_ids !=[]:\n",
    "    sd_mtm_list_1 = list(np.concatenate(sd_mtm_1_ids))\n",
    "else:\n",
    "    sd_mtm_list_1 = []\n",
    "\n",
    "if sd_mtm_0_ids !=[]:\n",
    "    sd_mtm_list_0 = list(np.concatenate(sd_mtm_0_ids))\n",
    "else:\n",
    "    sd_mtm_list_0 = []\n",
    "\n",
    "## Data Frame for MTM from equity Swap\n",
    "\n",
    "mtm_df_sd = pd.DataFrame(np.arange(len(sd_mtm_0_ids)))\n",
    "mtm_df_sd.columns = ['index']\n",
    "\n",
    "mtm_df_sd['ViewData.Side0_UniqueIds'] = sd_mtm_0_ids\n",
    "mtm_df_sd['ViewData.Side1_UniqueIds'] = sd_mtm_1_ids\n",
    "mtm_df_sd = mtm_df_sd.drop('index',1)\n",
    "\n",
    "# TODO: remove mtm_df_sd ids from aa and bb\n",
    "# TODO: include mtm_df_sd into final_df_2\n",
    "# End change on 20-12-2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['19_3791153165_Advent Geneva',\n",
       "  '133_3791153165_Advent Geneva',\n",
       "  '126_3791153165_Advent Geneva',\n",
       "  '20_3791153165_Advent Geneva',\n",
       "  '135_3791153165_Advent Geneva',\n",
       "  '21_3791153165_Advent Geneva',\n",
       "  '132_3791153165_Advent Geneva',\n",
       "  '128_3791153165_Advent Geneva',\n",
       "  '136_3791153165_Advent Geneva',\n",
       "  '127_3791153165_Advent Geneva',\n",
       "  '137_3791153165_Advent Geneva',\n",
       "  '134_3791153165_Advent Geneva'],\n",
       " ['124_3791153165_Advent Geneva',\n",
       "  '16_3791153165_Advent Geneva',\n",
       "  '13_3791153165_Advent Geneva',\n",
       "  '122_3791153165_Advent Geneva',\n",
       "  '123_3791153165_Advent Geneva',\n",
       "  '14_3791153165_Advent Geneva',\n",
       "  '121_3791153165_Advent Geneva',\n",
       "  '15_3791153165_Advent Geneva'],\n",
       " ['57_3791152753_Advent Geneva',\n",
       "  '53_3791152753_Advent Geneva',\n",
       "  '54_3791152753_Advent Geneva',\n",
       "  '56_3791152753_Advent Geneva',\n",
       "  '55_3791152753_Advent Geneva',\n",
       "  '142_3791152753_Advent Geneva',\n",
       "  '147_3791152753_Advent Geneva',\n",
       "  '148_3791152753_Advent Geneva',\n",
       "  '149_3791152753_Advent Geneva',\n",
       "  '146_3791152753_Advent Geneva',\n",
       "  '143_3791152753_Advent Geneva',\n",
       "  '145_3791152753_Advent Geneva'],\n",
       " ['6_3791152753_Advent Geneva',\n",
       "  '2_3791152753_Advent Geneva',\n",
       "  '5_3791152753_Advent Geneva',\n",
       "  '4_3791152753_Advent Geneva'],\n",
       " ['111_3791153165_Advent Geneva',\n",
       "  '108_3791153165_Advent Geneva',\n",
       "  '110_3791153165_Advent Geneva']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_mtm_0_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['105_3791151084_State Street',\n",
       "  '112_3791151084_State Street',\n",
       "  '110_3791151084_State Street',\n",
       "  '111_3791151084_State Street',\n",
       "  '116_3791151084_State Street',\n",
       "  '106_3791151084_State Street',\n",
       "  '115_3791151084_State Street',\n",
       "  '109_3791151084_State Street'],\n",
       " ['90_3791151084_State Street',\n",
       "  '101_3791151084_State Street',\n",
       "  '103_3791151084_State Street',\n",
       "  '104_3791151084_State Street'],\n",
       " ['12_3791152753_BNP Paribas',\n",
       "  '9_3791152753_BNP Paribas',\n",
       "  '11_3791152753_BNP Paribas',\n",
       "  '13_3791152753_BNP Paribas'],\n",
       " ['24_3791152753_BNP Paribas',\n",
       "  '21_3791152753_BNP Paribas',\n",
       "  '22_3791152753_BNP Paribas',\n",
       "  '23_3791152753_BNP Paribas'],\n",
       " ['195_3791153165_State Street',\n",
       "  '198_3791153165_State Street',\n",
       "  '197_3791153165_State Street']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_mtm_1_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViewData.Side0_UniqueIds</th>\n",
       "      <th>ViewData.Side1_UniqueIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[19_3791153165_Advent Geneva, 133_3791153165_A...</td>\n",
       "      <td>[105_3791151084_State Street, 112_3791151084_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[124_3791153165_Advent Geneva, 16_3791153165_A...</td>\n",
       "      <td>[90_3791151084_State Street, 101_3791151084_St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[57_3791152753_Advent Geneva, 53_3791152753_Ad...</td>\n",
       "      <td>[12_3791152753_BNP Paribas, 9_3791152753_BNP P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[6_3791152753_Advent Geneva, 2_3791152753_Adve...</td>\n",
       "      <td>[24_3791152753_BNP Paribas, 21_3791152753_BNP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[111_3791153165_Advent Geneva, 108_3791153165_...</td>\n",
       "      <td>[195_3791153165_State Street, 198_3791153165_S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ViewData.Side0_UniqueIds  \\\n",
       "0  [19_3791153165_Advent Geneva, 133_3791153165_A...   \n",
       "1  [124_3791153165_Advent Geneva, 16_3791153165_A...   \n",
       "2  [57_3791152753_Advent Geneva, 53_3791152753_Ad...   \n",
       "3  [6_3791152753_Advent Geneva, 2_3791152753_Adve...   \n",
       "4  [111_3791153165_Advent Geneva, 108_3791153165_...   \n",
       "\n",
       "                            ViewData.Side1_UniqueIds  \n",
       "0  [105_3791151084_State Street, 112_3791151084_S...  \n",
       "1  [90_3791151084_State Street, 101_3791151084_St...  \n",
       "2  [12_3791152753_BNP Paribas, 9_3791152753_BNP P...  \n",
       "3  [24_3791152753_BNP Paribas, 21_3791152753_BNP ...  \n",
       "4  [195_3791153165_State Street, 198_3791153165_S...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtm_df_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19_3791153165_Advent Geneva', '133_3791153165_Advent Geneva', '126_3791153165_Advent Geneva', '20_3791153165_Advent Geneva', '135_3791153165_Advent Geneva', '21_3791153165_Advent Geneva', '132_3791153165_Advent Geneva', '128_3791153165_Advent Geneva', '136_3791153165_Advent Geneva', '127_3791153165_Advent Geneva', '137_3791153165_Advent Geneva', '134_3791153165_Advent Geneva', '124_3791153165_Advent Geneva', '16_3791153165_Advent Geneva', '13_3791153165_Advent Geneva', '122_3791153165_Advent Geneva', '123_3791153165_Advent Geneva', '14_3791153165_Advent Geneva', '121_3791153165_Advent Geneva', '15_3791153165_Advent Geneva', '57_3791152753_Advent Geneva', '53_3791152753_Advent Geneva', '54_3791152753_Advent Geneva', '56_3791152753_Advent Geneva', '55_3791152753_Advent Geneva', '142_3791152753_Advent Geneva', '147_3791152753_Advent Geneva', '148_3791152753_Advent Geneva', '149_3791152753_Advent Geneva', '146_3791152753_Advent Geneva', '143_3791152753_Advent Geneva', '145_3791152753_Advent Geneva', '6_3791152753_Advent Geneva', '2_3791152753_Advent Geneva', '5_3791152753_Advent Geneva', '4_3791152753_Advent Geneva', '111_3791153165_Advent Geneva', '108_3791153165_Advent Geneva', '110_3791153165_Advent Geneva']\n"
     ]
    }
   ],
   "source": [
    "sd_mtm_0_ids_flat = []\n",
    "for sublist in sd_mtm_0_ids:\n",
    "#     print(sublist)\n",
    "    for item in sublist:\n",
    "#         print(item)\n",
    "        sd_mtm_0_ids_flat.append(item)\n",
    "# sd_mtm_0_ids_flat = [item for sublist in sd_mtm_0_ids for item in sublist]\n",
    "print(sd_mtm_0_ids_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['105_3791151084_State Street', '112_3791151084_State Street', '110_3791151084_State Street', '111_3791151084_State Street', '116_3791151084_State Street', '106_3791151084_State Street', '115_3791151084_State Street', '109_3791151084_State Street', '90_3791151084_State Street', '101_3791151084_State Street', '103_3791151084_State Street', '104_3791151084_State Street', '12_3791152753_BNP Paribas', '9_3791152753_BNP Paribas', '11_3791152753_BNP Paribas', '13_3791152753_BNP Paribas', '24_3791152753_BNP Paribas', '21_3791152753_BNP Paribas', '22_3791152753_BNP Paribas', '23_3791152753_BNP Paribas', '195_3791153165_State Street', '198_3791153165_State Street', '197_3791153165_State Street']\n"
     ]
    }
   ],
   "source": [
    "sd_mtm_1_ids_flat = []\n",
    "for sublist in sd_mtm_1_ids:\n",
    "#     print(sublist)\n",
    "    for item in sublist:\n",
    "#         print(item)\n",
    "        sd_mtm_1_ids_flat.append(item)\n",
    "# sd_mtm_1_ids_flat = [item for sublist in sd_mtm_1_ids for item in sublist]\n",
    "print(sd_mtm_1_ids_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395, 74)\n"
     ]
    }
   ],
   "source": [
    "print(aa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(374, 74)\n"
     ]
    }
   ],
   "source": [
    "print(bb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(sd_mtm_1_ids_flat))\n",
    "print(len(sd_mtm_0_ids_flat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViewData.Currency</th>\n",
       "      <th>ViewData.Account Type</th>\n",
       "      <th>ViewData.Accounting Net Amount</th>\n",
       "      <th>ViewData.Task ID</th>\n",
       "      <th>ViewData.Source Combination Code</th>\n",
       "      <th>ViewData.Activity Code</th>\n",
       "      <th>ViewData.Age</th>\n",
       "      <th>ViewData.Age WK</th>\n",
       "      <th>ViewData.Asset Type Category</th>\n",
       "      <th>ViewData.Base Currency</th>\n",
       "      <th>...</th>\n",
       "      <th>ViewData.Side0_UniqueIds</th>\n",
       "      <th>ViewData.Side1_UniqueIds</th>\n",
       "      <th>ViewData.Task Business Date</th>\n",
       "      <th>Date</th>\n",
       "      <th>flag_side0</th>\n",
       "      <th>flag_side1</th>\n",
       "      <th>Trans_side</th>\n",
       "      <th>ViewData.B-P Currency</th>\n",
       "      <th>ViewData.Accounting Currency</th>\n",
       "      <th>filter_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ViewData.Currency, ViewData.Account Type, ViewData.Accounting Net Amount, ViewData.Task ID, ViewData.Source Combination Code, ViewData.Activity Code, ViewData.Age, ViewData.Age WK, ViewData.Asset Type Category, ViewData.Base Currency, ViewData.Base Net Amount, ViewData.Bloomberg_Yellow_Key, ViewData.B-P Net Amount, ViewData.BreakID, ViewData.Business Date, ViewData.Cancel Amount, ViewData.Cancel Flag, ViewData.CUSIP, ViewData.Custodian, ViewData.Custodian Account, ViewData.Derived Source, ViewData.Description, ViewData.Department, ViewData.ExpiryDate, ViewData.ExternalComment1, ViewData.ExternalComment2, ViewData.ExternalComment3, ViewData.Fund, ViewData.FX Rate, ViewData.Interest Amount, ViewData.InternalComment1, ViewData.InternalComment2, ViewData.InternalComment3, ViewData.Investment Type, ViewData.Is Combined Data, ViewData.ISIN, ViewData.Keys, ViewData.Mapped Custodian Account, ViewData.Net Amount Difference, ViewData.Net Amount Difference Absolute, ViewData.Non Trade Description, ViewData.OTE Custodian Account, ViewData.Price, ViewData.Prime Broker, ViewData.Quantity, ViewData.SEDOL, ViewData.Settle Date, ViewData.SPM ID, ViewData.Status, ViewData.Strike Price, ViewData.System Comments, ViewData.Ticker, ViewData.Trade Date, ViewData.Trade Expenses, ViewData.Transaction Category, ViewData.Transaction ID, ViewData.Transaction Type, ViewData.Underlying Cusip, ViewData.Underlying Investment ID, ViewData.Underlying ISIN, ViewData.Underlying Sedol, ViewData.Underlying Ticker, ViewData.Source Combination, ViewData._ID, ViewData.Side0_UniqueIds, ViewData.Side1_UniqueIds, ViewData.Task Business Date, Date, flag_side0, flag_side1, Trans_side, ViewData.B-P Currency, ViewData.Accounting Currency, filter_key]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 74 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa[aa['ViewData.Side0_UniqueIds'] == '135_3791153165_Advent Geneva']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372, 74)\n"
     ]
    }
   ],
   "source": [
    "aa = aa[~((aa['ViewData.Side0_UniqueIds'].isin(sd_mtm_0_ids_flat)) |(aa['ViewData.Side1_UniqueIds'].isin(sd_mtm_1_ids_flat)))]\n",
    "aa = aa.reset_index().drop('index',1)\n",
    "print(aa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335, 74)\n"
     ]
    }
   ],
   "source": [
    "bb = bb[~((bb['ViewData.Side0_UniqueIds'].isin(sd_mtm_0_ids_flat)) |(bb['ViewData.Side1_UniqueIds'].isin(sd_mtm_1_ids_flat)))]\n",
    "bb = bb.reset_index().drop('index',1)\n",
    "\n",
    "print(bb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### loop m*n ###############################\n",
    "\n",
    "pool =[]\n",
    "key_index =[]\n",
    "training_df =[]\n",
    "\n",
    "no_pair_ids = []\n",
    "#max_rows = 5\n",
    "\n",
    "for d in tqdm(aa['Date'].unique()):\n",
    "    aa1 = aa.loc[aa['Date']==d,:][common_cols]\n",
    "    bb1 = bb.loc[bb['Date']==d,:][common_cols]\n",
    "    \n",
    "    aa1 = aa1.reset_index()\n",
    "    aa1 = aa1.drop('index',1)\n",
    "    bb1 = bb1.reset_index()\n",
    "    bb1 = bb1.drop('index', 1)\n",
    "    \n",
    "    bb1 = bb1.sort_values(by='filter_key',ascending =True)\n",
    "    \n",
    "    for key in (list(np.unique(np.array(list(aa1['filter_key'].values) + list(bb1['filter_key'].values))))):\n",
    "        \n",
    "        df1 = aa1[aa1['filter_key']==key]\n",
    "        df2 = bb1[bb1['filter_key']==key]\n",
    "\n",
    "        if df1.empty == False and df2.empty == False:\n",
    "            #aa_df = pd.concat([aa1[aa1.index==i]]*repeat_num, ignore_index=True)\n",
    "            #bb_df = bb1.loc[pool[len(pool)-1],:][common_cols].reset_index()\n",
    "            #bb_df = bb_df.drop('index', 1)\n",
    "\n",
    "            df1 = df1.rename(columns={'ViewData.BreakID':'ViewData.BreakID_A_side'})\n",
    "            df2 = df2.rename(columns={'ViewData.BreakID':'ViewData.BreakID_B_side'})\n",
    "\n",
    "            #dff  = pd.concat([aa[aa.index==i],bb.loc[pool[i],:][accounting_vars]],axis=1)\n",
    "\n",
    "            df1 = df1.reset_index()\n",
    "            df2 = df2.reset_index()\n",
    "            df1 = df1.drop('index', 1)\n",
    "            df2 = df2.drop('index', 1)\n",
    "\n",
    "            df1.columns = ['SideA.' + x  for x in df1.columns] \n",
    "            df2.columns = ['SideB.' + x  for x in df2.columns]\n",
    "\n",
    "            df1 = df1.rename(columns={'SideA.filter_key':'filter_key'})\n",
    "            df2 = df2.rename(columns={'SideB.filter_key':'filter_key'})\n",
    "\n",
    "            #dff = pd.concat([aa_df,bb_df],axis=1)\n",
    "            dff = merge(df1, df2, on='filter_key')\n",
    "            training_df.append(dff)\n",
    "                #key_index.append(i)\n",
    "            #else:\n",
    "            #no_pair_ids.append([aa1[(aa1['filter_key']=='key') & (aa1['ViewData.Status'].isin(['OB','SDB']))]['ViewData.Side1_UniqueIds'].values[0]])\n",
    "               # no_pair_ids.append(aa1[(aa1['filter_key']== key) & (aa1['ViewData.Status'].isin(['OB','SDB']))]['ViewData.Side1_UniqueIds'].values[0])\n",
    "    \n",
    "        else:\n",
    "#Change made on 26-11-2020 to include CMF and CNF as well, as long as they are single sided. For now, we are assuming they are single sided and no other precaution has been made to explicitely include single sided CNF and CMF\n",
    "#Change made as per above and commenting below on 26-11-2020\n",
    "#            no_pair_ids.append([aa1[(aa1['filter_key']==key) & (aa1['ViewData.Status'].isin(['OB','SDB']))]['ViewData.Side1_UniqueIds'].values])\n",
    "#            no_pair_ids.append([bb1[(bb1['filter_key']==key) & (bb1['ViewData.Status'].isin(['OB','SDB']))]['ViewData.Side0_UniqueIds'].values])\n",
    "#Change made on 26-11-2020 to include CNF and CMF\n",
    "            no_pair_ids.append([aa1[(aa1['filter_key']==key) & (aa1['ViewData.Status'].isin(['OB','SDB','CNF','CMF']))]['ViewData.Side1_UniqueIds'].values])\n",
    "            no_pair_ids.append([bb1[(bb1['filter_key']==key) & (bb1['ViewData.Status'].isin(['OB','SDB','CNF','CMF']))]['ViewData.Side0_UniqueIds'].values])\n",
    "            \n",
    "\n",
    "\n",
    "if len(no_pair_ids) != 0:\n",
    "    no_pair_ids = np.unique(np.concatenate(no_pair_ids,axis=1)[0])\n",
    "    no_pair_ids_df = pd.DataFrame(no_pair_ids, columns = ['Side0_1_UniqueIds'])\n",
    "#    no_pair_ids_df = pd.merge(no_pair_ids_df, meo_df[['ViewData.Side1_UniqueIds','ViewData.BreakID','ViewData.Task ID','ViewData.Task Business Date']].drop_duplicates(), left_on = 'Side0_1_UniqueIds',right_on = 'ViewData.Side1_UniqueIds', how='left')\n",
    "#    no_pair_ids_df = pd.merge(no_pair_ids_df, meo_df[['ViewData.Side0_UniqueIds','ViewData.BreakID','ViewData.Task ID','ViewData.Task Business Date']].drop_duplicates(), left_on = 'Side0_1_UniqueIds',right_on = 'ViewData.Side0_UniqueIds', how='left')\n",
    "#    #no_pair_ids_df = no_pair_ids_df.rename(columns={'0':'filter_key'})\n",
    "#    no_pair_ids_df['Predicted_Status'] = 'OB'\n",
    "#    no_pair_ids_df['Predicted_action'] = 'No-Pair'\n",
    "#    no_pair_ids_df['probability_No_pair'] = 0.9933\n",
    "#    no_pair_ids_df['probability_UMB'] = 0.0033\n",
    "#    no_pair_ids_df['probability_UMR'] = 0.0033    \n",
    "#    no_pair_ids_df['ML_flag'] = 'ML'\n",
    "#    no_pair_ids_df['TaskID'] = setup_code \n",
    "    no_pair_ids_df.to_csv(filepaths_no_pair_id_data)\n",
    "else:\n",
    "     with open(filepaths_no_pair_id_no_data_warning, 'w') as f:\n",
    "         f.write('No no pair ids found for this setup and date combination')\n",
    "\n",
    "\n",
    "test_file = pd.concat(training_df)\n",
    "\n",
    "test_file = test_file.reset_index()\n",
    "test_file = test_file.drop('index',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViewData.Side0_UniqueIds</th>\n",
       "      <th>ViewData.Side1_UniqueIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[19_3791153165_Advent Geneva, 133_3791153165_A...</td>\n",
       "      <td>[105_3791151084_State Street, 112_3791151084_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[124_3791153165_Advent Geneva, 16_3791153165_A...</td>\n",
       "      <td>[90_3791151084_State Street, 101_3791151084_St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[57_3791152753_Advent Geneva, 53_3791152753_Ad...</td>\n",
       "      <td>[12_3791152753_BNP Paribas, 9_3791152753_BNP P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[6_3791152753_Advent Geneva, 2_3791152753_Adve...</td>\n",
       "      <td>[24_3791152753_BNP Paribas, 21_3791152753_BNP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[111_3791153165_Advent Geneva, 108_3791153165_...</td>\n",
       "      <td>[195_3791153165_State Street, 198_3791153165_S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ViewData.Side0_UniqueIds  \\\n",
       "0  [19_3791153165_Advent Geneva, 133_3791153165_A...   \n",
       "1  [124_3791153165_Advent Geneva, 16_3791153165_A...   \n",
       "2  [57_3791152753_Advent Geneva, 53_3791152753_Ad...   \n",
       "3  [6_3791152753_Advent Geneva, 2_3791152753_Adve...   \n",
       "4  [111_3791153165_Advent Geneva, 108_3791153165_...   \n",
       "\n",
       "                            ViewData.Side1_UniqueIds  \n",
       "0  [105_3791151084_State Street, 112_3791151084_S...  \n",
       "1  [90_3791151084_State Street, 101_3791151084_St...  \n",
       "2  [12_3791152753_BNP Paribas, 9_3791152753_BNP P...  \n",
       "3  [24_3791152753_BNP Paribas, 21_3791152753_BNP ...  \n",
       "4  [195_3791153165_State Street, 198_3791153165_S...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtm_df_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['105_3791151084_State Street', '112_3791151084_State Street', '110_3791151084_State Street', '111_3791151084_State Street', '116_3791151084_State Street', '106_3791151084_State Street', '115_3791151084_State Street', '109_3791151084_State Street']\n",
      "['90_3791151084_State Street', '101_3791151084_State Street', '103_3791151084_State Street', '104_3791151084_State Street']\n",
      "['12_3791152753_BNP Paribas', '9_3791152753_BNP Paribas', '11_3791152753_BNP Paribas', '13_3791152753_BNP Paribas']\n",
      "['24_3791152753_BNP Paribas', '21_3791152753_BNP Paribas', '22_3791152753_BNP Paribas', '23_3791152753_BNP Paribas']\n",
      "['195_3791153165_State Street', '198_3791153165_State Street', '197_3791153165_State Street']\n",
      "['19_3791153165_Advent Geneva', '133_3791153165_Advent Geneva', '126_3791153165_Advent Geneva', '20_3791153165_Advent Geneva', '135_3791153165_Advent Geneva', '21_3791153165_Advent Geneva', '132_3791153165_Advent Geneva', '128_3791153165_Advent Geneva', '136_3791153165_Advent Geneva', '127_3791153165_Advent Geneva', '137_3791153165_Advent Geneva', '134_3791153165_Advent Geneva']\n",
      "['124_3791153165_Advent Geneva', '16_3791153165_Advent Geneva', '13_3791153165_Advent Geneva', '122_3791153165_Advent Geneva', '123_3791153165_Advent Geneva', '14_3791153165_Advent Geneva', '121_3791153165_Advent Geneva', '15_3791153165_Advent Geneva']\n",
      "['57_3791152753_Advent Geneva', '53_3791152753_Advent Geneva', '54_3791152753_Advent Geneva', '56_3791152753_Advent Geneva', '55_3791152753_Advent Geneva', '142_3791152753_Advent Geneva', '147_3791152753_Advent Geneva', '148_3791152753_Advent Geneva', '149_3791152753_Advent Geneva', '146_3791152753_Advent Geneva', '143_3791152753_Advent Geneva', '145_3791152753_Advent Geneva']\n",
      "['6_3791152753_Advent Geneva', '2_3791152753_Advent Geneva', '5_3791152753_Advent Geneva', '4_3791152753_Advent Geneva']\n",
      "['111_3791153165_Advent Geneva', '108_3791153165_Advent Geneva', '110_3791153165_Advent Geneva']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "final_mtm_table_copy = mtm_df_sd.copy()\n",
    "final_mtm_table_copy.rename(columns = {'ViewData.Side1_UniqueIds' : 'SideA.ViewData.Side1_UniqueIds',\\\n",
    "                                          'ViewData.Side0_UniqueIds' : 'SideB.ViewData.Side0_UniqueIds'}, inplace = True)\n",
    "\n",
    "final_mtm_table_copy['BreakID_Side1'] = final_mtm_table_copy['SideA.ViewData.Side1_UniqueIds'].apply( \\\n",
    "                                        lambda x : get_BreakID_from_list_of_Side_01_UniqueIds(fun_meo_df = meo_df, \\\n",
    "                                                                                              fun_side_0_or_1 = 1, \\\n",
    "                                                                                              fun_str_list_Side_01_UniqueIds = x))\n",
    "\n",
    "final_mtm_table_copy['BreakID_Side0'] = final_mtm_table_copy['SideB.ViewData.Side0_UniqueIds'].apply( \\\n",
    "                                        lambda x : get_BreakID_from_list_of_Side_01_UniqueIds(fun_meo_df = meo_df, \\\n",
    "                                                                                              fun_side_0_or_1 = 0, \\\n",
    "                                                                                              fun_str_list_Side_01_UniqueIds = x))\n",
    "\n",
    "\n",
    "\n",
    "final_mtm_table_copy = normalize_final_no_pair_table_col_names(fun_final_no_pair_table = final_mtm_table_copy)\n",
    "final_mtm_table_copy['ViewData.Side0_UniqueIds'] = final_mtm_table_copy['ViewData.Side0_UniqueIds'].astype(str)\n",
    "final_mtm_table_copy['ViewData.Side1_UniqueIds'] = final_mtm_table_copy['ViewData.Side1_UniqueIds'].astype(str)\n",
    "\n",
    "#\n",
    "#Change made on 10-12-2020. Below piece of code is wrong for merging. Commenting out below two lines of code and writing replacement code below it\n",
    "#Single_Side0_UniqueId_for_merging_with_meo_df = final_mtm_table_copy['ViewData.Side0_UniqueIds'][0][0]\n",
    "#final_mtm_table_copy['ViewData.Side0_UniqueIds_for_merging'] = Single_Side0_UniqueId_for_merging_with_meo_df\n",
    "#Change made on 10-12-2020. Above piece of code is wrong for merging. Commenting out above two lines of code and writing replacement code below it\n",
    "final_mtm_table_copy['ViewData.Side0_UniqueIds_for_merging'] = final_mtm_table_copy['ViewData.Side0_UniqueIds'].map(lambda x:x.lstrip('[').rstrip(']')).apply(lambda x : get_first_non_null_value(str(x)))\n",
    "\n",
    "final_mtm_table_copy['ViewData.Side0_UniqueIds_for_merging'] = final_mtm_table_copy['ViewData.Side0_UniqueIds_for_merging'].astype(str) \n",
    "final_mtm_table_copy['ViewData.Side0_UniqueIds_for_merging'] = final_mtm_table_copy['ViewData.Side0_UniqueIds_for_merging'].replace('\\'','',regex = True)\n",
    "\n",
    "final_mtm_table_copy_new = pd.merge(final_mtm_table_copy, meo_df[['ViewData.Side0_UniqueIds','ViewData.Task ID','ViewData.Task Business Date','ViewData.Source Combination Code']].drop_duplicates(), left_on = 'ViewData.Side0_UniqueIds_for_merging', right_on = 'ViewData.Side0_UniqueIds', how='left')\n",
    "\n",
    "final_mtm_table_copy_new['Predicted_Status'] = 'UMR'\n",
    "final_mtm_table_copy_new['Predicted_action'] = 'UMR_Many_to_Many'\n",
    "final_mtm_table_copy_new['ML_flag'] = 'ML'\n",
    "final_mtm_table_copy_new['SetupID'] = setup_code \n",
    "\n",
    "del final_mtm_table_copy['ViewData.Side0_UniqueIds_for_merging']\n",
    "del final_mtm_table_copy_new['ViewData.Side0_UniqueIds_for_merging']\n",
    "\n",
    "filepaths_final_mtm_table_copy_new = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_mtm_table_copy_new_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "final_mtm_table_copy_new.to_csv(filepaths_final_mtm_table_copy_new)\n",
    "\n",
    "change_names_of_final_mtm_table_copy_new_mapping_dict = {\n",
    "                                            'ViewData.Side0_UniqueIds_x' : 'Side0_UniqueIds',\n",
    "                                            'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds',\n",
    "                                            'BreakID_Side0' : 'BreakID',\n",
    "                                            'BreakID_Side1' : 'Final_predicted_break',\n",
    "                                            'ViewData.Task ID' : 'Task ID',\n",
    "                                            'ViewData.Task Business Date' : 'Task Business Date',\n",
    "                                            'ViewData.Source Combination Code' : 'Source Combination Code'\n",
    "                                        }\n",
    "\n",
    "\n",
    "final_mtm_table_copy_new.rename(columns = change_names_of_final_mtm_table_copy_new_mapping_dict, inplace = True)\n",
    "\n",
    "final_mtm_table_copy_new['Task Business Date'] = pd.to_datetime(final_mtm_table_copy_new['Task Business Date'])\n",
    "final_mtm_table_copy_new['Task Business Date'] = final_mtm_table_copy_new['Task Business Date'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "final_mtm_table_copy_new['Task Business Date'] = pd.to_datetime(final_mtm_table_copy_new['Task Business Date'])\n",
    "\n",
    "\n",
    "final_mtm_table_copy_new['PredictedComment'] = ''\n",
    "\n",
    "#Changing data types of columns as follows:\n",
    "#Side0_UniqueIds, Side1_UniqueIds, Final_predicted_break, Predicted_action, probability_No_pair, probability_UMB, probability_UMR, BusinessDate, SourceCombinationCode, Predicted_Status, ML_flag - string\n",
    "#BreakID, TaskID - int64\n",
    "#SetupID - int32\n",
    "final_mtm_table_copy_new['probability_UMB'] = 0.017\n",
    "final_mtm_table_copy_new['probability_No_pair'] = 0.017\n",
    "final_mtm_table_copy_new['probability_UMR'] = 0.95\n",
    "final_mtm_table_copy_new['probability_UMT'] = 0.017\n",
    "    \n",
    "for i in range(0,final_mtm_table_copy_new.shape[0]):\n",
    "    final_mtm_table_copy_new['probability_UMB'].iloc[i] = float(decimal.Decimal(random.randrange(17, 100))/1000)\n",
    "    final_mtm_table_copy_new['probability_No_pair'].iloc[i] = float(decimal.Decimal(random.randrange(17, 100))/1000)\n",
    "    final_mtm_table_copy_new['probability_UMR'].iloc[i] = float(decimal.Decimal(random.randrange(950, 1000))/1000)\n",
    "    final_mtm_table_copy_new['probability_UMT'].iloc[i] = float(decimal.Decimal(random.randrange(17, 100))/1000)\n",
    "\n",
    "\n",
    "final_mtm_table_copy_new[['Side0_UniqueIds', 'Side1_UniqueIds', 'Final_predicted_break', 'Predicted_action', 'probability_No_pair', 'probability_UMB', 'probability_UMR', 'Source Combination Code', 'Predicted_Status', 'ML_flag']] = final_mtm_table_copy_new[['Side0_UniqueIds', 'Side1_UniqueIds', 'Final_predicted_break', 'Predicted_action', 'probability_No_pair', 'probability_UMB', 'probability_UMR', 'Source Combination Code', 'Predicted_Status', 'ML_flag']].astype(str)\n",
    "\n",
    "\n",
    "#Note that BreakID now if more than one and in a list, so we cant convert it to float and int64\n",
    "final_mtm_table_copy_new[['Task ID']] = final_mtm_table_copy_new[['Task ID']].astype(float)\n",
    "final_mtm_table_copy_new[['Task ID']] = final_mtm_table_copy_new[['Task ID']].astype(np.int64)\n",
    "\n",
    "final_mtm_table_copy_new[['SetupID']] = final_mtm_table_copy_new[['SetupID']].astype(int)\n",
    "\n",
    "#final_table_to_write['Task ID'] = final_table_to_write['Task ID'].astype(float)\n",
    "#final_table_to_write['Task ID'] = final_table_to_write['Task ID'].astype(np.int64)\n",
    "\n",
    "change_col_names_final_mtm_table_copy_new_dict = {\n",
    "                        'Task ID' : 'TaskID',\n",
    "                        'Task Business Date' : 'BusinessDate',\n",
    "                        'Source Combination Code' : 'SourceCombinationCode'\n",
    "                        }\n",
    "final_mtm_table_copy_new.rename(columns = change_col_names_final_mtm_table_copy_new_dict, inplace = True)\n",
    "\n",
    "cols_for_database_new = ['Side0_UniqueIds',\n",
    " 'Side1_UniqueIds',\n",
    " 'BreakID',\n",
    " 'Final_predicted_break',\n",
    " 'Predicted_action',\n",
    " 'probability_No_pair',\n",
    " 'probability_UMB',\n",
    " 'probability_UMR',\n",
    " 'probability_UMT',\n",
    " 'TaskID',\n",
    " 'BusinessDate',\n",
    " 'PredictedComment',\n",
    " 'SourceCombinationCode',\n",
    " 'Predicted_Status',\n",
    " 'ML_flag',\n",
    " 'SetupID']\n",
    "\n",
    "final_mtm_table_copy_new_to_write = final_mtm_table_copy_new[cols_for_database_new]\n",
    "\n",
    "filepaths_final_mtm_table_copy_new_to_write = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_mtm_table_copy_new_to_write_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "\n",
    "final_mtm_table_copy_new_to_write.to_csv(filepaths_final_mtm_table_copy_new_to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Side0_UniqueIds</th>\n",
       "      <th>Side1_UniqueIds</th>\n",
       "      <th>BreakID</th>\n",
       "      <th>Final_predicted_break</th>\n",
       "      <th>Predicted_action</th>\n",
       "      <th>probability_No_pair</th>\n",
       "      <th>probability_UMB</th>\n",
       "      <th>probability_UMR</th>\n",
       "      <th>probability_UMT</th>\n",
       "      <th>TaskID</th>\n",
       "      <th>BusinessDate</th>\n",
       "      <th>PredictedComment</th>\n",
       "      <th>SourceCombinationCode</th>\n",
       "      <th>Predicted_Status</th>\n",
       "      <th>ML_flag</th>\n",
       "      <th>SetupID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>['19_3791153165_Advent Geneva', '133_379115316...</td>\n",
       "      <td>['105_3791151084_State Street', '112_379115108...</td>\n",
       "      <td>[1672099633, 1672099637, 1672099641, 167209964...</td>\n",
       "      <td>[1669583828, 1669583833, 1669583836, 166958383...</td>\n",
       "      <td>UMR_Many_to_Many</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.076</td>\n",
       "      <td>3791153165</td>\n",
       "      <td>2020-12-17 00:00:00+00:00</td>\n",
       "      <td></td>\n",
       "      <td>108916259</td>\n",
       "      <td>UMR</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>['124_3791153165_Advent Geneva', '16_379115316...</td>\n",
       "      <td>['90_3791151084_State Street', '101_3791151084...</td>\n",
       "      <td>[1672099602, 1672099604, 1672099612, 167209961...</td>\n",
       "      <td>[1669583831, 1669583832, 1669583837, 1669583942]</td>\n",
       "      <td>UMR_Many_to_Many</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.067</td>\n",
       "      <td>3791153165</td>\n",
       "      <td>2020-12-17 00:00:00+00:00</td>\n",
       "      <td></td>\n",
       "      <td>108916259</td>\n",
       "      <td>UMR</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>['57_3791152753_Advent Geneva', '53_3791152753...</td>\n",
       "      <td>['12_3791152753_BNP Paribas', '9_3791152753_BN...</td>\n",
       "      <td>[1671540494, 1671540510, 1671540571, 167154061...</td>\n",
       "      <td>[1671540650, 1671540678, 1671540681, 1671540685]</td>\n",
       "      <td>UMR_Many_to_Many</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.073</td>\n",
       "      <td>3791152753</td>\n",
       "      <td>2020-12-17 00:00:00+00:00</td>\n",
       "      <td></td>\n",
       "      <td>1007311931</td>\n",
       "      <td>UMR</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>['6_3791152753_Advent Geneva', '2_3791152753_A...</td>\n",
       "      <td>['24_3791152753_BNP Paribas', '21_3791152753_B...</td>\n",
       "      <td>[1671540500, 1671540677, 1671540679, 1671540682]</td>\n",
       "      <td>[1671540662, 1671540675, 1671540676, 1671540684]</td>\n",
       "      <td>UMR_Many_to_Many</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.019</td>\n",
       "      <td>3791152753</td>\n",
       "      <td>2020-12-17 00:00:00+00:00</td>\n",
       "      <td></td>\n",
       "      <td>1007311931</td>\n",
       "      <td>UMR</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>['111_3791153165_Advent Geneva', '108_37911531...</td>\n",
       "      <td>['195_3791153165_State Street', '198_379115316...</td>\n",
       "      <td>[1672099722, 1672099724, 1672099725]</td>\n",
       "      <td>[1672099717, 1672099719, 1672099727]</td>\n",
       "      <td>UMR_Many_to_Many</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.058</td>\n",
       "      <td>3791153165</td>\n",
       "      <td>2020-12-17 00:00:00+00:00</td>\n",
       "      <td></td>\n",
       "      <td>108916259</td>\n",
       "      <td>UMR</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Side0_UniqueIds  \\\n",
       "0  ['19_3791153165_Advent Geneva', '133_379115316...   \n",
       "1  ['124_3791153165_Advent Geneva', '16_379115316...   \n",
       "2  ['57_3791152753_Advent Geneva', '53_3791152753...   \n",
       "3  ['6_3791152753_Advent Geneva', '2_3791152753_A...   \n",
       "4  ['111_3791153165_Advent Geneva', '108_37911531...   \n",
       "\n",
       "                                     Side1_UniqueIds  \\\n",
       "0  ['105_3791151084_State Street', '112_379115108...   \n",
       "1  ['90_3791151084_State Street', '101_3791151084...   \n",
       "2  ['12_3791152753_BNP Paribas', '9_3791152753_BN...   \n",
       "3  ['24_3791152753_BNP Paribas', '21_3791152753_B...   \n",
       "4  ['195_3791153165_State Street', '198_379115316...   \n",
       "\n",
       "                                             BreakID  \\\n",
       "0  [1672099633, 1672099637, 1672099641, 167209964...   \n",
       "1  [1672099602, 1672099604, 1672099612, 167209961...   \n",
       "2  [1671540494, 1671540510, 1671540571, 167154061...   \n",
       "3   [1671540500, 1671540677, 1671540679, 1671540682]   \n",
       "4               [1672099722, 1672099724, 1672099725]   \n",
       "\n",
       "                               Final_predicted_break  Predicted_action  \\\n",
       "0  [1669583828, 1669583833, 1669583836, 166958383...  UMR_Many_to_Many   \n",
       "1   [1669583831, 1669583832, 1669583837, 1669583942]  UMR_Many_to_Many   \n",
       "2   [1671540650, 1671540678, 1671540681, 1671540685]  UMR_Many_to_Many   \n",
       "3   [1671540662, 1671540675, 1671540676, 1671540684]  UMR_Many_to_Many   \n",
       "4               [1672099717, 1672099719, 1672099727]  UMR_Many_to_Many   \n",
       "\n",
       "  probability_No_pair probability_UMB probability_UMR  probability_UMT  \\\n",
       "0               0.046           0.052            0.95            0.076   \n",
       "1               0.062           0.041            0.96            0.067   \n",
       "2               0.049           0.074           0.969            0.073   \n",
       "3               0.042           0.054           0.973            0.019   \n",
       "4               0.085           0.029           0.993            0.058   \n",
       "\n",
       "       TaskID              BusinessDate PredictedComment  \\\n",
       "0  3791153165 2020-12-17 00:00:00+00:00                    \n",
       "1  3791153165 2020-12-17 00:00:00+00:00                    \n",
       "2  3791152753 2020-12-17 00:00:00+00:00                    \n",
       "3  3791152753 2020-12-17 00:00:00+00:00                    \n",
       "4  3791153165 2020-12-17 00:00:00+00:00                    \n",
       "\n",
       "  SourceCombinationCode Predicted_Status ML_flag  SetupID  \n",
       "0             108916259              UMR      ML      379  \n",
       "1             108916259              UMR      ML      379  \n",
       "2            1007311931              UMR      ML      379  \n",
       "3            1007311931              UMR      ML      379  \n",
       "4             108916259              UMR      ML      379  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mtm_table_copy_new_to_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ViteosModel\\OakTree - Pratik Code\n"
     ]
    }
   ],
   "source": [
    "test_file['SideB.ViewData.BreakID_B_side'] = test_file['SideB.ViewData.BreakID_B_side'].astype('int64')\n",
    "test_file['SideA.ViewData.BreakID_A_side'] = test_file['SideA.ViewData.BreakID_A_side'].astype('int64')\n",
    "\n",
    "test_file['SideB.ViewData.CUSIP'] = test_file['SideB.ViewData.CUSIP'].str.split(\".\",expand=True)[0]\n",
    "test_file['SideA.ViewData.CUSIP'] = test_file['SideA.ViewData.CUSIP'].str.split(\".\",expand=True)[0]\n",
    "\n",
    "test_file['SideA.ViewData.ISIN'] = test_file['SideA.ViewData.ISIN'].astype(str)\n",
    "test_file['SideB.ViewData.ISIN'] = test_file['SideB.ViewData.ISIN'].astype(str)\n",
    "test_file['SideA.ViewData.CUSIP'] = test_file['SideA.ViewData.CUSIP'].astype(str)\n",
    "test_file['SideB.ViewData.CUSIP'] = test_file['SideB.ViewData.CUSIP'].astype(str)\n",
    "test_file['SideA.ViewData.Currency'] = test_file['SideA.ViewData.Currency'].astype(str)\n",
    "test_file['SideB.ViewData.Currency'] = test_file['SideB.ViewData.Currency'].astype(str)\n",
    "\n",
    "test_file['SideA.ViewData.Trade Date'] = test_file['SideA.ViewData.Trade Date'].astype(str)\n",
    "test_file['SideB.ViewData.Trade Date'] = test_file['SideB.ViewData.Trade Date'].astype(str)\n",
    "test_file['SideA.ViewData.Settle Date'] = test_file['SideA.ViewData.Settle Date'].astype(str)\n",
    "test_file['SideB.ViewData.Settle Date'] = test_file['SideB.ViewData.Settle Date'].astype(str)\n",
    "test_file['SideA.ViewData.Fund'] = test_file['SideA.ViewData.Fund'].astype(str)\n",
    "test_file['SideB.ViewData.Fund'] = test_file['SideB.ViewData.Fund'].astype(str)\n",
    "\n",
    "values_ISIN_A_Side = test_file['SideA.ViewData.ISIN'].values\n",
    "values_ISIN_B_Side = test_file['SideB.ViewData.ISIN'].values\n",
    "#test_file['ISIN_match'] = vec_equals_fun(values_ISIN_A_Side,values_ISIN_B_Side)\n",
    "\n",
    "values_CUSIP_A_Side = test_file['SideA.ViewData.CUSIP'].values\n",
    "values_CUSIP_B_Side = test_file['SideB.ViewData.CUSIP'].values\n",
    "#\n",
    "# values_CUSIP_A_Side = test_file['SideA.ViewData.Currency'].values\n",
    "# values_CUSIP_B_Side = test_file['SideB.ViewData.Currency'].values\n",
    "\n",
    "values_Currency_match_A_Side = test_file['SideA.ViewData.Currency'].values\n",
    "values_Currency_match_B_Side = test_file['SideA.ViewData.Currency'].values\n",
    "\n",
    "values_Trade_Date_match_A_Side = test_file['SideA.ViewData.Trade Date'].values\n",
    "values_Trade_Date_match_B_Side = test_file['SideB.ViewData.Trade Date'].values\n",
    "\n",
    "values_Settle_Date_match_A_Side = test_file['SideA.ViewData.Settle Date'].values\n",
    "values_Settle_Date_match_B_Side = test_file['SideB.ViewData.Settle Date'].values\n",
    "\n",
    "values_Fund_match_A_Side = test_file['SideA.ViewData.Fund'].values\n",
    "values_Fund_match_B_Side = test_file['SideB.ViewData.Fund'].values\n",
    "\n",
    "test_file['ISIN_match'] = vec_equals_fun(values_ISIN_A_Side,values_ISIN_B_Side)\n",
    "test_file['CUSIP_match'] = vec_equals_fun(values_CUSIP_A_Side,values_CUSIP_B_Side)\n",
    "test_file['Currency_match'] = vec_equals_fun(values_Currency_match_A_Side,values_Currency_match_B_Side)\n",
    "test_file['Trade_Date_match'] = vec_equals_fun(values_Trade_Date_match_A_Side,values_Trade_Date_match_B_Side)\n",
    "test_file['Settle_Date_match'] = vec_equals_fun(values_Settle_Date_match_A_Side,values_Settle_Date_match_B_Side)\n",
    "test_file['Fund_match'] = vec_equals_fun(values_Fund_match_A_Side,values_Fund_match_B_Side)\n",
    "\n",
    "test_file['Amount_diff_1'] = test_file['SideA.ViewData.Accounting Net Amount'] - test_file['SideB.ViewData.B-P Net Amount']\n",
    "test_file['Amount_diff_2'] = test_file['SideB.ViewData.Accounting Net Amount'] - test_file['SideA.ViewData.B-P Net Amount']\n",
    "\n",
    "\n",
    "# ## Description code\n",
    "\n",
    "os.chdir('D:\\\\ViteosModel\\\\OakTree - Pratik Code')\n",
    "print(os.getcwd())\n",
    "\n",
    "## TODO - Import a csv file for description category mapping\n",
    "\n",
    "com = pd.read_csv('desc cat with naveen oaktree.csv')\n",
    "cat_list = list(set(com['Pairing']))\n",
    "\n",
    "\n",
    "\n",
    "test_file['SideA.desc_cat'] = test_file['SideA.ViewData.Description'].apply(lambda x : descclean(x,cat_list))\n",
    "test_file['SideB.desc_cat'] = test_file['SideB.ViewData.Description'].apply(lambda x : descclean(x,cat_list))\n",
    "\n",
    "test_file['SideA.desc_cat'] = test_file['SideA.desc_cat'].apply(lambda x : currcln(x))\n",
    "test_file['SideB.desc_cat'] = test_file['SideB.desc_cat'].apply(lambda x : currcln(x))\n",
    "\n",
    "com = com.drop(['var','Catogery'], axis = 1)\n",
    "\n",
    "com = com.drop_duplicates()\n",
    "\n",
    "com['Pairing'] = com['Pairing'].apply(lambda x : x.lower())\n",
    "com['replace'] = com['replace'].apply(lambda x : x.lower())\n",
    "\n",
    "\n",
    "test_file['SideA.new_desc_cat'] = test_file['SideA.desc_cat'].apply(lambda x : catcln1(x,com))\n",
    "test_file['SideB.new_desc_cat'] = test_file['SideB.desc_cat'].apply(lambda x : catcln1(x,com))\n",
    "\n",
    "comp = ['inc','stk','corp ','llc','pvt','plc']\n",
    "test_file['SideA.new_desc_cat'] = test_file['SideA.new_desc_cat'].apply(lambda x : 'Company' if x in comp else x)\n",
    "\n",
    "test_file['SideB.new_desc_cat'] = test_file['SideB.new_desc_cat'].apply(lambda x : 'Company' if x in comp else x)\n",
    "\n",
    "test_file['SideA.new_desc_cat'] = test_file['SideA.new_desc_cat'].apply(lambda x : desccat(x))\n",
    "test_file['SideB.new_desc_cat'] = test_file['SideB.new_desc_cat'].apply(lambda x : desccat(x))\n",
    "# ## Prime Broker\n",
    "test_file['new_pb'] = test_file['SideA.ViewData.Mapped Custodian Account'].apply(lambda x : x.split('_')[0] if type(x)==str else x)\n",
    "new_pb_mapping = {'GSIL':'GS','CITIGM':'CITI','JPMNA':'JPM'}\n",
    "test_file['SideA.ViewData.Prime Broker'] = test_file['SideA.ViewData.Prime Broker'].fillna('kkk')\n",
    "test_file['new_pb1'] = test_file.apply(lambda x : x['new_pb'] if x['SideA.ViewData.Prime Broker']=='kkk' else x['SideA.ViewData.Prime Broker'],axis = 1)\n",
    "test_file['Trade_date_diff'] = (pd.to_datetime(test_file['SideA.ViewData.Trade Date']) - pd.to_datetime(test_file['SideB.ViewData.Trade Date'])).dt.days\n",
    "\n",
    "test_file['Settle_date_diff'] = (pd.to_datetime(test_file['SideA.ViewData.Settle Date']) - pd.to_datetime(test_file['SideB.ViewData.Settle Date'])).dt.days\n",
    "\n",
    "############ Fund match new ########\n",
    "\n",
    "values_Fund_match_A_Side = test_file['SideA.ViewData.Fund'].values\n",
    "values_Fund_match_B_Side = test_file['SideB.ViewData.Fund'].values\n",
    "\n",
    "vec_fund_match = np.vectorize(fundmatch)\n",
    "\n",
    "test_file['SideA.ViewData.Fund'] = vec_fund_match(values_Fund_match_A_Side)\n",
    "test_file['SideB.ViewData.Fund'] = vec_fund_match(values_Fund_match_B_Side)\n",
    "\n",
    "### New code for cleaning text variables \n",
    "trans_type_A_side = test_file['SideA.ViewData.Transaction Type']\n",
    "trans_type_B_side = test_file['SideB.ViewData.Transaction Type']\n",
    "\n",
    "asset_type_cat_A_side = test_file['SideA.ViewData.Asset Type Category']\n",
    "asset_type_cat_B_side = test_file['SideB.ViewData.Asset Type Category']\n",
    "\n",
    "invest_type_A_side = test_file['SideA.ViewData.Investment Type']\n",
    "invest_type_B_side = test_file['SideB.ViewData.Investment Type']\n",
    "\n",
    "prime_broker_A_side = test_file['SideA.ViewData.Prime Broker']\n",
    "prime_broker_B_side = test_file['SideB.ViewData.Prime Broker']\n",
    "\n",
    "# LOWER CASE\n",
    "trans_type_A_side = [str(item).lower() for item in trans_type_A_side]\n",
    "trans_type_B_side = [str(item).lower() for item in trans_type_B_side]\n",
    "\n",
    "asset_type_cat_A_side = [str(item).lower() for item in asset_type_cat_A_side]\n",
    "asset_type_cat_B_side = [str(item).lower() for item in asset_type_cat_B_side]\n",
    "\n",
    "invest_type_A_side = [str(item).lower() for item in invest_type_A_side]\n",
    "invest_type_B_side = [str(item).lower() for item in invest_type_B_side]\n",
    "\n",
    "prime_broker_A_side = [str(item).lower() for item in prime_broker_A_side]\n",
    "prime_broker_B_side = [str(item).lower() for item in prime_broker_B_side]\n",
    "\n",
    "split_trans_A_side = [item.split() for item in trans_type_A_side]\n",
    "split_trans_B_side = [item.split() for item in trans_type_B_side]\n",
    "\n",
    "split_asset_A_side = [item.split() for item in asset_type_cat_A_side]\n",
    "split_asset_B_side = [item.split() for item in asset_type_cat_B_side]\n",
    "\n",
    "split_invest_A_side = [item.split() for item in invest_type_A_side]\n",
    "split_invest_B_side = [item.split() for item in invest_type_B_side]\n",
    "\n",
    "split_prime_A_side = [item.split() for item in prime_broker_A_side]\n",
    "split_prime_b_side = [item.split() for item in prime_broker_B_side]\n",
    "\n",
    "## Transacion type\n",
    "\n",
    "remove_nums_A_side = [[item for item in sublist if not is_num(item)] for sublist in split_trans_A_side]\n",
    "remove_nums_B_side = [[item for item in sublist if not is_num(item)] for sublist in split_trans_B_side]\n",
    "\n",
    "remove_dates_A_side = [[item for item in sublist if not (is_date_format(item) or date_edge_cases(item))] for sublist in remove_nums_A_side]\n",
    "remove_dates_B_side = [[item for item in sublist if not (is_date_format(item) or date_edge_cases(item))] for sublist in remove_nums_B_side]\n",
    "\n",
    "\n",
    "# Specific to clients already used on, will have to be edited for other edge cases\n",
    "remove_amts_A_side = [[item for item in sublist if item[0] != '$'] for sublist in remove_dates_A_side]\n",
    "remove_amts_B_side = [[item for item in sublist if item[0] != '$'] for sublist in remove_dates_B_side]\n",
    "\n",
    "\n",
    "clean_adr_A_side = [(['ADR'] if 'adr' in item else item) for item in remove_amts_A_side]\n",
    "clean_adr_B_side = [(['ADR'] if 'adr' in item else item) for item in remove_amts_B_side]\n",
    "\n",
    "clean_tax_A_side = [(item[:2] if '30%' in item else item) for item in clean_adr_A_side]\n",
    "clean_tax_B_side = [(item[:2] if '30%' in item else item) for item in clean_adr_B_side]\n",
    "\n",
    "remove_ons_A_side = [(item[:item.index('on')] if 'on' in item else item) for item in clean_tax_A_side]\n",
    "remove_ons_B_side = [(item[:item.index('on')] if 'on' in item else item) for item in clean_tax_B_side]\n",
    "\n",
    "clean_eqswap_A_side = [(item[1:] if 'eqswap' in item else item) for item in remove_ons_A_side]\n",
    "clean_eqswap_B_side = [(item[1:] if 'eqswap' in item else item) for item in remove_ons_B_side]\n",
    "\n",
    "remove_mh_A_side = [[item for item in sublist if 'mh' not in item] for sublist in clean_eqswap_A_side]\n",
    "remove_mh_B_side = [[item for item in sublist if 'mh' not in item] for sublist in clean_eqswap_B_side]\n",
    "\n",
    "remove_ats_A_side = [(item[:item.index('@')] if '@' in item else item) for item in remove_mh_A_side]\n",
    "remove_ats_B_side = [(item[:item.index('@')] if '@' in item else item) for item in remove_mh_B_side]\n",
    "\n",
    "cleaned_trans_types_A_side = [' '.join(item) for item in remove_ats_A_side]\n",
    "cleaned_trans_types_B_side = [' '.join(item) for item in remove_ats_B_side]\n",
    "\n",
    "# # INVESTMENT TYPE\n",
    "\n",
    "remove_nums_i_A_side = [[item for item in sublist if not is_num(item)] for sublist in split_invest_A_side]\n",
    "remove_nums_i_B_side = [[item for item in sublist if not is_num(item)] for sublist in split_invest_B_side]\n",
    "\n",
    "remove_dates_i_A_side = [[item for item in sublist if not is_date_format(item)] for sublist in remove_nums_i_A_side]\n",
    "remove_dates_i_B_side = [[item for item in sublist if not is_date_format(item)] for sublist in remove_nums_i_B_side]\n",
    "\n",
    "cleaned_invest_A_side = [' '.join(item) for item in remove_dates_i_A_side]\n",
    "cleaned_invest_B_side = [' '.join(item) for item in remove_dates_i_B_side]\n",
    "\n",
    "remove_nums_a_A_side = [[item for item in sublist if not is_num(item)] for sublist in split_asset_A_side]\n",
    "remove_nums_a_B_side = [[item for item in sublist if not is_num(item)] for sublist in split_asset_B_side]\n",
    "\n",
    "remove_dates_a_A_side = [[item for item in sublist if not is_date_format(item)] for sublist in remove_nums_a_A_side]\n",
    "remove_dates_a_B_side = [[item for item in sublist if not is_date_format(item)] for sublist in remove_nums_a_B_side]\n",
    "\n",
    "cleaned_asset_A_side = [' '.join(item) for item in remove_dates_a_A_side]\n",
    "cleaned_asset_B_side = [' '.join(item) for item in remove_dates_a_B_side]\n",
    "\n",
    "test_file['SideA.ViewData.Transaction Type'] = cleaned_trans_types_A_side\n",
    "test_file['SideB.ViewData.Transaction Type'] = cleaned_trans_types_B_side\n",
    "\n",
    "test_file['SideA.ViewData.Investment Type'] = cleaned_invest_A_side\n",
    "test_file['SideB.ViewData.Investment Type'] = cleaned_invest_B_side\n",
    "\n",
    "test_file['SideA.ViewData.Asset Category Type'] = cleaned_asset_A_side\n",
    "test_file['SideB.ViewData.Asset Category Type'] = cleaned_asset_B_side\n",
    "\n",
    "values_transaction_type_match_A_Side = test_file['SideA.ViewData.Transaction Type'].values\n",
    "values_transaction_type_match_B_Side = test_file['SideB.ViewData.Transaction Type'].values\n",
    "\n",
    "vec_tt_match = np.vectorize(mhreplaced)\n",
    "\n",
    "test_file['SideA.ViewData.Transaction Type'] = vec_tt_match(values_transaction_type_match_A_Side)\n",
    "test_file['SideB.ViewData.Transaction Type'] = vec_tt_match(values_transaction_type_match_B_Side)\n",
    "\n",
    "test_file.loc[test_file['SideA.ViewData.Transaction Type']=='int','SideA.ViewData.Transaction Type'] = 'interest'\n",
    "test_file.loc[test_file['SideA.ViewData.Transaction Type']=='wires','SideA.ViewData.Transaction Type'] = 'wire'\n",
    "test_file.loc[test_file['SideA.ViewData.Transaction Type']=='dividends','SideA.ViewData.Transaction Type'] = 'dividend'\n",
    "test_file.loc[test_file['SideA.ViewData.Transaction Type']=='miscellaneous','SideA.ViewData.Transaction Type'] = 'misc'\n",
    "test_file.loc[test_file['SideA.ViewData.Transaction Type']=='div','SideA.ViewData.Transaction Type'] = 'dividend'\n",
    "\n",
    "test_file['SideA.ViewData.Investment Type'] = test_file['SideA.ViewData.Investment Type'].apply(lambda x: x.replace('eqty','equity'))\n",
    "test_file['SideA.ViewData.Investment Type'] = test_file['SideA.ViewData.Investment Type'].apply(lambda x: x.replace('options','option'))\n",
    "test_file['SideA.ViewData.Investment Type'] = test_file['SideA.ViewData.Investment Type'].apply(lambda x: x.replace('eqt','equity'))\n",
    "test_file['SideA.ViewData.Investment Type'] = test_file['SideA.ViewData.Investment Type'].apply(lambda x: x.replace('eqty','equity'))\n",
    "\n",
    "test_file['ViewData.Combined Transaction Type'] = test_file['SideA.ViewData.Transaction Type'].astype(str) +  test_file['SideB.ViewData.Transaction Type'].astype(str)\n",
    "test_file['ViewData.Combined Fund'] = test_file['SideA.ViewData.Fund'].astype(str) + test_file['SideB.ViewData.Fund'].astype(str)\n",
    "\n",
    "test_file['Combined_Investment_Type'] = test_file['SideA.ViewData.Investment Type'].astype(str) + test_file['SideB.ViewData.Investment Type'].astype(str)\n",
    "\n",
    "test_file['Combined_Asset_Type_Category'] = test_file['SideA.ViewData.Asset Category Type'].astype(str) + test_file['SideB.ViewData.Asset Category Type'].astype(str)\n",
    "\n",
    "    \n",
    "vec_nan_fun = np.vectorize(nan_fun)\n",
    "values_ISIN_A_Side = test_file['SideA.ViewData.ISIN'].values\n",
    "values_ISIN_B_Side = test_file['SideB.ViewData.ISIN'].values\n",
    "test_file['SideA.ISIN_NA'] = vec_nan_fun(values_ISIN_A_Side)\n",
    "test_file['SideB.ISIN_NA'] = vec_nan_fun(values_ISIN_A_Side)\n",
    "\n",
    "vec_a_key_match_fun = np.vectorize(a_keymatch)\n",
    "vec_b_key_match_fun = np.vectorize(b_keymatch)\n",
    "\n",
    "values_ISIN_A_Side = test_file['SideA.ViewData.ISIN'].values\n",
    "values_ISIN_B_Side = test_file['SideB.ViewData.ISIN'].values\n",
    "\n",
    "values_CUSIP_A_Side = test_file['SideA.ViewData.CUSIP'].values\n",
    "values_CUSIP_B_Side = test_file['SideB.ViewData.CUSIP'].values\n",
    "\n",
    "test_file['SideB.ViewData.key_NAN']= vec_a_key_match_fun(values_CUSIP_B_Side,values_ISIN_B_Side)[0]\n",
    "test_file['SideB.ViewData.Common_key'] = vec_a_key_match_fun(values_CUSIP_B_Side,values_ISIN_B_Side)[1]\n",
    "test_file['SideA.ViewData.key_NAN'] = vec_b_key_match_fun(values_CUSIP_A_Side,values_ISIN_A_Side)[0]\n",
    "test_file['SideA.ViewData.Common_key'] = vec_b_key_match_fun(values_CUSIP_A_Side,values_ISIN_A_Side)[1]\n",
    "\n",
    "vec_nan_equal_fun = np.vectorize(nan_equals_fun)\n",
    "values_key_NAN_B_Side = test_file['SideB.ViewData.key_NAN'].values\n",
    "values_key_NAN_A_Side = test_file['SideA.ViewData.key_NAN'].values\n",
    "test_file['All_key_nan'] = vec_nan_equal_fun(values_key_NAN_B_Side,values_key_NAN_A_Side )\n",
    "\n",
    "test_file['SideB.ViewData.Common_key'] = test_file['SideB.ViewData.Common_key'].astype(str)\n",
    "test_file['SideA.ViewData.Common_key'] = test_file['SideA.ViewData.Common_key'].astype(str)\n",
    "\n",
    "vec_new_key_match_fun = np.vectorize(new_key_match_fun)\n",
    "values_Common_key_B_Side = test_file['SideB.ViewData.Common_key'].values\n",
    "values_Common_key_A_Side = test_file['SideA.ViewData.Common_key'].values\n",
    "values_All_key_NAN = test_file['All_key_nan'].values\n",
    "\n",
    "test_file['new_key_match']= vec_new_key_match_fun(values_Common_key_B_Side,values_Common_key_A_Side,values_All_key_NAN)\n",
    "\n",
    "test_file['amount_percent'] = (test_file['SideA.ViewData.B-P Net Amount']/test_file['SideB.ViewData.Accounting Net Amount']*100)\n",
    "\n",
    "test_file['SideB.ViewData.Investment Type'] = test_file['SideB.ViewData.Investment Type'].apply(lambda x: str(x).lower())\n",
    "test_file['SideA.ViewData.Investment Type'] = test_file['SideA.ViewData.Investment Type'].apply(lambda x: str(x).lower())\n",
    "\n",
    "test_file['SideB.ViewData.Prime Broker'] = test_file['SideB.ViewData.Prime Broker'].apply(lambda x: str(x).lower())\n",
    "test_file['SideA.ViewData.Prime Broker'] = test_file['SideA.ViewData.Prime Broker'].apply(lambda x: str(x).lower())\n",
    "\n",
    "test_file['SideB.ViewData.Asset Type Category'] = test_file['SideB.ViewData.Asset Type Category'].apply(lambda x: str(x).lower())\n",
    "test_file['SideA.ViewData.Asset Type Category'] = test_file['SideA.ViewData.Asset Type Category'].apply(lambda x: str(x).lower())\n",
    "\n",
    "test_file['ViewData.Combined Transaction Type'] = test_file['ViewData.Combined Transaction Type'].apply(lambda x: x.replace('jnl','journal'))\n",
    "\n",
    "test_file['SideA.ViewData.Transaction Type'] = test_file['SideA.ViewData.Transaction Type'].apply(lambda x: x.replace('cover short','covershort'))\n",
    "\n",
    "trade_types_A = ['buy', 'sell', 'covershort','sellshort',\n",
    "       'fx', 'fx settlement', 'sell short',\n",
    "       'trade not to be reported_buy', 'covershort','ptbl','ptss', 'ptcs', 'ptcl']\n",
    "trade_types_B = ['trade not to be reported_buy','buy', 'sellshort', 'sell', 'covershort',\n",
    "       'spotfx', 'forwardfx',\n",
    "       'trade not to be reported_sell',\n",
    "       'trade not to be reported_sellshort',\n",
    "       'trade not to be reported_covershort']\n",
    "\n",
    "test_file['SideA.TType'] = test_file.apply(lambda x: \"Trade\" if x['SideA.ViewData.Transaction Type'] in trade_types_A else \"Non-Trade\", axis=1)\n",
    "test_file['SideB.TType'] = test_file.apply(lambda x: \"Trade\" if x['SideB.ViewData.Transaction Type'] in trade_types_B else \"Non-Trade\", axis=1)\n",
    "\n",
    "test_file['Combined_Desc'] = test_file['SideA.new_desc_cat'] + test_file['SideB.new_desc_cat']\n",
    "\n",
    "test_file['Combined_TType'] = test_file['SideA.TType'].astype(str) + test_file['SideB.TType'].astype(str)\n",
    "\n",
    "for feature in ['SideA.Date','SideB.Date','SideA.ViewData.Settle Date','SideB.ViewData.Settle Date']:\n",
    "    #train_full_new12[feature] = le.fit_transform(train_full_new12[feature])\n",
    "    test_file[feature] = pd.to_datetime(test_file[feature],errors = 'coerce').dt.weekday\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4088, 28)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ## UMR Mapping\n",
    "## TODO Import HIstorical UMR FILE for Transaction Type mapping\n",
    "oaktree_umr = pd.read_csv('OakTree_UMR.csv')\n",
    "\n",
    "test_file['tt_map_flag'] = test_file.apply(lambda x: 1 if x['ViewData.Combined Transaction Type'] in oaktree_umr['ViewData.Combined Transaction Type'].unique() else 0, axis=1)\n",
    "\n",
    "test_file['abs_amount_flag'] = test_file.apply(lambda x: 1 if x['SideB.ViewData.Accounting Net Amount'] == x['SideA.ViewData.B-P Net Amount']*(-1) else 0, axis=1)\n",
    "\n",
    "test_file = test_file[~test_file['SideB.ViewData.Settle Date'].isnull()]\n",
    "test_file = test_file[~test_file['SideA.ViewData.Settle Date'].isnull()]\n",
    "\n",
    "test_file = test_file.reset_index().drop('index',1)\n",
    "test_file['SideA.ViewData.Settle Date'] = test_file['SideA.ViewData.Settle Date'].astype(int)\n",
    "test_file['SideB.ViewData.Settle Date'] = test_file['SideB.ViewData.Settle Date'].astype(int)\n",
    "\n",
    "\n",
    "# ## Test file served into the model\n",
    "\n",
    "test_file2 = test_file.copy()\n",
    "\n",
    "X_test = test_file2[model_cols]\n",
    "\n",
    "X_test = X_test.reset_index()\n",
    "X_test = X_test.drop('index',1)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "X_test.shape\n",
    "\n",
    "X_test = X_test.drop_duplicates()\n",
    "X_test = X_test.reset_index()\n",
    "X_test = X_test.drop('index',1)\n",
    "\n",
    "X_test.shape\n",
    "\n",
    "# ## Model Pickle file import\n",
    "## TODO Import Pickle file for 1st Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'OakTree_final_model2.sav'\n",
    "\n",
    "clf = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# ## Predictions\n",
    "\n",
    "# Actual class predictions\n",
    "rf_predictions = clf.predict(X_test.drop(['SideB.ViewData.Status','SideB.ViewData.BreakID_B_side', 'SideA.ViewData.Status','SideA.ViewData.BreakID_A_side','SideA.ViewData._ID','SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds'],1))\n",
    "# Probabilities for each class\n",
    "rf_probs = clf.predict_proba(X_test.drop(['SideB.ViewData.Status','SideB.ViewData.BreakID_B_side', 'SideA.ViewData.Status','SideA.ViewData.BreakID_A_side','SideA.ViewData._ID','SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds'],1))[:, 1]\n",
    "\n",
    "probability_class_0 = clf.predict_proba(X_test.drop(['SideB.ViewData.Status','SideB.ViewData.BreakID_B_side','SideA.ViewData.Status','SideA.ViewData.BreakID_A_side','SideA.ViewData._ID','SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds'],1))[:, 0]\n",
    "probability_class_1 = clf.predict_proba(X_test.drop(['SideB.ViewData.Status','SideB.ViewData.BreakID_B_side', 'SideA.ViewData.Status','SideA.ViewData.BreakID_A_side','SideA.ViewData._ID','SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds'],1))[:, 1]\n",
    "\n",
    "probability_class_2 = clf.predict_proba(X_test.drop(['SideB.ViewData.Status','SideB.ViewData.BreakID_B_side','SideA.ViewData.Status','SideA.ViewData.BreakID_A_side','SideA.ViewData._ID','SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds'],1))[:, 2]\n",
    "\n",
    "X_test['Predicted_action'] = rf_predictions\n",
    "X_test['probability_No_pair'] = probability_class_0\n",
    "X_test['probability_UMB'] = probability_class_1\n",
    "X_test['probability_UMR'] = probability_class_2\n",
    "X_test['Predicted_action'].value_counts()\n",
    "\n",
    "# ## Two Step Modeling\n",
    "\n",
    "X_test2 = test_file[model_cols_2]\n",
    "X_test2 = X_test2.reset_index()\n",
    "X_test2 = X_test2.drop('index',1)\n",
    "X_test2 = X_test2.fillna(0)\n",
    "\n",
    "X_test2.shape\n",
    "X_test2 = X_test2.drop_duplicates()\n",
    "X_test2 = X_test2.reset_index()\n",
    "X_test2 = X_test2.drop('index',1)\n",
    "\n",
    "X_test2.shape\n",
    "\n",
    "## TODO Import MOdel2 as per the two step modelling process\n",
    "\n",
    "filename2 = 'OakTree_final_model2_step_two.sav'\n",
    "clf2 = pickle.load(open(filename2, 'rb'))\n",
    "\n",
    "# Actual class predictions\n",
    "rf_predictions2 = clf2.predict(X_test2.drop(['SideB.ViewData.Status','SideB.ViewData.BreakID_B_side', 'SideA.ViewData.Status','SideA.ViewData.BreakID_A_side','SideA.ViewData._ID','SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds'],1))\n",
    "\n",
    "# Probabilities for each class\n",
    "rf_probs2 = clf2.predict_proba(X_test2.drop(['SideB.ViewData.Status','SideB.ViewData.BreakID_B_side', 'SideA.ViewData.Status','SideA.ViewData.BreakID_A_side','SideA.ViewData._ID','SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds'],1))[:, 1]\n",
    "\n",
    "probability_class_0_two = clf2.predict_proba(X_test2.drop(['SideB.ViewData.Status','SideB.ViewData.BreakID_B_side','SideA.ViewData.Status','SideA.ViewData.BreakID_A_side','SideA.ViewData._ID','SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds'],1))[:, 0]\n",
    "probability_class_1_two = clf2.predict_proba(X_test2.drop(['SideB.ViewData.Status','SideB.ViewData.BreakID_B_side', 'SideA.ViewData.Status','SideA.ViewData.BreakID_A_side','SideA.ViewData._ID','SideB.ViewData._ID','SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds'],1))[:, 1]\n",
    "\n",
    "X_test2['Predicted_action_2'] = rf_predictions2\n",
    "X_test2['probability_No_pair_2'] = probability_class_0_two\n",
    "X_test2['probability_UMB_2'] = probability_class_1_two\n",
    "\n",
    "X_test = pd.concat([X_test, X_test2[['Predicted_action_2','probability_No_pair_2','probability_UMB_2']]],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes made on 25-11-2020.\n",
    "filepaths_X_test = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\X_Test_for_Pratik_setup_' + setup_code + '_date_' + str(date_i) + '_2.csv'\n",
    "X_test.to_csv(filepaths_X_test)\n",
    "\n",
    "# ## New Aggregation\n",
    "X_test['Tolerance_level'] = np.abs(X_test['probability_UMB_2'] - X_test['probability_No_pair_2'])\n",
    "b_side_agg = X_test.groupby(['SideB.ViewData.Side0_UniqueIds'])['Predicted_action_2'].unique().reset_index()\n",
    "a_side_agg = X_test.groupby(['SideA.ViewData.Side1_UniqueIds'])['Predicted_action_2'].unique().reset_index()\n",
    "\n",
    "\n",
    "# ## UMR segregation\n",
    "umr_ids_0 = umr_seg(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(umr_ids_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 36)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[X_test['SideB.ViewData.Side0_UniqueIds'].isin(umr_ids_0) & (X_test['Predicted_action']=='UMR_One_to_One')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_umr_table = X_test[X_test['SideB.ViewData.Side0_UniqueIds'].isin(umr_ids_0) & (X_test['Predicted_action']=='UMR_One_to_One')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(duplicate_ids_final_umr_table_Side0) != 0):\n",
    "    final_umr_table_duplicates_Side0 = final_umr_table[final_umr_table['SideB.ViewData.Side0_UniqueIds'].isin(duplicate_ids_final_umr_table_Side0)]\n",
    "    final_umr_table_duplicates_Side0 = final_umr_table_duplicates_Side0[['SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds','SideB.ViewData.BreakID_B_side','SideA.ViewData.BreakID_A_side','Predicted_action','probability_No_pair','probability_UMB','probability_UMR']]#,'probability_UMT']]\n",
    "    final_umr_table = final_umr_table[~final_umr_table['SideB.ViewData.Side0_UniqueIds'].isin(duplicate_ids_final_umr_table_Side0)]\n",
    "    final_umr_table_side0_ids = list(set(final_umr_table['SideB.ViewData.Side0_UniqueIds']))\n",
    "    side0_umr_ids_to_remove_from_final_open_table = final_umr_table_side0_ids + list(duplicate_ids_final_umr_table_Side0)\n",
    "    \n",
    "else:\n",
    "    final_umr_table_duplicates_Side0 = pd.DataFrame()\n",
    "    final_umr_table_side0_ids = list(set(final_umr_table['SideB.ViewData.Side0_UniqueIds']))\n",
    "    side0_umr_ids_to_remove_from_final_open_table = final_umr_table_side0_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(duplicate_ids_final_umr_table_Side1) != 0):\n",
    "    final_umr_table_duplicates_Side1 = final_umr_table[final_umr_table['SideA.ViewData.Side1_UniqueIds'].isin(duplicate_ids_final_umr_table_Side1)]\n",
    "    final_umr_table_duplicates_Side1 = final_umr_table_duplicates_Side1[['SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds','SideB.ViewData.BreakID_B_side','SideA.ViewData.BreakID_A_side','Predicted_action','probability_No_pair','probability_UMB','probability_UMR']]#,'probability_UMT']]\n",
    "    final_umr_table = final_umr_table[~final_umr_table['SideA.ViewData.Side1_UniqueIds'].isin(duplicate_ids_final_umr_table_Side1)]\n",
    "    final_umr_table_side1_ids = list(set(final_umr_table['SideA.ViewData.Side1_UniqueIds'])) \n",
    "    side1_umr_ids_to_remove_from_final_open_table = final_umr_table_side1_ids + list(duplicate_ids_final_umr_table_Side1)\n",
    "\n",
    "else:\n",
    "    final_umr_table_duplicates_Side1 = pd.DataFrame()\n",
    "    final_umr_table_side1_ids = list(set(final_umr_table['SideA.ViewData.Side1_UniqueIds']))\n",
    "    side1_umr_ids_to_remove_from_final_open_table = final_umr_table_side1_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(side1_umr_ids_to_remove_from_final_open_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1st Prediction Table for One to One UMR\n",
    "    \n",
    "\n",
    "final_umr_table = final_umr_table[['SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds','SideB.ViewData.BreakID_B_side','SideA.ViewData.BreakID_A_side','Predicted_action','probability_No_pair','probability_UMB','probability_UMR']]\n",
    "\n",
    "# ## No-Pair segregation\n",
    "\n",
    "no_pair_ids_b_side, no_pair_ids_a_side = no_pair_seg(X_test)\n",
    "\n",
    "X_test[(X_test['SideA.ViewData.Side1_UniqueIds'].isin(no_pair_ids_a_side))]['Predicted_action_2'].value_counts()\n",
    "\n",
    "X_test.groupby(['SideA.ViewData.Side1_UniqueIds'])['Predicted_action_2'].unique().reset_index()\n",
    "\n",
    "X_test[X_test['SideA.ViewData.Side1_UniqueIds'].isin(no_pair_ids_a_side)]['Predicted_action_2'].value_counts()\n",
    "\n",
    "final_open_table = X_test[(X_test['SideB.ViewData.Side0_UniqueIds'].isin(no_pair_ids_b_side)) | (X_test['SideA.ViewData.Side1_UniqueIds'].isin(no_pair_ids_a_side))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_open_table = final_open_table[~final_open_table['SideA.ViewData.Side1_UniqueIds'].isin(side1_umr_ids_to_remove_from_final_open_table)]\n",
    "final_open_table = final_open_table[~final_open_table['SideB.ViewData.Side0_UniqueIds'].isin(side0_umr_ids_to_remove_from_final_open_table)]\n",
    "\n",
    "final_open_table = final_open_table[['SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds','SideB.ViewData.BreakID_B_side','SideA.ViewData.BreakID_A_side','Predicted_action_2','probability_No_pair_2','probability_UMB_2','probability_UMR']]\n",
    "\n",
    "final_open_table['probability_UMR'] = 0.00010\n",
    "final_open_table = final_open_table.rename(columns = {'Predicted_action_2':'Predicted_action','probability_No_pair_2':'probability_No_pair','probability_UMB_2':'probability_UMB'})\n",
    "\n",
    "\n",
    "b_side_open_table = final_open_table.groupby('SideB.ViewData.Side0_UniqueIds')[['probability_No_pair','probability_UMB','probability_UMR']].mean().reset_index()\n",
    "a_side_open_table = final_open_table.groupby('SideA.ViewData.Side1_UniqueIds')[['probability_No_pair','probability_UMB','probability_UMR']].mean().reset_index()\n",
    "\n",
    "a_side_open_table = a_side_open_table[a_side_open_table['SideA.ViewData.Side1_UniqueIds'].isin(no_pair_ids_a_side)]\n",
    "b_side_open_table = b_side_open_table[b_side_open_table['SideB.ViewData.Side0_UniqueIds'].isin(no_pair_ids_b_side)]\n",
    "\n",
    "b_side_open_table = b_side_open_table.reset_index().drop('index',1)\n",
    "a_side_open_table = a_side_open_table.reset_index().drop('index',1)\n",
    "\n",
    "final_no_pair_table = pd.concat([a_side_open_table,b_side_open_table], axis=0)\n",
    "final_no_pair_table = final_no_pair_table.reset_index().drop('index',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SideA.ViewData.Side1_UniqueIds</th>\n",
       "      <th>SideB.ViewData.Side0_UniqueIds</th>\n",
       "      <th>probability_No_pair</th>\n",
       "      <th>probability_UMB</th>\n",
       "      <th>probability_UMR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100_3791127028_The Bank of New York Mellon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.945703</td>\n",
       "      <td>0.054297</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>103_379982081_The Bank of New York Mellon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.996101</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10_3791057814_Credit Suisse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.806415</td>\n",
       "      <td>0.193585</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10_3791151811_Royal Bank of Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999671</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>114_3791044124_The Bank of New York Mellon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.991148</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94_3791149195_Advent Geneva</td>\n",
       "      <td>0.715987</td>\n",
       "      <td>0.284013</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95_3791149195_Advent Geneva</td>\n",
       "      <td>0.661822</td>\n",
       "      <td>0.338178</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9_3791025073_Advent Geneva</td>\n",
       "      <td>0.998878</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9_3791147265_Advent Geneva</td>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.295842</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9_3791151084_Advent Geneva</td>\n",
       "      <td>0.995998</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 SideA.ViewData.Side1_UniqueIds  \\\n",
       "0    100_3791127028_The Bank of New York Mellon   \n",
       "1     103_379982081_The Bank of New York Mellon   \n",
       "2                   10_3791057814_Credit Suisse   \n",
       "3            10_3791151811_Royal Bank of Canada   \n",
       "4    114_3791044124_The Bank of New York Mellon   \n",
       "..                                          ...   \n",
       "369                                         NaN   \n",
       "370                                         NaN   \n",
       "371                                         NaN   \n",
       "372                                         NaN   \n",
       "373                                         NaN   \n",
       "\n",
       "    SideB.ViewData.Side0_UniqueIds  probability_No_pair  probability_UMB  \\\n",
       "0                              NaN             0.945703         0.054297   \n",
       "1                              NaN             0.996101         0.003899   \n",
       "2                              NaN             0.806415         0.193585   \n",
       "3                              NaN             0.999671         0.000329   \n",
       "4                              NaN             0.991148         0.008852   \n",
       "..                             ...                  ...              ...   \n",
       "369    94_3791149195_Advent Geneva             0.715987         0.284013   \n",
       "370    95_3791149195_Advent Geneva             0.661822         0.338178   \n",
       "371     9_3791025073_Advent Geneva             0.998878         0.001122   \n",
       "372     9_3791147265_Advent Geneva             0.704158         0.295842   \n",
       "373     9_3791151084_Advent Geneva             0.995998         0.004002   \n",
       "\n",
       "     probability_UMR  \n",
       "0             0.0001  \n",
       "1             0.0001  \n",
       "2             0.0001  \n",
       "3             0.0001  \n",
       "4             0.0001  \n",
       "..               ...  \n",
       "369           0.0001  \n",
       "370           0.0001  \n",
       "371           0.0001  \n",
       "372           0.0001  \n",
       "373           0.0001  \n",
       "\n",
       "[374 rows x 5 columns]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_no_pair_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#final_no_pair_table = pd.merge(final_no_pair_table, final_open_table[['SideA.ViewData.Side1_UniqueIds','SideA.ViewData.BreakID_A_side']].drop_duplicates(), on = 'SideA.ViewData.Side1_UniqueIds', how='left')\n",
    "#final_no_pair_table = pd.merge(final_no_pair_table, final_open_table[['SideB.ViewData.Side0_UniqueIds','SideB.ViewData.BreakID_B_side']].drop_duplicates(), on = 'SideB.ViewData.Side0_UniqueIds', how='left')\n",
    "#\n",
    "\n",
    "final_no_pair_table = normalize_final_no_pair_table_col_names(fun_final_no_pair_table = final_no_pair_table)\n",
    "final_no_pair_table_copy = final_no_pair_table.copy()\n",
    "\n",
    "final_no_pair_table_copy['ViewData.Side0_UniqueIds'] = final_no_pair_table_copy['ViewData.Side0_UniqueIds'].astype(str)\n",
    "final_no_pair_table_copy['ViewData.Side1_UniqueIds'] = final_no_pair_table_copy['ViewData.Side1_UniqueIds'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViewData.Side1_UniqueIds</th>\n",
       "      <th>ViewData.Side0_UniqueIds</th>\n",
       "      <th>probability_No_pair</th>\n",
       "      <th>probability_UMB</th>\n",
       "      <th>probability_UMR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100_3791127028_The Bank of New York Mellon</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.945703</td>\n",
       "      <td>0.054297</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>103_379982081_The Bank of New York Mellon</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.996101</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10_3791057814_Credit Suisse</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.806415</td>\n",
       "      <td>0.193585</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10_3791151811_Royal Bank of Canada</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.999671</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>114_3791044124_The Bank of New York Mellon</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.991148</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>nan</td>\n",
       "      <td>94_3791149195_Advent Geneva</td>\n",
       "      <td>0.715987</td>\n",
       "      <td>0.284013</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>nan</td>\n",
       "      <td>95_3791149195_Advent Geneva</td>\n",
       "      <td>0.661822</td>\n",
       "      <td>0.338178</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>nan</td>\n",
       "      <td>9_3791025073_Advent Geneva</td>\n",
       "      <td>0.998878</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>nan</td>\n",
       "      <td>9_3791147265_Advent Geneva</td>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.295842</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>nan</td>\n",
       "      <td>9_3791151084_Advent Geneva</td>\n",
       "      <td>0.995998</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ViewData.Side1_UniqueIds     ViewData.Side0_UniqueIds  \\\n",
       "0    100_3791127028_The Bank of New York Mellon                          nan   \n",
       "1     103_379982081_The Bank of New York Mellon                          nan   \n",
       "2                   10_3791057814_Credit Suisse                          nan   \n",
       "3            10_3791151811_Royal Bank of Canada                          nan   \n",
       "4    114_3791044124_The Bank of New York Mellon                          nan   \n",
       "..                                          ...                          ...   \n",
       "369                                         nan  94_3791149195_Advent Geneva   \n",
       "370                                         nan  95_3791149195_Advent Geneva   \n",
       "371                                         nan   9_3791025073_Advent Geneva   \n",
       "372                                         nan   9_3791147265_Advent Geneva   \n",
       "373                                         nan   9_3791151084_Advent Geneva   \n",
       "\n",
       "     probability_No_pair  probability_UMB  probability_UMR  \n",
       "0               0.945703         0.054297           0.0001  \n",
       "1               0.996101         0.003899           0.0001  \n",
       "2               0.806415         0.193585           0.0001  \n",
       "3               0.999671         0.000329           0.0001  \n",
       "4               0.991148         0.008852           0.0001  \n",
       "..                   ...              ...              ...  \n",
       "369             0.715987         0.284013           0.0001  \n",
       "370             0.661822         0.338178           0.0001  \n",
       "371             0.998878         0.001122           0.0001  \n",
       "372             0.704158         0.295842           0.0001  \n",
       "373             0.995998         0.004002           0.0001  \n",
       "\n",
       "[374 rows x 5 columns]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_no_pair_table_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Side0_UniqueIds']=='None','Side0_1_UniqueIds'] = final_no_pair_table_copy['ViewData.Side1_UniqueIds']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Side1_UniqueIds']=='None','Side0_1_UniqueIds'] = final_no_pair_table_copy['ViewData.Side0_UniqueIds']\n",
    "\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Side0_UniqueIds']=='nan','Side0_1_UniqueIds'] = final_no_pair_table_copy['ViewData.Side1_UniqueIds']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Side1_UniqueIds']=='nan','Side0_1_UniqueIds'] = final_no_pair_table_copy['ViewData.Side0_UniqueIds']\n",
    "\n",
    "del final_no_pair_table_copy['ViewData.Side0_UniqueIds']\n",
    "del final_no_pair_table_copy['ViewData.Side1_UniqueIds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62_3791114597_Advent Geneva     28\n",
       "44_3791011889_Advent Geneva     28\n",
       "64_3791114597_Advent Geneva     28\n",
       "61_3791114597_Advent Geneva     24\n",
       "49_3791048390_Advent Geneva     24\n",
       "                                ..\n",
       "107_3791006998_Advent Geneva     1\n",
       "188_3791149195_Advent Geneva     1\n",
       "63_3791151084_Advent Geneva      1\n",
       "1_3791006998_Advent Geneva       1\n",
       "99_3791150882_Advent Geneva      1\n",
       "Name: SideB.ViewData.Side0_UniqueIds, Length: 146, dtype: int64"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OTM,MTO,OTO code begin\n",
    "\n",
    "# ## Remove Open Ids\n",
    "\n",
    "umr_ids_a_side = final_umr_table['SideA.ViewData.Side1_UniqueIds'].unique()\n",
    "umr_ids_b_side = final_umr_table['SideB.ViewData.Side0_UniqueIds'].unique()\n",
    "\n",
    "### Remove Open IDs\n",
    "\n",
    "X_test_left = X_test[~(X_test['SideB.ViewData.Side0_UniqueIds'].isin(no_pair_ids_b_side))]\n",
    "X_test_left = X_test_left[~(X_test_left['SideA.ViewData.Side1_UniqueIds'].isin(no_pair_ids_a_side))]\n",
    "\n",
    "## Remove UMR IDs\n",
    "\n",
    "X_test_left = X_test_left[~(X_test_left['SideA.ViewData.Side1_UniqueIds'].isin(umr_ids_a_side))]\n",
    "X_test_left = X_test_left[~(X_test_left['SideB.ViewData.Side0_UniqueIds'].isin(umr_ids_b_side))]\n",
    "\n",
    "\n",
    "X_test_left = X_test_left.reset_index().drop('index',1)\n",
    "\n",
    "# ## One to One UMB segregation\n",
    "\n",
    "X_test_left['Predicted_action_2'].value_counts()\n",
    "\n",
    "### IDs left after removing UMR ids from 0 and 1 side\n",
    "\n",
    "X_test_left = X_test_left[~(X_test_left['SideA.ViewData.Side1_UniqueIds'].isin(final_umr_table['SideA.ViewData.Side1_UniqueIds']))]\n",
    "\n",
    "X_test_left = X_test_left[~(X_test_left['SideB.ViewData.Side0_UniqueIds'].isin(final_umr_table['SideB.ViewData.Side0_UniqueIds']))]\n",
    "\n",
    "X_test_left.shape\n",
    "X_test_left['Predicted_action_2'].value_counts()\n",
    "\n",
    "X_test_left = X_test_left.drop(['SideB.ViewData._ID','SideA.ViewData._ID'],1).drop_duplicates()\n",
    "X_test_left = X_test_left.reset_index().drop('index',1)\n",
    "\n",
    "for key in X_test_left['SideB.ViewData.Side0_UniqueIds'].unique():\n",
    "    umb_ids_1 = X_test_left[(X_test_left['SideB.ViewData.Side0_UniqueIds']==key) & (X_test_left['Predicted_action_2']=='UMB_One_to_One')]['SideA.ViewData.Side1_UniqueIds'].unique()\n",
    "\n",
    "X_test_left['SideB.ViewData.Side0_UniqueIds'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Before changes on 17-12-2020\n",
    "# # ## UMR One to Many and Many to One \n",
    "\n",
    "# # ### One to Many\n",
    "# cliff_for_loop = 16\n",
    "\n",
    "# threshold_0 = X_test['SideB.ViewData.Side0_UniqueIds'].value_counts().reset_index(name='count')\n",
    "# threshold_0_umb = threshold_0[threshold_0['count']>cliff_for_loop]['index'].unique()\n",
    "# threshold_0_without_umb = threshold_0[threshold_0['count']<=cliff_for_loop]['index'].unique()\n",
    "\n",
    "# exceptions_0_umb = X_test[X_test['Predicted_action_2']=='UMB_One_to_One']['SideB.ViewData.Side0_UniqueIds'].value_counts().reset_index(name='count')\n",
    "# exceptions_0_umb_ids = exceptions_0_umb[exceptions_0_umb['count']>cliff_for_loop]['index'].unique()\n",
    "\n",
    "# many_ids_1 = []\n",
    "# one_id_0 = []\n",
    "# amount_array =[]\n",
    "# for key in X_test[~((X_test['SideB.ViewData.Side0_UniqueIds'].isin(exceptions_0_umb_ids)) | (X_test['SideB.ViewData.Side0_UniqueIds'].isin(final_umr_table['SideB.ViewData.Side0_UniqueIds'])))]['SideB.ViewData.Side0_UniqueIds'].unique():\n",
    "#     print(key)\n",
    "    \n",
    "#     if key in threshold_0_umb:\n",
    "\n",
    "#         values =  X_test[(X_test['SideB.ViewData.Side0_UniqueIds']==key) & (X_test['Predicted_action_2']=='UMB_One_to_One')]['SideA.ViewData.B-P Net Amount'].values\n",
    "#         net_sum = X_test[X_test['SideB.ViewData.Side0_UniqueIds']==key]['SideB.ViewData.Accounting Net Amount'].max()\n",
    "\n",
    "#         #memo = dict()\n",
    "#         #print(values)\n",
    "#         #print(net_sum)\n",
    "\n",
    "#         if subSum(values,net_sum) == []: \n",
    "#             #print(\"There are no valid subsets.\")\n",
    "#             amount_array = ['NULL']\n",
    "#         else:\n",
    "#             amount_array = subSum(values,net_sum)\n",
    "\n",
    "#             id1_aggregation = X_test[(X_test['SideA.ViewData.B-P Net Amount'].isin(amount_array)) & (X_test['SideB.ViewData.Side0_UniqueIds']==key)]['SideA.ViewData.Side1_UniqueIds'].values\n",
    "#             id0_unique = key       \n",
    "\n",
    "#             if len(id1_aggregation)>1: \n",
    "#                 many_ids_1.append(id1_aggregation)\n",
    "#                 one_id_0.append(id0_unique)\n",
    "#             else:\n",
    "#                 pass\n",
    "            \n",
    "#     else:\n",
    "#         values =  X_test[(X_test['SideB.ViewData.Side0_UniqueIds']==key)]['SideA.ViewData.B-P Net Amount'].values\n",
    "#         net_sum = X_test[X_test['SideB.ViewData.Side0_UniqueIds']==key]['SideB.ViewData.Accounting Net Amount'].max()\n",
    "\n",
    "#         #memo = dict()\n",
    "#         #print(values)\n",
    "#         #print(net_sum)\n",
    "\n",
    "#         if subSum(values,net_sum) == []: \n",
    "#             #print(\"There are no valid subsets.\")\n",
    "#             amount_array = ['NULL']\n",
    "#         else:\n",
    "#             amount_array = subSum(values,net_sum)\n",
    "\n",
    "#             id1_aggregation = X_test[(X_test['SideA.ViewData.B-P Net Amount'].isin(amount_array)) & (X_test['SideB.ViewData.Side0_UniqueIds']==key)]['SideA.ViewData.Side1_UniqueIds'].values\n",
    "#             id0_unique = key       \n",
    "\n",
    "#             if len(id1_aggregation)>1: \n",
    "#                 many_ids_1.append(id1_aggregation)\n",
    "#                 one_id_0.append(id0_unique)\n",
    "#             else:\n",
    "#                 pass\n",
    "\n",
    "# umr_otm_table = pd.DataFrame(one_id_0)\n",
    "\n",
    "# if(umr_otm_table.empty == False):\n",
    "#     umr_otm_table.columns = ['SideB.ViewData.Side0_UniqueIds']\n",
    "#     umr_otm_table['SideA.ViewData.Side1_UniqueIds'] =many_ids_1 \n",
    "# else:\n",
    "#     temp_umr_otm_table_message = 'No One to many found'\n",
    "#     print(temp_umr_otm_table_message)\n",
    "\n",
    "\n",
    "# # ## Removing duplicate IDs from side 1\n",
    "\n",
    "# if(len(many_ids_1) == 0):\n",
    "#     unique_many_ids_1 = ['None']\n",
    "# else:\n",
    "#     unique_many_ids_1 = np.unique(np.concatenate(many_ids_1))\n",
    "\n",
    "# dup_ids_0 = []\n",
    "# for i in unique_many_ids_1:\n",
    "#     count =0\n",
    "#     for j in many_ids_1:\n",
    "#         if i in j:\n",
    "#             count = count+1\n",
    "#             if count==2:\n",
    "#                 dup_ids_0.append(i)\n",
    "#                 break             \n",
    "            \n",
    "# dup_array_0 = []\n",
    "# for i in many_ids_1:\n",
    "#     #print(i)\n",
    "#     if any(x in dup_ids_0 for x in i):\n",
    "#         dup_array_0.append(i)\n",
    "        \n",
    "\n",
    "# ### Converting array to list\n",
    "# dup_array_0_list = []\n",
    "# for i in dup_array_0:\n",
    "#     dup_array_0_list.append(list(i))\n",
    "    \n",
    "# many_ids_1_list =[] \n",
    "# for j in many_ids_1:\n",
    "#     many_ids_1_list.append(list(j))\n",
    "    \n",
    "    \n",
    "# filtered_otm = [i for i in many_ids_1_list if not i in dup_array_0_list]\n",
    "\n",
    "# one_id_0_final = []\n",
    "# for i, j in zip(many_ids_1_list, one_id_0):\n",
    "#     if i in filtered_otm:\n",
    "#         one_id_0_final.append(j) \n",
    "\n",
    "# umr_otm_table = umr_otm_table[umr_otm_table['SideB.ViewData.Side0_UniqueIds'].isin(one_id_0_final)]\n",
    "\n",
    "# filtered_otm_flat = [item for sublist in filtered_otm for item in sublist]\n",
    "\n",
    "\n",
    "# # ## Including UMR double count into OTM\n",
    "# umr_double_count = X_test.groupby(['SideB.ViewData.Side0_UniqueIds'])['Predicted_action'].value_counts().reset_index(name='count')\n",
    "# umr_double_count = umr_double_count[(umr_double_count['Predicted_action']=='UMR_One_to_One') & (umr_double_count['count']==2)]\n",
    "\n",
    "# umr_double_count_left = umr_double_count[~umr_double_count['SideB.ViewData.Side0_UniqueIds'].isin(umr_otm_table['SideB.ViewData.Side0_UniqueIds'].unique())]\n",
    "\n",
    "# pb_ids_otm_left = []\n",
    "# acc_id_single = []\n",
    "\n",
    "# for key in umr_double_count_left['SideB.ViewData.Side0_UniqueIds'].unique():\n",
    "#     acc_amount = X_test[X_test['SideB.ViewData.Side0_UniqueIds']==key]['SideB.ViewData.Accounting Net Amount'].max()\n",
    "#     pb_ids_otm = X_test[(X_test['SideB.ViewData.Side0_UniqueIds']==key) & ((X_test['SideB.ViewData.Accounting Net Amount']==X_test['SideA.ViewData.B-P Net Amount']) | (X_test['SideB.ViewData.Accounting Net Amount']== (-1)*X_test['SideA.ViewData.B-P Net Amount']))]['SideA.ViewData.Side1_UniqueIds'].values\n",
    "#     pb_ids_otm_left.append(pb_ids_otm)\n",
    "#     acc_id_single.append(key)\n",
    "\n",
    "# umr_otm_table_double_count = pd.DataFrame(acc_id_single)\n",
    "# if(umr_otm_table_double_count.shape[0] != 0):\n",
    "#     umr_otm_table_double_count.columns = ['SideB.ViewData.Side0_UniqueIds']\n",
    "\n",
    "#     umr_otm_table_double_count['SideA.ViewData.Side1_UniqueIds'] = pb_ids_otm_left\n",
    "\n",
    "#     umr_otm_table_final = pd.concat([umr_otm_table, umr_otm_table_double_count], axis=0)\n",
    "# else:\n",
    "#     umr_otm_table_final = umr_otm_table.copy()\n",
    "    \n",
    "# if(umr_otm_table_final.empty == False):\n",
    "#     umr_otm_table_final = umr_otm_table_final.reset_index().drop('index',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "# After changes on 17-12-2020\n",
    "cliff_for_loop = 16\n",
    "\n",
    "threshold_0 = X_test['SideB.ViewData.Side0_UniqueIds'].value_counts().reset_index(name='count')\n",
    "threshold_0_umb = threshold_0[threshold_0['count']>cliff_for_loop]['index'].unique()\n",
    "threshold_0_without_umb = threshold_0[threshold_0['count']<=cliff_for_loop]['index'].unique()\n",
    "\n",
    "exceptions_0_umb = X_test[X_test['Predicted_action_2']!='UMR_One_to_One']['SideB.ViewData.Side0_UniqueIds'].value_counts().reset_index(name='count')\n",
    "exceptions_0_umb_ids = exceptions_0_umb[exceptions_0_umb['count']>cliff_for_loop]['index'].unique()\n",
    "\n",
    "def subSum(numbers,total):\n",
    "    for length in range(1, 3):\n",
    "        if len(numbers) < length or length < 1:\n",
    "            return []\n",
    "        for index,number in enumerate(numbers):\n",
    "            if length == 1 and np.isclose(number, total, atol=0.02).any():\n",
    "                return [number]\n",
    "            subset = subSum(numbers[index+1:],total-number)\n",
    "            if subset: \n",
    "                return [number] + subset\n",
    "        return []\n",
    "               \n",
    "\n",
    "#null_value ='No'\n",
    "many_ids_1 = []\n",
    "one_id_0 = []\n",
    "amount_array =[]\n",
    "\n",
    "loop_count = 0\n",
    "for key in exceptions_0_umb['index'].unique():\n",
    "#for key in ['553_1251128974_Advent Geneva','409_1251128952_Advent Geneva']:\n",
    "    #print(key)\n",
    "    loop_count = loop_count + 1\n",
    "    print(loop_count)\n",
    "    if key in exceptions_0_umb_ids:\n",
    "        sort_data = X_test[(X_test['SideB.ViewData.Side0_UniqueIds']==key) & (X_test['Predicted_action_2']!='UMR_One_to_One')].sort_values(by = ['probability_UMB_2'], ascending =[False])\n",
    "        sort_data = sort_data.reset_index().drop('index',1)\n",
    "#        Change made on 17-12-2020. As per Pratik, we will take the first 10 values, as oaktree is smaller data. For weiss, we take 8 as weiss is too big and it takes too much time with value of 15\n",
    "        sort_data = sort_data.loc[0:10,:]\n",
    "#         sort_data = sort_data.loc[0:15,:]\n",
    "        sort_data = sort_data.drop_duplicates(subset=['SideA.ViewData.B-P Net Amount'])\n",
    "        sort_data = sort_data.reset_index().drop('index',1)\n",
    "        #print(sort_data)\n",
    "\n",
    "        values =  sort_data['SideA.ViewData.B-P Net Amount'].values\n",
    "        net_sum = sort_data['SideB.ViewData.Accounting Net Amount'].max()\n",
    "\n",
    "        #memo = dict()\n",
    "        #print(values)\n",
    "        #print(net_sum)\n",
    "\n",
    "        if subSum(values,net_sum) == []: \n",
    "            #print(\"There are no valid subsets.\")\n",
    "            amount_array = ['NULL']\n",
    "        else:\n",
    "            amount_array = subSum(values,net_sum)\n",
    "\n",
    "            id1_aggregation = sort_data[(sort_data['SideA.ViewData.B-P Net Amount'].isin(amount_array)) & (sort_data['SideB.ViewData.Side0_UniqueIds']==key)]['SideA.ViewData.Side1_UniqueIds'].values\n",
    "            id0_unique = key       \n",
    "\n",
    "            if len(id1_aggregation)>1: \n",
    "                many_ids_1.append(id1_aggregation)\n",
    "                one_id_0.append(id0_unique)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        sort_data2 = X_test[(X_test['SideB.ViewData.Side0_UniqueIds']==key) & (X_test['Predicted_action_2']!='UMR_One_to_One')].sort_values(by = ['probability_UMB_2'], ascending =[False])\n",
    "        sort_data2 = sort_data2.reset_index().drop('index',1)\n",
    "\n",
    "#        Change made on 08-12-2020. As per Pratik, we will take the first 10 values, not 15 in order to not overpredict otm and mto umrs\n",
    "        sort_data2 = sort_data2.loc[0:10,:]\n",
    "#         sort_data2 = sort_data2.loc[0:8,:]\n",
    "        \n",
    "        sort_data2 = sort_data2.drop_duplicates(subset=['SideA.ViewData.B-P Net Amount'])\n",
    "        sort_data2 = sort_data2.reset_index().drop('index',1)\n",
    "        \n",
    "\n",
    "        values =  sort_data2['SideA.ViewData.B-P Net Amount'].values\n",
    "        net_sum = sort_data2['SideB.ViewData.Accounting Net Amount'].max()\n",
    "        #values =  X_test[(X_test['SideB.ViewData.Side0_UniqueIds']==key) & (X_test['Predicted_action_2']=='UMB_One_to_One')]['SideA.ViewData.B-P Net Amount'].values\n",
    "        #net_sum = X_test[(X_test['SideB.ViewData.Side0_UniqueIds']==key)& (X_test['Predicted_action_2']=='UMB_One_to_One')]['SideB.ViewData.Accounting Net Amount'].max()\n",
    "\n",
    "        #memo = dict()\n",
    "        #print(values)\n",
    "        #print(net_sum)\n",
    "\n",
    "        if subSum(values,net_sum) == []: \n",
    "            #print(\"There are no valid subsets.\")\n",
    "            amount_array = ['NULL']\n",
    "        else:\n",
    "            amount_array = subSum(values,net_sum)\n",
    "\n",
    "            id1_aggregation = sort_data2[(sort_data2['SideA.ViewData.B-P Net Amount'].isin(amount_array)) & (sort_data2['SideB.ViewData.Side0_UniqueIds']==key)]['SideA.ViewData.Side1_UniqueIds'].values\n",
    "            id0_unique = key       \n",
    "\n",
    "            if len(id1_aggregation)>1: \n",
    "                many_ids_1.append(id1_aggregation)\n",
    "                one_id_0.append(id0_unique)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "umr_otm_table = pd.DataFrame(one_id_0)\n",
    "\n",
    "#End change\n",
    "\n",
    "\n",
    "if umr_otm_table.empty == False:\n",
    "    umr_otm_table.columns = ['SideB.ViewData.Side0_UniqueIds']\n",
    "    umr_otm_table['SideA.ViewData.Side1_UniqueIds'] =many_ids_1\n",
    "else:\n",
    "    print('No One to Many found')\n",
    "\n",
    "# ## Removing duplicate IDs from side 1\n",
    "\n",
    "if len(many_ids_1)!=0:\n",
    "    unique_many_ids_1 = np.unique(np.concatenate(many_ids_1))\n",
    "else:\n",
    "    unique_many_ids_1 = np.array(['None'])\n",
    "\n",
    "dup_ids_0 = []\n",
    "for i in unique_many_ids_1:\n",
    "    count =0\n",
    "    for j in many_ids_1:\n",
    "        if i in j:\n",
    "            count = count+1\n",
    "            if count==2:\n",
    "                dup_ids_0.append(i)\n",
    "                break             \n",
    "            \n",
    "dup_array_0 = []\n",
    "for i in many_ids_1:\n",
    "    #print(i)\n",
    "    if any(x in dup_ids_0 for x in i):\n",
    "        dup_array_0.append(i)\n",
    "        \n",
    "\n",
    "### Converting array to list\n",
    "dup_array_0_list = []\n",
    "for i in dup_array_0:\n",
    "    dup_array_0_list.append(list(i))\n",
    "    \n",
    "many_ids_1_list =[] \n",
    "for j in many_ids_1:\n",
    "    many_ids_1_list.append(list(j))\n",
    "    \n",
    "    \n",
    "filtered_otm = [i for i in many_ids_1_list if not i in dup_array_0_list]\n",
    "\n",
    "one_id_0_final = []\n",
    "for i, j in zip(many_ids_1_list, one_id_0):\n",
    "    if i in filtered_otm:\n",
    "        one_id_0_final.append(j) \n",
    "\n",
    "#meo[meo['ViewData.Side0_UniqueIds'] =='162_153156748_Advent Geneva']\n",
    "\n",
    "if len(one_id_0_final)!=0:\n",
    "    #unique_many_ids_1 = np.unique(np.concatenate(many_ids_1))\n",
    "    one_id_0_final = one_id_0_final\n",
    "else:\n",
    "    one_id_0_final = np.array(['None'])\n",
    "    \n",
    "if umr_otm_table.empty == False:    \n",
    "    umr_otm_table = umr_otm_table[umr_otm_table['SideB.ViewData.Side0_UniqueIds'].isin(one_id_0_final)]\n",
    "\n",
    "filtered_otm_flat = [item for sublist in filtered_otm for item in sublist]\n",
    "\n",
    "# ## Including UMR double count into OTM\n",
    "\n",
    "umr_double_count = X_test.groupby(['SideB.ViewData.Side0_UniqueIds'])['Predicted_action'].value_counts().reset_index(name='count')\n",
    "umr_double_count = umr_double_count[(umr_double_count['Predicted_action']=='UMR_One_to_One') & (umr_double_count['count']==2)]\n",
    "\n",
    "\n",
    "if umr_otm_table.empty == False:\n",
    "    sideb_unique = umr_otm_table['SideB.ViewData.Side0_UniqueIds'].unique()\n",
    "else:\n",
    "    sideb_unique =['None']\n",
    "if umr_double_count.empty == False:\n",
    "\n",
    "    umr_double_count_left = umr_double_count[~umr_double_count['SideB.ViewData.Side0_UniqueIds'].isin(sideb_unique)]\n",
    "\n",
    "pb_ids_otm_left = []\n",
    "acc_id_single = []\n",
    "\n",
    "for key in umr_double_count_left['SideB.ViewData.Side0_UniqueIds'].unique():\n",
    "    acc_amount = X_test[X_test['SideB.ViewData.Side0_UniqueIds']==key]['SideB.ViewData.Accounting Net Amount'].max()\n",
    "    pb_ids_otm = X_test[(X_test['SideB.ViewData.Side0_UniqueIds']==key) & ((X_test['SideB.ViewData.Accounting Net Amount']==X_test['SideA.ViewData.B-P Net Amount']) | (X_test['SideB.ViewData.Accounting Net Amount']== (-1)*X_test['SideA.ViewData.B-P Net Amount']))]['SideA.ViewData.Side1_UniqueIds'].values\n",
    "    pb_ids_otm_left.append(pb_ids_otm)\n",
    "    acc_id_single.append(key)\n",
    "    \n",
    "umr_otm_table_double_count = pd.DataFrame(acc_id_single)\n",
    "umr_otm_table_double_count.columns = ['SideB.ViewData.Side0_UniqueIds']\n",
    "\n",
    "umr_otm_table_double_count['SideA.ViewData.Side1_UniqueIds'] = pb_ids_otm_left\n",
    "\n",
    "umr_otm_table_final = pd.concat([umr_otm_table, umr_otm_table_double_count], axis=0)\n",
    "\n",
    "umr_otm_table_final = umr_otm_table_final.reset_index().drop('index',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SideB.ViewData.Side0_UniqueIds</th>\n",
       "      <th>SideA.ViewData.Side1_UniqueIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>151_3791149966_Advent Geneva</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20_3791135305_Advent Geneva</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SideB.ViewData.Side0_UniqueIds SideA.ViewData.Side1_UniqueIds\n",
       "0   151_3791149966_Advent Geneva                             []\n",
       "1    20_3791135305_Advent Geneva                             []"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umr_otm_table_double_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SideB.ViewData.Side0_UniqueIds</th>\n",
       "      <th>SideA.ViewData.Side1_UniqueIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>62_3791114597_Advent Geneva</td>\n",
       "      <td>[36_3791008925_State Street, 35_3791017949_Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>681_379919610_Advent Geneva</td>\n",
       "      <td>[132_379970783_State Street, 127_3791151084_St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>78_3791150882_Advent Geneva</td>\n",
       "      <td>[101_3791044124_The Bank of New York Mellon, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SideB.ViewData.Side0_UniqueIds  \\\n",
       "0    62_3791114597_Advent Geneva   \n",
       "1    681_379919610_Advent Geneva   \n",
       "2    78_3791150882_Advent Geneva   \n",
       "\n",
       "                      SideA.ViewData.Side1_UniqueIds  \n",
       "0  [36_3791008925_State Street, 35_3791017949_Sta...  \n",
       "1  [132_379970783_State Street, 127_3791151084_St...  \n",
       "2  [101_3791044124_The Bank of New York Mellon, 1...  "
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " umr_otm_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Before changes on 17-12-2020\n",
    "# # ### Many to One\n",
    "\n",
    "# cliff_for_loop = 16\n",
    "\n",
    "# threshold_1 = X_test['SideA.ViewData.Side1_UniqueIds'].value_counts().reset_index(name='count')\n",
    "# threshold_1_umb = threshold_1[threshold_1['count']>cliff_for_loop]['index'].unique()\n",
    "# threshold_1_without_umb = threshold_1[threshold_1['count']<=cliff_for_loop]['index'].unique()\n",
    "\n",
    "# exceptions_1_umb = X_test[X_test['Predicted_action_2']=='UMB_One_to_One']['SideA.ViewData.Side1_UniqueIds'].value_counts().reset_index(name='count')\n",
    "# exceptions_1_umb_ids = exceptions_1_umb[exceptions_1_umb['count']>cliff_for_loop]['index'].unique()\n",
    "\n",
    "# def subSum(numbers,total):\n",
    "#     for length in range(1, 4):\n",
    "#         if len(numbers) < length or length < 1:\n",
    "#             return []\n",
    "#         for index,number in enumerate(numbers):\n",
    "#             if length == 1 and np.isclose(number, total,atol=0.25).any():\n",
    "#                 return [number]\n",
    "#             subset = subSum(numbers[index+1:],total-number)\n",
    "#             if subset: \n",
    "#                 return [number] + subset\n",
    "#         return []\n",
    "\n",
    "# many_ids_0 = []\n",
    "# one_id_1 = []\n",
    "# amount_array2 =[]\n",
    "# for key in X_test[~((X_test['SideA.ViewData.Side1_UniqueIds'].isin(exceptions_1_umb_ids)) |(X_test['SideA.ViewData.Side1_UniqueIds'].isin(final_umr_table['SideA.ViewData.Side1_UniqueIds'])))]['SideA.ViewData.Side1_UniqueIds'].unique():\n",
    "#     #if key not in ['1174_379879573_State Street','201_379823765_State Street']:\n",
    "#     print(key)\n",
    "#     if key in threshold_1_umb:\n",
    "\n",
    "#         values2 =  X_test[(X_test['SideA.ViewData.Side1_UniqueIds']==key) & (X_test['Predicted_action_2']=='UMB_One_to_One')]['SideB.ViewData.Accounting Net Amount'].values\n",
    "#         net_sum2 = X_test[X_test['SideA.ViewData.Side1_UniqueIds']==key]['SideA.ViewData.B-P Net Amount'].max()\n",
    "\n",
    "#         #memo = dict()\n",
    "\n",
    "#         if subSum(values2,net_sum2) == []: \n",
    "#             amount_array2 =[]\n",
    "#             #print(\"There are no valid subsets.\")\n",
    "\n",
    "#         else:\n",
    "#             amount_array2 = subSum(values2,net_sum2)\n",
    "\n",
    "#             id0_aggregation = X_test[(X_test['SideB.ViewData.Accounting Net Amount'].isin(amount_array2)) & (X_test['SideA.ViewData.Side1_UniqueIds']==key)]['SideB.ViewData.Side0_UniqueIds'].values\n",
    "#             id1_unique = key       \n",
    "\n",
    "#             if len(id0_aggregation)>1: \n",
    "#                 many_ids_0.append(id0_aggregation)\n",
    "#                 one_id_1.append(id1_unique)\n",
    "#             else:\n",
    "#                 pass\n",
    "\n",
    "#     else:\n",
    "#         values2 =  X_test[(X_test['SideA.ViewData.Side1_UniqueIds']==key)]['SideB.ViewData.Accounting Net Amount'].values\n",
    "#         net_sum2 = X_test[X_test['SideA.ViewData.Side1_UniqueIds']==key]['SideA.ViewData.B-P Net Amount'].max()\n",
    "\n",
    "#         #memo = dict()\n",
    "\n",
    "#         if subSum(values2,net_sum2) == []: \n",
    "#             amount_array2 =[]\n",
    "#             #print(\"There are no valid subsets.\")\n",
    "\n",
    "#         else:\n",
    "#             amount_array2 = subSum(values2,net_sum2)\n",
    "\n",
    "#             id0_aggregation = X_test[(X_test['SideB.ViewData.Accounting Net Amount'].isin(amount_array2)) & (X_test['SideA.ViewData.Side1_UniqueIds']==key)]['SideB.ViewData.Side0_UniqueIds'].values\n",
    "#             id1_unique = key       \n",
    "\n",
    "#             if len(id0_aggregation)>1: \n",
    "#                 many_ids_0.append(id0_aggregation)\n",
    "#                 one_id_1.append(id1_unique)\n",
    "#             else:\n",
    "#                 pass\n",
    "\n",
    "# umr_mto_table = pd.DataFrame(one_id_1)\n",
    "# if(umr_mto_table.empty == False):\n",
    "#     umr_mto_table.columns = ['SideA.ViewData.Side1_UniqueIds']\n",
    "#     umr_mto_table['SideB.ViewData.Side0_UniqueIds'] =many_ids_0 \n",
    "# #    umr_mto_table = umr_mto_table[umr_mto_table['SideA.ViewData.Side1_UniqueIds'].isin(one_id_1_final)]\n",
    "# #    for i in range(0,umr_mto_table.shape[0]):\n",
    "# #        umr_mto_table['BreakID_Side0'].iloc[i] = list(meo_df[meo_df['ViewData.Side0_UniqueIds'].isin(umr_mto_table['SideB.ViewData.Side0_UniqueIds'].values[i])]['ViewData.BreakID'])\n",
    "#         #        fun_otm_mto_df['BreakID_Side1'].iloc[i] = list(fun_meo_df[fun_meo_df['ViewData.Side1_UniqueIds'].isin(fun_otm_mto_df['SideA.ViewData.Side1_UniqueIds'].iloc[i])]['ViewData.BreakID'])\n",
    "\n",
    "# else:\n",
    "#     temp_umr_mto_table_message = 'No Many to One found'\n",
    "#     print(temp_umr_mto_table_message)\n",
    "\n",
    "# # ## Removing duplicate IDs from side 0\n",
    "\n",
    "# if(len(many_ids_0) == 0):\n",
    "#     unique_many_ids_0 = ['None']\n",
    "# else:\n",
    "#     unique_many_ids_0 = np.unique(np.concatenate(many_ids_0))\n",
    "\n",
    "# dup_ids_1 = []\n",
    "# for i in unique_many_ids_0:\n",
    "#     count =0\n",
    "#     for j in many_ids_0:\n",
    "#         if i in j:\n",
    "#             count = count+1\n",
    "#             if count==2:\n",
    "#                 dup_ids_1.append(i)\n",
    "#                 break             \n",
    "            \n",
    "# dup_array_1 = []\n",
    "# for i in many_ids_0:\n",
    "#     #print(i)\n",
    "#     if any(x in dup_ids_1 for x in i):\n",
    "#         dup_array_1.append(i)\n",
    "        \n",
    "\n",
    "# ### Converting array to list\n",
    "# dup_array_1_list = []\n",
    "# for i in dup_array_1:\n",
    "#     dup_array_1_list.append(list(i))\n",
    "    \n",
    "# many_ids_0_list =[] \n",
    "# for j in many_ids_0:\n",
    "#     many_ids_0_list.append(list(j))\n",
    "    \n",
    "    \n",
    "# filtered_mto = [i for i in many_ids_0_list if not i in dup_array_1_list]\n",
    "\n",
    "# one_id_1_final = []\n",
    "# for i, j in zip(many_ids_0_list, one_id_1):\n",
    "#     if i in filtered_mto:\n",
    "#         one_id_1_final.append(j) \n",
    "\n",
    "\n",
    "# #pd.set_option('max_columns',50)\n",
    "# if(umr_mto_table.empty == False):\n",
    "#     umr_mto_table = umr_mto_table[umr_mto_table['SideA.ViewData.Side1_UniqueIds'].isin(one_id_1_final)]\n",
    "#     umr_mto_table['BreakID_Side0'] = umr_mto_table.apply(lambda x: list(meo_df[meo_df['ViewData.Side0_UniqueIds'].isin(umr_otm_table_final['SideB.ViewData.Side0_UniqueIds'])]['ViewData.BreakID']), axis=1)\n",
    "#     for i in range(0,umr_mto_table.shape[0]):\n",
    "#         umr_mto_table['BreakID_Side0'].iloc[i] = list(meo_df[meo_df['ViewData.Side0_UniqueIds'].isin(umr_mto_table['SideB.ViewData.Side0_UniqueIds'].values[i])]['ViewData.BreakID'])#        fun_otm_mto_df['BreakID_Side1'].iloc[i] = list(fun_meo_df[fun_meo_df['ViewData.Side1_UniqueIds'].isin(fun_otm_mto_df['SideA.ViewData.Side1_UniqueIds'].iloc[i])]['ViewData.BreakID'])\n",
    "\n",
    "# else:\n",
    "#     temp_umr_mto_table_message = 'No Many to One found'\n",
    "#     print(temp_umr_mto_table_message)\n",
    "\n",
    "\n",
    "# filtered_mto_flat = [item for sublist in filtered_mto for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "cliff_for_loop = 17\n",
    "\n",
    "threshold_1 = X_test['SideA.ViewData.Side1_UniqueIds'].value_counts().reset_index(name='count')\n",
    "threshold_1_umb = threshold_1[threshold_1['count']>cliff_for_loop]['index'].unique()\n",
    "threshold_1_without_umb = threshold_1[threshold_1['count']<=cliff_for_loop]['index'].unique()\n",
    "\n",
    "exceptions_1_umb = X_test[X_test['Predicted_action_2']!='UMR_One_to_One']['SideA.ViewData.Side1_UniqueIds'].value_counts().reset_index(name='count')\n",
    "exceptions_1_umb_ids = exceptions_1_umb[exceptions_1_umb['count']>cliff_for_loop]['index'].unique()\n",
    "\n",
    "def subSum(numbers,total):\n",
    "    for length in range(1, 3):\n",
    "        if len(numbers) < length or length < 1:\n",
    "            return []\n",
    "        for index,number in enumerate(numbers):\n",
    "            if length == 1 and np.isclose(number, total, atol=0.02).any():\n",
    "                return [number]\n",
    "            subset = subSum(numbers[index+1:],total-number)\n",
    "            if subset: \n",
    "                return [number] + subset\n",
    "        return []\n",
    "               \n",
    "#null_value ='No'\n",
    "many_ids_0 = []\n",
    "one_id_1 = []\n",
    "amount_array_2 =[]\n",
    "\n",
    "loop_count = 0\n",
    "for key in exceptions_1_umb['index'].unique():\n",
    "#for key in ['553_1251128974_Advent Geneva','409_1251128952_Advent Geneva']:\n",
    "    #print(key)\n",
    "    loop_count = loop_count + 1\n",
    "    print(loop_count)\n",
    "    if key in exceptions_1_umb_ids:\n",
    "        sort_data = X_test[(X_test['SideA.ViewData.Side1_UniqueIds']==key) & (X_test['Predicted_action_2']!='UMR_One_to_One')].sort_values(by = ['probability_UMB_2'], ascending =[False])\n",
    "        sort_data = sort_data.reset_index().drop('index',1)\n",
    "        \n",
    "#        Change made on 08-12-2020. As per Pratik, we will take the first 10 values, as oaktree is smaller data. For weiss, we take 8 as weiss is too big and it takes too much time with value of 15\n",
    "        sort_data = sort_data.loc[0:10,:]\n",
    "#         sort_data = sort_data.loc[0:15,:]\n",
    "        sort_data = sort_data.drop_duplicates(subset=['SideB.ViewData.Accounting Net Amount'])\n",
    "        sort_data = sort_data.reset_index().drop('index',1)\n",
    "        #print(sort_data)\n",
    "\n",
    "        values2 =  sort_data['SideB.ViewData.Accounting Net Amount'].values\n",
    "        net_sum2 = sort_data['SideA.ViewData.B-P Net Amount'].max()\n",
    "\n",
    "        #memo = dict()\n",
    "        #print(values)\n",
    "        #print(net_sum)\n",
    "\n",
    "        if subSum(values2,net_sum2) == []: \n",
    "            #print(\"There are no valid subsets.\")\n",
    "            amount_array2 = ['NULL']\n",
    "        else:\n",
    "            amount_array2 = subSum(values2,net_sum2)\n",
    "\n",
    "            id0_aggregation = sort_data[(sort_data['SideB.ViewData.Accounting Net Amount'].isin(amount_array2)) & (sort_data['SideA.ViewData.Side1_UniqueIds']==key)]['SideB.ViewData.Side0_UniqueIds'].values\n",
    "            id1_unique = key       \n",
    "\n",
    "            if len(id0_aggregation)>1: \n",
    "                many_ids_0.append(id0_aggregation)\n",
    "                one_id_1.append(id1_unique)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        sort_data2 = X_test[(X_test['SideA.ViewData.Side1_UniqueIds']==key) & (X_test['Predicted_action_2']!='UMR_One_to_One')].sort_values(by = ['probability_UMB_2'], ascending =[False])\n",
    "        sort_data2 = sort_data2.reset_index().drop('index',1)\n",
    "        \n",
    "#        Change made on 08-12-2020. As per Pratik, we will take the first 10 values, not 15 in order to not overpredict otm and mto umrs\n",
    "        sort_data2 = sort_data2.loc[0:10,:]\n",
    "#         sort_data2 = sort_data2.loc[0:8,:]\n",
    "        sort_data2 = sort_data2.drop_duplicates(subset=['SideB.ViewData.Accounting Net Amount'])\n",
    "        sort_data2 = sort_data2.reset_index().drop('index',1)\n",
    "        \n",
    "\n",
    "        values2 =  sort_data2['SideB.ViewData.Accounting Net Amount'].values\n",
    "        net_sum2 = sort_data2['SideA.ViewData.B-P Net Amount'].max()\n",
    "        #values =  X_test[(X_test['SideB.ViewData.Side0_UniqueIds']==key) & (X_test['Predicted_action_2']=='UMB_One_to_One')]['SideA.ViewData.B-P Net Amount'].values\n",
    "        #net_sum = X_test[(X_test['SideB.ViewData.Side0_UniqueIds']==key)& (X_test['Predicted_action_2']=='UMB_One_to_One')]['SideB.ViewData.Accounting Net Amount'].max()\n",
    "\n",
    "        #memo = dict()\n",
    "        #print(values)\n",
    "        #print(net_sum)\n",
    "\n",
    "\n",
    "        if subSum(values2,net_sum2) == []: \n",
    "            #print(\"There are no valid subsets.\")\n",
    "            amount_array2 = ['NULL']\n",
    "        else:\n",
    "            amount_array2 = subSum(values2,net_sum2)\n",
    "\n",
    "            id0_aggregation = sort_data2[(sort_data2['SideB.ViewData.Accounting Net Amount'].isin(amount_array2)) & (sort_data2['SideA.ViewData.Side1_UniqueIds']==key)]['SideB.ViewData.Side0_UniqueIds'].values\n",
    "            id1_unique = key       \n",
    "\n",
    "            if len(id0_aggregation)>1: \n",
    "                many_ids_0.append(id0_aggregation)\n",
    "                one_id_1.append(id1_unique)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "umr_mto_table = pd.DataFrame(one_id_1)\n",
    "\n",
    "#End change\n",
    "\n",
    "if umr_mto_table.empty == False:\n",
    "    umr_mto_table.columns = ['SideA.ViewData.Side1_UniqueIds']\n",
    "    umr_mto_table['SideB.ViewData.Side0_UniqueIds'] =many_ids_0\n",
    "else:\n",
    "    print('No Many to One found')\n",
    "\n",
    "# ## Removing duplicate IDs from side 0\n",
    "\n",
    "if len(many_ids_0)!=0:\n",
    "    unique_many_ids_0 = np.unique(np.concatenate(many_ids_0))\n",
    "else:\n",
    "    unique_many_ids_0 = np.array(['None'])\n",
    "\n",
    "dup_ids_1 = []\n",
    "for i in unique_many_ids_0:\n",
    "    count =0\n",
    "    for j in many_ids_0:\n",
    "        if i in j:\n",
    "            count = count+1\n",
    "            if count==2:\n",
    "                dup_ids_1.append(i)\n",
    "                break             \n",
    "            \n",
    "dup_array_1 = []\n",
    "for i in many_ids_0:\n",
    "    #print(i)\n",
    "    if any(x in dup_ids_1 for x in i):\n",
    "        dup_array_1.append(i)\n",
    " \n",
    "# Converting array to list\n",
    "dup_array_1_list = []\n",
    "for i in dup_array_1:\n",
    "    dup_array_1_list.append(list(i))\n",
    "    \n",
    "many_ids_0_list =[] \n",
    "for j in many_ids_0:\n",
    "    many_ids_0_list.append(list(j))\n",
    "    \n",
    "    \n",
    "filtered_mto = [i for i in many_ids_0_list if not i in dup_array_1_list]\n",
    "\n",
    "one_id_1_final = []\n",
    "for i, j in zip(many_ids_0_list, one_id_1):\n",
    "    if i in filtered_mto:\n",
    "        one_id_1_final.append(j) \n",
    "\n",
    "if len(one_id_1_final)!=0:\n",
    "    #unique_many_ids_1 = np.unique(np.concatenate(many_ids_1))\n",
    "    one_id_1_final = one_id_1_final\n",
    "else:\n",
    "    one_id_1_final = np.array(['None'])\n",
    "\n",
    "#umr_otm_table = umr_otm_table[umr_otm_table['SideB.ViewData.Side0_UniqueIds'].isin(one_id_0_final)]\n",
    "#umr_mto_table = umr_mto_table[umr_mto_table['SideA.ViewData.Side1_UniqueIds'].isin(one_id_1_final)]\n",
    "#umr_mto_table = umr_mto_table.reset_index().drop('index',1)\n",
    "\n",
    "\n",
    "if(umr_mto_table.empty == False):\n",
    "    umr_mto_table = umr_mto_table[umr_mto_table['SideA.ViewData.Side1_UniqueIds'].isin(one_id_1_final)]\n",
    "    umr_mto_table = umr_mto_table.reset_index().drop('index',1)\n",
    "#TODO : Revisit this code later - start here\n",
    "#    umr_mto_table['BreakID_Side0'] = umr_mto_table.apply(lambda x: list(meo_df[meo_df['ViewData.Side0_UniqueIds'].isin(umr_mto_table['SideB.ViewData.Side0_UniqueIds'])]['ViewData.BreakID']), axis=1)\n",
    "#    for i in range(0,umr_mto_table.shape[0]):\n",
    "#        umr_mto_table['BreakID_Side0'].iloc[i] = list(meo_df[meo_df['ViewData.Side0_UniqueIds'].isin(umr_mto_table['SideB.ViewData.Side0_UniqueIds'].values[i])]['ViewData.BreakID'])#        fun_otm_mto_df['BreakID_Side1'].iloc[i] = list(fun_meo_df[fun_meo_df['ViewData.Side1_UniqueIds'].isin(fun_otm_mto_df['SideA.ViewData.Side1_UniqueIds'].iloc[i])]['ViewData.BreakID'])\n",
    "#TODO : Revisit this code later - end here\n",
    "else:\n",
    "    temp_umr_mto_table_message = 'No Many to One found'\n",
    "    print(temp_umr_mto_table_message)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filtered_mto_flat = [item for sublist in filtered_mto for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Removing all the OTM and MTO Ids\n",
    "\n",
    "X_test_left2 = X_test_left[~(X_test_left['SideB.ViewData.Side0_UniqueIds'].isin(filtered_mto_flat))]\n",
    "\n",
    "X_test_left2 = X_test_left2[~(X_test_left2['SideA.ViewData.Side1_UniqueIds'].isin(list(one_id_1)))]\n",
    "\n",
    "X_test_left2 = X_test_left[~(X_test_left['SideB.ViewData.Side0_UniqueIds'].isin(filtered_otm_flat))]\n",
    "X_test_left2 = X_test_left2[~(X_test_left2['SideB.ViewData.Side0_UniqueIds'].isin(list(one_id_0)))]\n",
    "\n",
    "X_test_left2 = X_test_left2.reset_index().drop('index',1)\n",
    "\n",
    "# ## UMB one to one (final)\n",
    "\n",
    "X_test_umb = X_test_left2[X_test_left2['Predicted_action_2']=='UMB_One_to_One']\n",
    "X_test_umb = X_test_umb.reset_index().drop('index',1)\n",
    "\n",
    "one_side_unique_umb_ids = one_to_one_umb(X_test_umb)\n",
    "\n",
    "final_oto_umb_table = X_test_umb[X_test_umb['SideA.ViewData.Side1_UniqueIds'].isin(one_side_unique_umb_ids)]\n",
    "\n",
    "final_oto_umb_table = final_oto_umb_table[['SideB.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds','SideB.ViewData.BreakID_B_side','SideA.ViewData.BreakID_A_side','Predicted_action_2','probability_No_pair_2','probability_UMB_2','probability_UMR']]\n",
    "\n",
    "final_oto_umb_table['probability_UMR'] = 0.00010\n",
    "final_oto_umb_table = final_oto_umb_table.rename(columns = {'Predicted_action_2':'Predicted_action','probability_No_pair_2':'probability_No_pair','probability_UMB_2':'probability_UMB'})\n",
    "\n",
    "# ## Removing IDs from OTO UMB\n",
    "\n",
    "X_test_left3 = X_test_left2[~(X_test_left2['SideB.ViewData.Side0_UniqueIds'].isin(final_oto_umb_table['SideB.ViewData.Side0_UniqueIds']))]\n",
    "X_test_left3 = X_test_left3[~(X_test_left3['SideA.ViewData.Side1_UniqueIds'].isin(final_oto_umb_table['SideA.ViewData.Side1_UniqueIds']))]\n",
    "\n",
    "\n",
    "X_test_left3 = X_test_left3.reset_index().drop('index',1)\n",
    "\n",
    "# ## UMB One to Many and Many to One\n",
    "\n",
    "## Total IDs \n",
    "\n",
    "X_test['SideB.ViewData.Side0_UniqueIds'].nunique() + X_test['SideA.ViewData.Side1_UniqueIds'].nunique()\n",
    "X_test_left3['SideB.ViewData.Side0_UniqueIds'].nunique() + X_test_left3['SideA.ViewData.Side1_UniqueIds'].nunique()\n",
    "\n",
    "open_ids_0_last , open_ids_1_last = no_pair_seg2(X_test_left3)\n",
    "\n",
    "X_test_left3[~X_test_left3['SideB.ViewData.Side0_UniqueIds'].isin(open_ids_0_last)]\n",
    "\n",
    "X_test_left4 = X_test_left3[~((X_test_left3['SideB.ViewData.Side0_UniqueIds'].isin(open_ids_0_last)) | (X_test_left3['SideA.ViewData.Side1_UniqueIds'].isin(open_ids_1_last)))]\n",
    "\n",
    "X_test_left4 = X_test_left4.reset_index().drop('index',1)\n",
    "X_test_left4['SideB.ViewData.Side0_UniqueIds'].nunique() + X_test_left4['SideA.ViewData.Side1_UniqueIds'].nunique()\n",
    "\n",
    "# ## Many to Many new\n",
    "\n",
    "#MANY TO MANY NEW\n",
    "rr2 = X_test[X_test['Predicted_action_2']=='UMB_One_to_One'].groupby(['SideB.ViewData.Side0_UniqueIds'])['SideA.ViewData.Side1_UniqueIds'].unique().reset_index()\n",
    "rr2['SideA.ViewData.Side1_UniqueIds'] = rr2['SideA.ViewData.Side1_UniqueIds'].apply(tuple)\n",
    "\n",
    "rr2.groupby(['SideA.ViewData.Side1_UniqueIds'])['SideB.ViewData.Side0_UniqueIds'].unique().reset_index()\n",
    "\n",
    "rr2 = X_test_left4[X_test_left4['Predicted_action_2']=='UMB_One_to_One'].groupby(['SideB.ViewData.Side0_UniqueIds'])['SideA.ViewData.Side1_UniqueIds'].unique().reset_index()\n",
    "\n",
    "acc_amount = X_test[X_test['Predicted_action_2']=='UMB_One_to_One'].groupby(['SideB.ViewData.Side0_UniqueIds'])['SideB.ViewData.Accounting Net Amount'].max().reset_index()\n",
    "pb_amount_sum =  X_test[X_test['Predicted_action_2']=='UMB_One_to_One'].groupby(['SideB.ViewData.Side0_UniqueIds'])['SideA.ViewData.B-P Net Amount'].sum().reset_index()\n",
    "\n",
    "rr3 = pd.merge(rr2, acc_amount, on='SideB.ViewData.Side0_UniqueIds', how='left')\n",
    "rr4 = pd.merge(rr3, pb_amount_sum, on='SideB.ViewData.Side0_UniqueIds', how='left')\n",
    "\n",
    "rr4['SideA.ViewData.Side1_UniqueIds'] = rr4['SideA.ViewData.Side1_UniqueIds'].apply(tuple)\n",
    "\n",
    "rr5 = rr4.groupby(['SideA.ViewData.Side1_UniqueIds'])['SideB.ViewData.Side0_UniqueIds'].unique().reset_index()\n",
    "\n",
    "rr6 = pd.merge(rr5, rr4.groupby(['SideA.ViewData.Side1_UniqueIds'])['SideB.ViewData.Accounting Net Amount'].sum().reset_index(), on='SideA.ViewData.Side1_UniqueIds', how='left')\n",
    "\n",
    "rr7 = pd.merge(rr6,rr4[['SideA.ViewData.Side1_UniqueIds','SideA.ViewData.B-P Net Amount']].drop_duplicates(), on='SideA.ViewData.Side1_UniqueIds',how='left')\n",
    "\n",
    "rr7['diff'] = rr7['SideB.ViewData.Accounting Net Amount'] - rr7['SideA.ViewData.B-P Net Amount']\n",
    "\n",
    "rr7['pb_len'] = rr7['SideA.ViewData.Side1_UniqueIds'].apply(lambda x: len(x))\n",
    "rr7['acc_len'] = rr7['SideB.ViewData.Side0_UniqueIds'].apply(lambda x: len(x))\n",
    "\n",
    "rr8 = rr7[~((rr7['pb_len']==1)|(rr7['acc_len']==1))]\n",
    "rr8 = rr8.reset_index().drop('index',1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BreakID_from_list_of_Side_01_UniqueIds(fun_str_list_Side_01_UniqueIds, fun_meo_df, fun_side_0_or_1):\n",
    "    list_BreakID_corresponding_to_Side_01_UniqueIds = []\n",
    "    print(fun_str_list_Side_01_UniqueIds)\n",
    "    for str_element_Side_01_UniqueIds in fun_str_list_Side_01_UniqueIds:\n",
    "        if(fun_side_0_or_1 == 0):\n",
    "            element_BreakID_corresponding_to_Side_01_UniqueIds = fun_meo_df[fun_meo_df['ViewData.Side0_UniqueIds'].isin([str_element_Side_01_UniqueIds])]['ViewData.BreakID'].unique()\n",
    "            list_BreakID_corresponding_to_Side_01_UniqueIds.append(element_BreakID_corresponding_to_Side_01_UniqueIds[0])\n",
    "        elif(fun_side_0_or_1 == 1):\n",
    "            element_BreakID_corresponding_to_Side_01_UniqueIds = fun_meo_df[fun_meo_df['ViewData.Side1_UniqueIds'].isin([str_element_Side_01_UniqueIds])]['ViewData.BreakID'].unique()\n",
    "            list_BreakID_corresponding_to_Side_01_UniqueIds.append(element_BreakID_corresponding_to_Side_01_UniqueIds[0])\n",
    "    return(list_BreakID_corresponding_to_Side_01_UniqueIds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SideB.ViewData.Side0_UniqueIds</th>\n",
       "      <th>SideA.ViewData.Side1_UniqueIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>62_3791114597_Advent Geneva</td>\n",
       "      <td>[36_3791008925_State Street, 35_3791017949_Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>681_379919610_Advent Geneva</td>\n",
       "      <td>[132_379970783_State Street, 127_3791151084_St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>78_3791150882_Advent Geneva</td>\n",
       "      <td>[101_3791044124_The Bank of New York Mellon, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>151_3791149966_Advent Geneva</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20_3791135305_Advent Geneva</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SideB.ViewData.Side0_UniqueIds  \\\n",
       "0    62_3791114597_Advent Geneva   \n",
       "1    681_379919610_Advent Geneva   \n",
       "2    78_3791150882_Advent Geneva   \n",
       "3   151_3791149966_Advent Geneva   \n",
       "4    20_3791135305_Advent Geneva   \n",
       "\n",
       "                      SideA.ViewData.Side1_UniqueIds  \n",
       "0  [36_3791008925_State Street, 35_3791017949_Sta...  \n",
       "1  [132_379970783_State Street, 127_3791151084_St...  \n",
       "2  [101_3791044124_The Bank of New York Mellon, 1...  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umr_otm_table_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['36_3791008925_State Street' '35_3791017949_State Street'\n",
      " '33_3791017949_State Street' '271_3791001204_State Street'\n",
      " '87_3791006998_State Street']\n",
      "['132_379970783_State Street' '127_3791151084_State Street'\n",
      " '237_379926812_State Street']\n",
      "['101_3791044124_The Bank of New York Mellon'\n",
      " '104_3791044124_The Bank of New York Mellon']\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NaTType does not support timetuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-364-8ea54f78460e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%Y-%m-%dT%H:%M:%SZ'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3823\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3824\u001b[0m         \"\"\"\n\u001b[1;32m-> 3825\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3826\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1300\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-364-8ea54f78460e>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%Y-%m-%dT%H:%M:%SZ'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_otm_table_copy_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Task Business Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\nattype.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.nattype._make_error_func.f\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: NaTType does not support timetuple"
     ]
    }
   ],
   "source": [
    "# umr_otm_table_final\n",
    "#final_otm_table\n",
    "# final_otm_table_copy = final_otm_table.copy()\n",
    "final_otm_table_copy = umr_otm_table_final.copy()\n",
    "#final_otm_table_copy['BreakID_Side0'] = meo_df[meo_df['ViewData.Side0_UniqueIds'].isin(list(final_otm_table_copy['SideB.ViewData.Side0_UniqueIds']))]['ViewData.BreakID'].values\n",
    "#final_otm_table_copy['BreakID_Side0'] = final_otm_table_copy['BreakID_Side0'].astype(int)\n",
    "\n",
    "final_otm_table_copy['BreakID_Side0'] = final_otm_table_copy['SideB.ViewData.Side0_UniqueIds'].apply(lambda x : meo_df[meo_df['ViewData.Side0_UniqueIds'] == x]['ViewData.BreakID'].unique())\n",
    "\n",
    "\n",
    "final_otm_table_copy['BreakID_Side1'] = final_otm_table_copy['SideA.ViewData.Side1_UniqueIds'].apply( \\\n",
    "                                        lambda x : get_BreakID_from_list_of_Side_01_UniqueIds(fun_meo_df = meo_df, \\\n",
    "                                                                                              fun_side_0_or_1 = 1, \\\n",
    "                                                                                              fun_str_list_Side_01_UniqueIds = x))\n",
    "\n",
    "final_otm_table_copy = normalize_final_no_pair_table_col_names(fun_final_no_pair_table = final_otm_table_copy)\n",
    "#\n",
    "final_otm_table_copy['ViewData.Side0_UniqueIds'] = final_otm_table_copy['ViewData.Side0_UniqueIds'].astype(str)\n",
    "final_otm_table_copy['ViewData.Side1_UniqueIds'] = final_otm_table_copy['ViewData.Side1_UniqueIds'].astype(str)\n",
    " \n",
    "final_otm_table_copy_new = pd.merge(final_otm_table_copy, meo_df[['ViewData.Side0_UniqueIds','ViewData.Task ID','ViewData.Task Business Date','ViewData.Source Combination Code']].drop_duplicates(), on = 'ViewData.Side0_UniqueIds', how='left')\n",
    "final_otm_table_copy_new['Predicted_Status'] = 'UMR'\n",
    "final_otm_table_copy_new['Predicted_action'] = 'UMR_One-Many_to_Many-One'\n",
    "final_otm_table_copy_new['ML_flag'] = 'ML'\n",
    "final_otm_table_copy_new['SetupID'] = setup_code \n",
    "\n",
    "filepaths_final_otm_table_copy = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\UAT_Run\\\\X_Test_' + setup_code +'\\\\final_otm_table_copy_new.csv'\n",
    "final_otm_table_copy_new.to_csv(filepaths_final_otm_table_copy)\n",
    "\n",
    "change_names_of_final_otm_table_copy_new_mapping_dict = {\n",
    "                                            'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds',\n",
    "                                            'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds',\n",
    "                                            'BreakID_Side0' : 'BreakID',\n",
    "                                            'BreakID_Side1' : 'Final_predicted_break',\n",
    "                                            'ViewData.Task ID' : 'Task ID',\n",
    "                                            'ViewData.Task Business Date' : 'Task Business Date',\n",
    "                                            'ViewData.Source Combination Code' : 'Source Combination Code'\n",
    "                                        }\n",
    "\n",
    "\n",
    "final_otm_table_copy_new.rename(columns = change_names_of_final_otm_table_copy_new_mapping_dict, inplace = True)\n",
    "\n",
    "final_otm_table_copy_new['Task Business Date'] = pd.to_datetime(final_otm_table_copy_new['Task Business Date'])\n",
    "final_otm_table_copy_new['Task Business Date'] = final_otm_table_copy_new['Task Business Date'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "final_otm_table_copy_new['Task Business Date'] = pd.to_datetime(final_otm_table_copy_new['Task Business Date'])\n",
    "\n",
    "\n",
    "final_otm_table_copy_new['PredictedComment'] = ''\n",
    "\n",
    "#Changing data types of columns as follows:\n",
    "#Side0_UniqueIds, Side1_UniqueIds, Final_predicted_break, Predicted_action, probability_No_pair, probability_UMB, probability_UMR, BusinessDate, SourceCombinationCode, Predicted_Status, ML_flag - string\n",
    "#BreakID, TaskID - int64\n",
    "#SetupID - int32\n",
    "final_otm_table_copy_new['probability_UMB'] = 0.017\n",
    "final_otm_table_copy_new['probability_No_pair'] = 0.017\n",
    "final_otm_table_copy_new['probability_UMR'] = 0.95\n",
    "final_otm_table_copy_new['probability_UMT'] = 0.017\n",
    "    \n",
    "for i in range(0,final_otm_table_copy_new.shape[0]):\n",
    "    final_otm_table_copy_new['probability_UMB'].iloc[i] = float(decimal.Decimal(random.randrange(17, 100))/1000)\n",
    "    final_otm_table_copy_new['probability_No_pair'].iloc[i] = float(decimal.Decimal(random.randrange(17, 100))/1000)\n",
    "    final_otm_table_copy_new['probability_UMR'].iloc[i] = float(decimal.Decimal(random.randrange(950, 1000))/1000)\n",
    "    final_otm_table_copy_new['probability_UMT'].iloc[i] = float(decimal.Decimal(random.randrange(17, 100))/1000)\n",
    "\n",
    "\n",
    "final_otm_table_copy_new[['Side0_UniqueIds', 'Side1_UniqueIds', 'Final_predicted_break', 'Predicted_action', 'probability_No_pair', 'probability_UMB', 'probability_UMR', 'Source Combination Code', 'Predicted_Status', 'ML_flag']] = final_otm_table_copy_new[['Side0_UniqueIds', 'Side1_UniqueIds', 'Final_predicted_break', 'Predicted_action', 'probability_No_pair', 'probability_UMB', 'probability_UMR', 'Source Combination Code', 'Predicted_Status', 'ML_flag']].astype(str)\n",
    "\n",
    "final_otm_table_copy_new[['Task ID']] = final_otm_table_copy_new[['Task ID']].astype(float)\n",
    "final_otm_table_copy_new[['Task ID']] = final_otm_table_copy_new[['Task ID']].astype(np.int64)\n",
    "\n",
    "final_otm_table_copy_new[['SetupID']] = final_otm_table_copy_new[['SetupID']].astype(int)\n",
    "\n",
    "#final_table_to_write['Task ID'] = final_table_to_write['Task ID'].astype(float)\n",
    "#final_table_to_write['Task ID'] = final_table_to_write['Task ID'].astype(np.int64)\n",
    "\n",
    "change_col_names_final_otm_table_copy_new_dict = {\n",
    "                        'Task ID' : 'TaskID',\n",
    "                        'Task Business Date' : 'BusinessDate',\n",
    "                        'Source Combination Code' : 'SourceCombinationCode'\n",
    "                        }\n",
    "final_otm_table_copy_new.rename(columns = change_col_names_final_otm_table_copy_new_dict, inplace = True)\n",
    "\n",
    "cols_for_database_new = ['Side0_UniqueIds',\n",
    " 'Side1_UniqueIds',\n",
    " 'BreakID',\n",
    " 'Final_predicted_break',\n",
    " 'Predicted_action',\n",
    " 'probability_No_pair',\n",
    " 'probability_UMB',\n",
    " 'probability_UMR',\n",
    " 'probability_UMT',\n",
    " 'TaskID',\n",
    " 'BusinessDate',\n",
    " 'PredictedComment',\n",
    " 'SourceCombinationCode',\n",
    " 'Predicted_Status',\n",
    " 'ML_flag',\n",
    " 'SetupID']\n",
    "\n",
    "final_otm_table_copy_new_to_write = final_otm_table_copy_new[cols_for_database_new]\n",
    "\n",
    "filepaths_final_otm_table_copy_new_to_write = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_otm_table_copy_new_to_write_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "final_otm_table_copy_new_to_write.to_csv(filepaths_final_otm_table_copy_new_to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Side0_UniqueIds</th>\n",
       "      <th>Side1_UniqueIds</th>\n",
       "      <th>BreakID</th>\n",
       "      <th>Final_predicted_break</th>\n",
       "      <th>Task ID</th>\n",
       "      <th>Task Business Date</th>\n",
       "      <th>Source Combination Code</th>\n",
       "      <th>Predicted_Status</th>\n",
       "      <th>Predicted_action</th>\n",
       "      <th>ML_flag</th>\n",
       "      <th>SetupID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>62_3791114597_Advent Geneva</td>\n",
       "      <td>['36_3791008925_State Street' '35_3791017949_S...</td>\n",
       "      <td>[1620537140]</td>\n",
       "      <td>[1480438960, 1492493331, 1492493316, 147001142...</td>\n",
       "      <td>3.791151e+09</td>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>108916259</td>\n",
       "      <td>UMR</td>\n",
       "      <td>UMR_One-Many_to_Many-One</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>681_379919610_Advent Geneva</td>\n",
       "      <td>['132_379970783_State Street' '127_3791151084_...</td>\n",
       "      <td>[1448513996]</td>\n",
       "      <td>[1448514152, 1669584015, 1448514150]</td>\n",
       "      <td>3.791151e+09</td>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>108916259</td>\n",
       "      <td>UMR</td>\n",
       "      <td>UMR_One-Many_to_Many-One</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>78_3791150882_Advent Geneva</td>\n",
       "      <td>['101_3791044124_The Bank of New York Mellon'\\...</td>\n",
       "      <td>[1669108181]</td>\n",
       "      <td>[1527516653, 1667346215]</td>\n",
       "      <td>3.791151e+09</td>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>94406485</td>\n",
       "      <td>UMR</td>\n",
       "      <td>UMR_One-Many_to_Many-One</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>151_3791149966_Advent Geneva</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMR</td>\n",
       "      <td>UMR_One-Many_to_Many-One</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20_3791135305_Advent Geneva</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMR</td>\n",
       "      <td>UMR_One-Many_to_Many-One</td>\n",
       "      <td>ML</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Side0_UniqueIds  \\\n",
       "0   62_3791114597_Advent Geneva   \n",
       "1   681_379919610_Advent Geneva   \n",
       "2   78_3791150882_Advent Geneva   \n",
       "3  151_3791149966_Advent Geneva   \n",
       "4   20_3791135305_Advent Geneva   \n",
       "\n",
       "                                     Side1_UniqueIds       BreakID  \\\n",
       "0  ['36_3791008925_State Street' '35_3791017949_S...  [1620537140]   \n",
       "1  ['132_379970783_State Street' '127_3791151084_...  [1448513996]   \n",
       "2  ['101_3791044124_The Bank of New York Mellon'\\...  [1669108181]   \n",
       "3                                                 []            []   \n",
       "4                                                 []            []   \n",
       "\n",
       "                               Final_predicted_break       Task ID  \\\n",
       "0  [1480438960, 1492493331, 1492493316, 147001142...  3.791151e+09   \n",
       "1               [1448514152, 1669584015, 1448514150]  3.791151e+09   \n",
       "2                           [1527516653, 1667346215]  3.791151e+09   \n",
       "3                                                 []           NaN   \n",
       "4                                                 []           NaN   \n",
       "\n",
       "  Task Business Date Source Combination Code Predicted_Status  \\\n",
       "0         2020-12-16               108916259              UMR   \n",
       "1         2020-12-16               108916259              UMR   \n",
       "2         2020-12-16                94406485              UMR   \n",
       "3                NaT                     NaN              UMR   \n",
       "4                NaT                     NaN              UMR   \n",
       "\n",
       "           Predicted_action ML_flag SetupID  \n",
       "0  UMR_One-Many_to_Many-One      ML     379  \n",
       "1  UMR_One-Many_to_Many-One      ML     379  \n",
       "2  UMR_One-Many_to_Many-One      ML     379  \n",
       "3  UMR_One-Many_to_Many-One      ML     379  \n",
       "4  UMR_One-Many_to_Many-One      ML     379  "
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_otm_table_copy_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1620537140]\n",
       "1    [1448513996]\n",
       "2    [1669108181]\n",
       "3              []\n",
       "4              []\n",
       "Name: BreakID_Side0, dtype: object"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_otm_table_copy['BreakID_Side0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_otm_table_copy_new_to_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_mto_table_copy = final_mto_table.copy()\n",
    "final_mto_table_copy = umr_mto_table.copy()\n",
    "#final_mto_table_copy['BreakID_Side1'] = meo_df[meo_df['ViewData.Side1_UniqueIds'] == final_mto_table_copy['SideA.ViewData.Side1_UniqueIds']]['ViewData.BreakID'].unique()\n",
    "\n",
    "final_mto_table_copy['BreakID_Side1'] = final_mto_table_copy['SideA.ViewData.Side1_UniqueIds'].apply(lambda x : meo_df[meo_df['ViewData.Side1_UniqueIds'] == x]['ViewData.BreakID'].unique())\n",
    "\n",
    "final_mto_table_copy['BreakID_Side1'] = final_mto_table_copy['BreakID_Side1'].astype(int)\n",
    "\n",
    "\n",
    "final_mto_table_copy['BreakID_Side0'] = final_mto_table_copy['SideB.ViewData.Side0_UniqueIds'].apply( \\\n",
    "                                        lambda x : get_BreakID_from_list_of_Side_01_UniqueIds(fun_meo_df = meo_df, \\\n",
    "                                                                                              fun_side_0_or_1 = 0, \\\n",
    "                                                                                              fun_str_list_Side_01_UniqueIds = x))\n",
    "\n",
    "\n",
    "final_mto_table_copy = normalize_final_no_pair_table_col_names(fun_final_no_pair_table = final_mto_table_copy)\n",
    "#\n",
    "final_mto_table_copy['ViewData.Side0_UniqueIds'] = final_mto_table_copy['ViewData.Side0_UniqueIds'].astype(str)\n",
    "final_mto_table_copy['ViewData.Side1_UniqueIds'] = final_mto_table_copy['ViewData.Side1_UniqueIds'].astype(str)\n",
    " \n",
    "final_mto_table_copy_new = pd.merge(final_mto_table_copy, meo_df[['ViewData.Side1_UniqueIds','ViewData.Task ID','ViewData.Task Business Date','ViewData.Source Combination Code']].drop_duplicates(), on = 'ViewData.Side1_UniqueIds', how='left')\n",
    "final_mto_table_copy_new['Predicted_Status'] = 'UMR'\n",
    "final_mto_table_copy_new['Predicted_action'] = 'UMR_One-Many_to_Many-One'\n",
    "final_mto_table_copy_new['ML_flag'] = 'ML'\n",
    "final_mto_table_copy_new['SetupID'] = setup_code \n",
    "\n",
    "#filepaths_final_mto_table_copy = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\UAT_Run\\\\X_Test_' + setup_code +'\\\\final_mto_table_copy.csv'\n",
    "filepaths_final_mto_table_copy = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_mto_table_copy_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "\n",
    "final_mto_table_copy.to_csv(filepaths_final_mto_table_copy)\n",
    "\n",
    "change_names_of_final_mto_table_copy_new_mapping_dict = {\n",
    "                                            'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds',\n",
    "                                            'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds',\n",
    "                                            'BreakID_Side1' : 'BreakID',\n",
    "                                            'BreakID_Side0' : 'Final_predicted_break',\n",
    "                                            'ViewData.Task ID' : 'Task ID',\n",
    "                                            'ViewData.Task Business Date' : 'Task Business Date',\n",
    "                                            'ViewData.Source Combination Code' : 'Source Combination Code'\n",
    "                                        }\n",
    "\n",
    "\n",
    "final_mto_table_copy_new.rename(columns = change_names_of_final_mto_table_copy_new_mapping_dict, inplace = True)\n",
    "\n",
    "final_mto_table_copy_new['Task Business Date'] = pd.to_datetime(final_mto_table_copy_new['Task Business Date'])\n",
    "final_mto_table_copy_new['Task Business Date'] = final_mto_table_copy_new['Task Business Date'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "final_mto_table_copy_new['Task Business Date'] = pd.to_datetime(final_mto_table_copy_new['Task Business Date'])\n",
    "\n",
    "\n",
    "final_mto_table_copy_new['PredictedComment'] = ''\n",
    "\n",
    "#Changing data types of columns as follows:\n",
    "#Side0_UniqueIds, Side1_UniqueIds, Final_predicted_break, Predicted_action, probability_No_pair, probability_UMB, probability_UMR, BusinessDate, SourceCombinationCode, Predicted_Status, ML_flag - string\n",
    "#BreakID, TaskID - int64\n",
    "#SetupID - int32\n",
    "final_mto_table_copy_new['probability_UMB'] = 0.017\n",
    "final_mto_table_copy_new['probability_No_pair'] = 0.017\n",
    "final_mto_table_copy_new['probability_UMR'] = 0.95\n",
    "final_mto_table_copy_new['probability_UMT'] = 0.017\n",
    "    \n",
    "for i in range(0,final_mto_table_copy_new.shape[0]):\n",
    "    final_mto_table_copy_new['probability_UMB'].iloc[i] = float(decimal.Decimal(random.randrange(17, 100))/1000)\n",
    "    final_mto_table_copy_new['probability_No_pair'].iloc[i] = float(decimal.Decimal(random.randrange(17, 100))/1000)\n",
    "    final_mto_table_copy_new['probability_UMR'].iloc[i] = float(decimal.Decimal(random.randrange(950, 1000))/1000)\n",
    "    final_mto_table_copy_new['probability_UMT'].iloc[i] = float(decimal.Decimal(random.randrange(17, 100))/1000)\n",
    "\n",
    "\n",
    "final_mto_table_copy_new[['Side0_UniqueIds', 'Side1_UniqueIds', 'Final_predicted_break', 'Predicted_action', 'probability_No_pair', 'probability_UMB', 'probability_UMR', 'Source Combination Code', 'Predicted_Status', 'ML_flag']] = final_mto_table_copy_new[['Side0_UniqueIds', 'Side1_UniqueIds', 'Final_predicted_break', 'Predicted_action', 'probability_No_pair', 'probability_UMB', 'probability_UMR', 'Source Combination Code', 'Predicted_Status', 'ML_flag']].astype(str)\n",
    "\n",
    "final_mto_table_copy_new[['BreakID', 'Task ID']] = final_mto_table_copy_new[['BreakID', 'Task ID']].astype(float)\n",
    "final_mto_table_copy_new[['BreakID', 'Task ID']] = final_mto_table_copy_new[['BreakID', 'Task ID']].astype(np.int64)\n",
    "\n",
    "final_mto_table_copy_new[['SetupID']] = final_mto_table_copy_new[['SetupID']].astype(int)\n",
    "\n",
    "#final_table_to_write['Task ID'] = final_table_to_write['Task ID'].astype(float)\n",
    "#final_table_to_write['Task ID'] = final_table_to_write['Task ID'].astype(np.int64)\n",
    "\n",
    "change_col_names_final_mto_table_copy_new_dict = {\n",
    "                        'Task ID' : 'TaskID',\n",
    "                        'Task Business Date' : 'BusinessDate',\n",
    "                        'Source Combination Code' : 'SourceCombinationCode'\n",
    "                        }\n",
    "final_mto_table_copy_new.rename(columns = change_col_names_final_mto_table_copy_new_dict, inplace = True)\n",
    "\n",
    "cols_for_database_new = ['Side0_UniqueIds',\n",
    " 'Side1_UniqueIds',\n",
    " 'BreakID',\n",
    " 'Final_predicted_break',\n",
    " 'Predicted_action',\n",
    " 'probability_No_pair',\n",
    " 'probability_UMB',\n",
    " 'probability_UMR',\n",
    " 'probability_UMT',\n",
    " 'TaskID',\n",
    " 'BusinessDate',\n",
    " 'PredictedComment',\n",
    " 'SourceCombinationCode',\n",
    " 'Predicted_Status',\n",
    " 'ML_flag',\n",
    " 'SetupID']\n",
    "\n",
    "final_mto_table_copy_new_to_write = final_mto_table_copy_new[cols_for_database_new]\n",
    "\n",
    "filepaths_final_mto_table_copy_new_to_write = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_mto_table_copy_new_to_write_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "\n",
    "final_mto_table_copy_new_to_write.to_csv(filepaths_final_mto_table_copy_new_to_write)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_mto_table_copy_new_to_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_oto_umb_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_oto_umb_table = normalize_final_no_pair_table_col_names(fun_final_no_pair_table = final_oto_umb_table)\n",
    "#\n",
    "final_oto_umb_table['ViewData.Side0_UniqueIds'] = final_oto_umb_table['ViewData.Side0_UniqueIds'].astype(str)\n",
    "final_oto_umb_table['ViewData.Side1_UniqueIds'] = final_oto_umb_table['ViewData.Side1_UniqueIds'].astype(str)\n",
    " \n",
    "final_oto_umb_table_new = pd.merge(final_oto_umb_table, meo_df[['ViewData.Side1_UniqueIds','ViewData.Task ID','ViewData.Task Business Date','ViewData.Source Combination Code']].drop_duplicates(), on = 'ViewData.Side1_UniqueIds', how='left')\n",
    "#    #no_pair_ids_df = no_pair_ids_df.rename(columns={'0':'filter_key'})\n",
    "final_oto_umb_table_new['Predicted_Status'] = 'UMB'\n",
    "final_oto_umb_table_new['ML_flag'] = 'ML'\n",
    "final_oto_umb_table_new['SetupID'] = setup_code \n",
    "\n",
    "\n",
    "filepaths_final_oto_umb_table_new = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_oto_umb_table_new_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "\n",
    "final_oto_umb_table_new.to_csv(filepaths_final_oto_umb_table_new)\n",
    "\n",
    "change_names_of_final_oto_umb_table_new_mapping_dict = {\n",
    "                                            'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds',\n",
    "                                            'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds',\n",
    "                                            'ViewData.BreakID_Side1' : 'BreakID',\n",
    "                                            'ViewData.BreakID_Side0' : 'Final_predicted_break',\n",
    "                                            'ViewData.Task ID' : 'Task ID',\n",
    "                                            'ViewData.Task Business Date' : 'Task Business Date',\n",
    "                                            'ViewData.Source Combination Code' : 'Source Combination Code'\n",
    "                                        }\n",
    "\n",
    "\n",
    "final_oto_umb_table_new.rename(columns = change_names_of_final_oto_umb_table_new_mapping_dict, inplace = True)\n",
    "\n",
    "final_oto_umb_table_new['Task Business Date'] = pd.to_datetime(final_oto_umb_table_new['Task Business Date'])\n",
    "final_oto_umb_table_new['Task Business Date'] = final_oto_umb_table_new['Task Business Date'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "final_oto_umb_table_new['Task Business Date'] = pd.to_datetime(final_oto_umb_table_new['Task Business Date'])\n",
    "\n",
    "\n",
    "final_oto_umb_table_new['PredictedComment'] = ''\n",
    "\n",
    "#Changing data types of columns as follows:\n",
    "#Side0_UniqueIds, Side1_UniqueIds, Final_predicted_break, Predicted_action, probability_No_pair, probability_UMB, probability_UMR, BusinessDate, SourceCombinationCode, Predicted_Status, ML_flag - string\n",
    "#BreakID, TaskID - int64\n",
    "#SetupID - int32\n",
    "\n",
    "final_oto_umb_table_new[['Side0_UniqueIds', 'Side1_UniqueIds', 'Final_predicted_break', 'Predicted_action', 'probability_No_pair', 'probability_UMB', 'probability_UMR', 'Source Combination Code', 'Predicted_Status', 'ML_flag']] = final_oto_umb_table_new[['Side0_UniqueIds', 'Side1_UniqueIds', 'Final_predicted_break', 'Predicted_action', 'probability_No_pair', 'probability_UMB', 'probability_UMR', 'Source Combination Code', 'Predicted_Status', 'ML_flag']].astype(str)\n",
    "\n",
    "final_oto_umb_table_new[['Task ID']] = final_oto_umb_table_new[['Task ID']].astype(float)\n",
    "final_oto_umb_table_new[['Task ID']] = final_oto_umb_table_new[['Task ID']].astype(np.int64)\n",
    "\n",
    "final_oto_umb_table_new[['SetupID']] = final_oto_umb_table_new[['SetupID']].astype(int)\n",
    "\n",
    "change_col_names_final_oto_umb_table_new_dict = {\n",
    "                        'Task ID' : 'TaskID',\n",
    "                        'Task Business Date' : 'BusinessDate',\n",
    "                        'Source Combination Code' : 'SourceCombinationCode'\n",
    "                        }\n",
    "final_oto_umb_table_new.rename(columns = change_col_names_final_oto_umb_table_new_dict, inplace = True)\n",
    "\n",
    "cols_for_database_new = ['Side0_UniqueIds',\n",
    " 'Side1_UniqueIds',\n",
    " 'BreakID',\n",
    " 'Final_predicted_break',\n",
    " 'Predicted_action',\n",
    " 'probability_No_pair',\n",
    " 'probability_UMB',\n",
    " 'probability_UMR',\n",
    " 'TaskID',\n",
    " 'BusinessDate',\n",
    " 'SourceCombinationCode',\n",
    " 'Predicted_Status',\n",
    " 'ML_flag',\n",
    " 'SetupID']\n",
    "\n",
    "final_oto_umb_table_new_to_write = final_oto_umb_table_new[cols_for_database_new]\n",
    "\n",
    "filepaths_final_oto_umb_table_new_to_write = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_oto_umb_table_new_to_write_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "final_oto_umb_table_new_to_write.to_csv(filepaths_final_oto_umb_table_new_to_write)\n",
    "#OTM,MTO,OTO code end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(np.concatenate(many_ids_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fill probability between 80 - 90 for no_pair \n",
    "#open_ids_1_last\n",
    "#open_ids_0_last\n",
    "no_pair_ids_last = list(open_ids_1_last)\n",
    "for x in list(open_ids_0_last):\n",
    "    no_pair_ids_last.append(x)\n",
    "no_pair_ids_last_df = pd.DataFrame(no_pair_ids_last, columns = ['Side0_1_UniqueIds'])\n",
    "no_pair_ids_last_df = no_pair_ids_last_df[~no_pair_ids_last_df['Side0_1_UniqueIds'].isin(['None'])]\n",
    "\n",
    "final_no_pair_table_copy = final_no_pair_table_copy.append([no_pair_ids_df,no_pair_ids_last_df])\n",
    "\n",
    "#final_no_pair_table_copy = final_no_pair_table_copy.append(no_pair_ids_df)\n",
    "\n",
    "if(len(many_ids_1) != 0):\n",
    "    umr_otm_table_many_ids_1_list = list(np.concatenate(many_ids_1))\n",
    "    final_no_pair_table_copy = final_no_pair_table_copy[~final_no_pair_table_copy['Side0_1_UniqueIds'].isin(umr_otm_table_many_ids_1_list)]\n",
    "if(len(one_id_0) != 0):\n",
    "    umr_otm_table_one_id_0_list = one_id_0\n",
    "    final_no_pair_table_copy = final_no_pair_table_copy[~final_no_pair_table_copy['Side0_1_UniqueIds'].isin(umr_otm_table_one_id_0_list)]\n",
    "\n",
    "if(len(many_ids_0) != 0):\n",
    "    umr_mto_table_many_ids_0_list = list(np.concatenate(many_ids_0))\n",
    "    final_no_pair_table_copy = final_no_pair_table_copy[~final_no_pair_table_copy['Side0_1_UniqueIds'].isin(umr_mto_table_many_ids_0_list)]\n",
    "if(len(one_id_1_final) != 0):\n",
    "    umr_mto_table_one_id_1_list = one_id_1_final\n",
    "    final_no_pair_table_copy = final_no_pair_table_copy[~final_no_pair_table_copy['Side0_1_UniqueIds'].isin(umr_mto_table_one_id_1_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_no_pair_table_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_no_pair_table_copy = pd.merge(final_no_pair_table_copy, meo_df[['ViewData.Side1_UniqueIds','ViewData.BreakID','ViewData.Task ID','ViewData.Task Business Date','ViewData.Source Combination Code']].drop_duplicates(), left_on = 'Side0_1_UniqueIds',right_on = 'ViewData.Side1_UniqueIds', how='left')\n",
    "final_no_pair_table_copy = pd.merge(final_no_pair_table_copy, meo_df[['ViewData.Side0_UniqueIds','ViewData.BreakID','ViewData.Task ID','ViewData.Task Business Date','ViewData.Source Combination Code']].drop_duplicates(), left_on = 'Side0_1_UniqueIds',right_on = 'ViewData.Side0_UniqueIds', how='left')\n",
    "#    #no_pair_ids_df = no_pair_ids_df.rename(columns={'0':'filter_key'})\n",
    "final_no_pair_table_copy['Predicted_Status'] = 'OB'\n",
    "final_no_pair_table_copy['Predicted_action'] = 'No-Pair'\n",
    "final_no_pair_table_copy['ML_flag'] = 'ML'\n",
    "final_no_pair_table_copy['SetupID'] = setup_code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_no_pair_table_copy['ViewData.Task ID_x'] = final_no_pair_table_copy['ViewData.Task ID_x'].astype(str)\n",
    "final_no_pair_table_copy['ViewData.Task ID_y'] = final_no_pair_table_copy['ViewData.Task ID_y'].astype(str)\n",
    " \n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task ID_x']=='None','Task ID'] = final_no_pair_table_copy['ViewData.Task ID_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task ID_y']=='None','Task ID'] = final_no_pair_table_copy['ViewData.Task ID_x']\n",
    "\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task ID_x']=='nan','Task ID'] = final_no_pair_table_copy['ViewData.Task ID_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task ID_y']=='nan','Task ID'] = final_no_pair_table_copy['ViewData.Task ID_x']\n",
    "\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task ID_x']=='NaN','Task ID'] = final_no_pair_table_copy['ViewData.Task ID_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task ID_y']=='NaN','Task ID'] = final_no_pair_table_copy['ViewData.Task ID_x']\n",
    "\n",
    "final_no_pair_table_copy['Task ID'] = final_no_pair_table_copy['Task ID'].replace('\\.0','', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_no_pair_table_copy['ViewData.BreakID_x'] = final_no_pair_table_copy['ViewData.BreakID_x'].astype(str)\n",
    "final_no_pair_table_copy['ViewData.BreakID_y'] = final_no_pair_table_copy['ViewData.BreakID_y'].astype(str)\n",
    " \n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.BreakID_x']=='None','BreakID'] = final_no_pair_table_copy['ViewData.BreakID_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.BreakID_y']=='None','BreakID'] = final_no_pair_table_copy['ViewData.BreakID_x']\n",
    "\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.BreakID_x']=='nan','BreakID'] = final_no_pair_table_copy['ViewData.BreakID_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.BreakID_y']=='nan','BreakID'] = final_no_pair_table_copy['ViewData.BreakID_x']\n",
    "\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.BreakID_x']=='NaN','BreakID'] = final_no_pair_table_copy['ViewData.BreakID_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.BreakID_y']=='NaN','BreakID'] = final_no_pair_table_copy['ViewData.BreakID_x']\n",
    "\n",
    "final_no_pair_table_copy['BreakID'] = final_no_pair_table_copy['BreakID'].replace('\\.0','', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_no_pair_table_copy['ViewData.Task Business Date_x'] = final_no_pair_table_copy['ViewData.Task Business Date_x'].astype(str)\n",
    "final_no_pair_table_copy['ViewData.Task Business Date_y'] = final_no_pair_table_copy['ViewData.Task Business Date_y'].astype(str)\n",
    " \n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task Business Date_x']=='None','Task Business Date'] = final_no_pair_table_copy['ViewData.Task Business Date_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task Business Date_y']=='None','Task Business Date'] = final_no_pair_table_copy['ViewData.Task Business Date_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task Business Date_x']=='nan','Task Business Date'] = final_no_pair_table_copy['ViewData.Task Business Date_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task Business Date_y']=='nan','Task Business Date'] = final_no_pair_table_copy['ViewData.Task Business Date_x']\n",
    "\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task Business Date_x']=='NaN','Task Business Date'] = final_no_pair_table_copy['ViewData.Task Business Date_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task Business Date_y']=='NaN','Task Business Date'] = final_no_pair_table_copy['ViewData.Task Business Date_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task Business Date_x']=='NaT','Task Business Date'] = final_no_pair_table_copy['ViewData.Task Business Date_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Task Business Date_y']=='NaT','Task Business Date'] = final_no_pair_table_copy['ViewData.Task Business Date_x']\n",
    "\n",
    "final_no_pair_table_copy['ViewData.Source Combination Code_x'] = final_no_pair_table_copy['ViewData.Source Combination Code_x'].astype(str)\n",
    "final_no_pair_table_copy['ViewData.Source Combination Code_y'] = final_no_pair_table_copy['ViewData.Source Combination Code_y'].astype(str)\n",
    " \n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Source Combination Code_x']=='None','Source Combination Code'] = final_no_pair_table_copy['ViewData.Source Combination Code_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Source Combination Code_y']=='None','Source Combination Code'] = final_no_pair_table_copy['ViewData.Source Combination Code_x']\n",
    "\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Source Combination Code_x']=='nan','Source Combination Code'] = final_no_pair_table_copy['ViewData.Source Combination Code_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Source Combination Code_y']=='nan','Source Combination Code'] = final_no_pair_table_copy['ViewData.Source Combination Code_x']\n",
    "\n",
    "\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Source Combination Code_x']=='NaT','Source Combination Code'] = final_no_pair_table_copy['ViewData.Source Combination Code_y']\n",
    "final_no_pair_table_copy.loc[final_no_pair_table_copy['ViewData.Source Combination Code_y']=='NaT','Source Combination Code'] = final_no_pair_table_copy['ViewData.Source Combination Code_x']\n",
    "\n",
    "\n",
    "final_no_pair_table_copy['Final_predicted_break'] = ''\n",
    "\n",
    "final_no_pair_table_copy['Task Business Date'] = pd.to_datetime(final_no_pair_table_copy['Task Business Date'])\n",
    "final_no_pair_table_copy['Task Business Date'] = final_no_pair_table_copy['Task Business Date'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "final_no_pair_table_copy['Task Business Date'] = pd.to_datetime(final_no_pair_table_copy['Task Business Date'])\n",
    "\n",
    "filepaths_final_no_pair_table = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_no_pair_table_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "final_no_pair_table_copy.to_csv(filepaths_final_no_pair_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_umr_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_umr_table_copy = final_umr_table.copy()\n",
    "final_umr_table_copy = normalize_final_no_pair_table_col_names(fun_final_no_pair_table = final_umr_table_copy)\n",
    "filepaths_final_umr_table = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_umr_table_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "\n",
    "final_umr_table_copy.to_csv(filepaths_final_umr_table)\n",
    "\n",
    "\n",
    "final_umr_table_copy = pd.merge(final_umr_table_copy, meo_df[['ViewData.Side1_UniqueIds','ViewData.Task ID','ViewData.Task Business Date','ViewData.Source Combination Code']].drop_duplicates(), on = 'ViewData.Side1_UniqueIds', how='left')\n",
    "final_umr_table_copy['Predicted_Status'] = 'UMR'\n",
    "#final_umr_table_copy['Predicted_action'] = 'No-Pair'\n",
    "final_umr_table_copy['ML_flag'] = 'ML'\n",
    "final_umr_table_copy['SetupID'] = setup_code \n",
    "\n",
    "change_names_of_umr_table_mapping_dict = {\n",
    "                                            'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds',\n",
    "                                            'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds',\n",
    "                                            'ViewData.BreakID_Side0' : 'BreakID',\n",
    "                                            'ViewData.BreakID_Side1' : 'Final_predicted_break',\n",
    "                                            'ViewData.Task ID' : 'Task ID',\n",
    "                                            'ViewData.Task Business Date' : 'Task Business Date',\n",
    "                                            'ViewData.Source Combination Code' : 'Source Combination Code'\n",
    "                                        }\n",
    "\n",
    "\n",
    "\n",
    "final_umr_table_copy.rename(columns = change_names_of_umr_table_mapping_dict, inplace = True)\n",
    "\n",
    "final_umr_table_copy['Task Business Date'] = pd.to_datetime(final_umr_table_copy['Task Business Date'])\n",
    "final_umr_table_copy['Task Business Date'] = final_umr_table_copy['Task Business Date'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "final_umr_table_copy['Task Business Date'] = pd.to_datetime(final_umr_table_copy['Task Business Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_no_pair_table_copy.rename(columns = change_names_of_umr_table_mapping_dict, inplace = True)\n",
    "cols_for_database = list(final_umr_table_copy.columns)\n",
    "\n",
    "\n",
    "final_no_pair_table_to_write = final_no_pair_table_copy[cols_for_database]\n",
    "\n",
    "final_table_to_write = final_no_pair_table_to_write.append(final_umr_table_copy)\n",
    "\n",
    "final_table_to_write['PredictedComment'] = ''\n",
    "\n",
    "#Changing data types of columns as follows:\n",
    "#Side0_UniqueIds, Side1_UniqueIds, Final_predicted_break, Predicted_action, probability_No_pair, probability_UMB, probability_UMR, BusinessDate, SourceCombinationCode, Predicted_Status, ML_flag - string\n",
    "#BreakID, TaskID - int64\n",
    "#SetupID - int32\n",
    "\n",
    "final_table_to_write[['Side0_UniqueIds', 'Side1_UniqueIds', 'Final_predicted_break', 'Predicted_action', 'probability_No_pair', 'probability_UMB', 'probability_UMR', 'Source Combination Code', 'Predicted_Status', 'ML_flag']] = final_table_to_write[['Side0_UniqueIds', 'Side1_UniqueIds', 'Final_predicted_break', 'Predicted_action', 'probability_No_pair', 'probability_UMB', 'probability_UMR', 'Source Combination Code', 'Predicted_Status', 'ML_flag']].astype(str)\n",
    "\n",
    "final_table_to_write[['BreakID', 'Task ID']] = final_table_to_write[['BreakID', 'Task ID']].astype(float)\n",
    "final_table_to_write[['BreakID', 'Task ID']] = final_table_to_write[['BreakID', 'Task ID']].astype(np.int64)\n",
    "\n",
    "final_table_to_write[['SetupID']] = final_table_to_write[['SetupID']].astype(int)\n",
    "\n",
    "#final_table_to_write['Task ID'] = final_table_to_write['Task ID'].astype(float)\n",
    "#final_table_to_write['Task ID'] = final_table_to_write['Task ID'].astype(np.int64)\n",
    "\n",
    "change_col_names_dict = {\n",
    "                        'Task ID' : 'TaskID',\n",
    "                        'Task Business Date' : 'BusinessDate',\n",
    "                        'Source Combination Code' : 'SourceCombinationCode'\n",
    "                        }\n",
    "final_table_to_write.rename(columns = change_col_names_dict, inplace = True)\n",
    "\n",
    "final_table_to_write = final_table_to_write.append([final_oto_umb_table_new_to_write, \\\n",
    "                                                    umr_mto_table_new_to_write, \\\n",
    "                                                    umr_otm_table_final_new_to_write])\n",
    "\n",
    "\n",
    "#filepaths_final_table_to_write = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_table_to_write.csv'\n",
    "#final_table_to_write.to_csv(filepaths_final_table_to_write)\n",
    "#filepaths_final_no_pair_table_copy = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_no_pair_table_copy.csv'\n",
    "#filepaths_final_no_pair_table = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_no_pair_table.csv'\n",
    "#\n",
    "#filepaths_meo_df = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\meo_df.csv'\n",
    "#\n",
    "#final_no_pair_table.to_csv(filepaths_final_no_pair_table)\n",
    "#\n",
    "#\n",
    "#final_no_pair_table_copy.to_csv(filepaths_final_no_pair_table_copy)\n",
    "#meo_df.to_csv(filepaths_meo_df)\n",
    "\n",
    "\n",
    "#Closed Begins\n",
    "closed_columns_for_updation = ['ViewData.BreakID','ViewData.Task Business Date','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.Source Combination Code','ViewData.Task ID']\n",
    "\n",
    "final_closed_df = closed_df[closed_columns_for_updation]\n",
    "final_closed_df['Predicted_Status'] = 'UCB'\n",
    "final_closed_df['Predicted_action'] = 'Closed'\n",
    "final_closed_df['ML_flag'] = 'ML'\n",
    "final_closed_df['SetupID'] = setup_code \n",
    "final_closed_df['Final_predicted_break'] = ''\n",
    "final_closed_df['PredictedComment'] = ''\n",
    "final_closed_df['PredictedCategory'] = ''\n",
    "final_closed_df['probability_UMB'] = ''\n",
    "final_closed_df['probability_No_pair'] = ''\n",
    "final_closed_df['probability_UMR'] = ''\n",
    "\n",
    "final_closed_df[closed_columns_for_updation] = final_closed_df[closed_columns_for_updation].astype(str)\n",
    "change_names_of_final_closed_df_mapping_dict = {\n",
    "                                            'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds',\n",
    "                                            'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds',\n",
    "                                            'ViewData.BreakID' : 'BreakID',\n",
    "                                            'ViewData.Task ID' : 'TaskID',\n",
    "                                            'ViewData.Task Business Date' : 'BusinessDate',\n",
    "                                            'ViewData.Source Combination Code' : 'SourceCombinationCode'\n",
    "                                        }\n",
    "\n",
    "final_closed_df.rename(columns = change_names_of_final_closed_df_mapping_dict, inplace = True)\n",
    "\n",
    "final_closed_df['BusinessDate'] = pd.to_datetime(final_closed_df['BusinessDate'])\n",
    "final_closed_df['BusinessDate'] = final_closed_df['BusinessDate'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "final_closed_df['BusinessDate'] = pd.to_datetime(final_closed_df['BusinessDate'])\n",
    "\n",
    "\n",
    "#final_closed_df[[\\\n",
    "#                 'Side0_UniqueIds', \\\n",
    "#                 'Side1_UniqueIds', \\\n",
    "#                 'Final_predicted_break', \\\n",
    "#                 'Predicted_action', \\\n",
    "#                 'probability_No_pair', \\\n",
    "#                 'probability_UMB', \\\n",
    "#                 'probability_UMR', \\\n",
    "#                 'SourceCombinationCode', \\\n",
    "#                 'Predicted_Status', \\\n",
    "#                 'ML_flag']] = \\\n",
    "#                 final_table_to_write[[\\\n",
    "#                                       'Side0_UniqueIds', \\\n",
    "#                                       'Side1_UniqueIds', \\\n",
    "#                                       'Final_predicted_break', \\\n",
    "#                                       'Predicted_action', \\\n",
    "#                                       'probability_No_pair', \\\n",
    "#                                       'probability_UMB', \\\n",
    "#                                       'probability_UMR', \\\n",
    "#                                       'SourceCombinationCode', \\\n",
    "#                                       'Predicted_Status', \\\n",
    "#                                       'ML_flag']] \\\n",
    "#                 .astype(str)\n",
    "\n",
    "final_closed_df['Side0_UniqueIds'] = final_closed_df['Side0_UniqueIds'].astype(str)\n",
    "final_closed_df['Side1_UniqueIds'] = final_closed_df['Side1_UniqueIds'].astype(str)\n",
    "final_closed_df['Final_predicted_break'] = final_closed_df['Final_predicted_break'].astype(str)\n",
    "final_closed_df['Predicted_action'] = final_closed_df['Predicted_action'].astype(str)\n",
    "final_closed_df['probability_No_pair'] = final_closed_df['probability_No_pair'].astype(str)\n",
    "final_closed_df['probability_UMB'] = final_closed_df['probability_UMB'].astype(str)\n",
    "final_closed_df['probability_UMR'] = final_closed_df['probability_UMR'].astype(str)\n",
    "final_closed_df['SourceCombinationCode'] = final_closed_df['SourceCombinationCode'].astype(str)\n",
    "final_closed_df['Predicted_Status'] = final_closed_df['Predicted_Status'].astype(str)\n",
    "final_closed_df['ML_flag'] = final_closed_df['ML_flag'].astype(str)\n",
    "\n",
    "\n",
    "final_closed_df[['BreakID', 'TaskID']] = final_closed_df[['BreakID', 'TaskID']].astype(float)\n",
    "final_closed_df[['BreakID', 'TaskID']] = final_closed_df[['BreakID', 'TaskID']].astype(np.int64)\n",
    "\n",
    "final_closed_df[['SetupID']] = final_closed_df[['SetupID']].astype(int)\n",
    "filepaths_final_closed_df = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_closed_df.csv'\n",
    "final_closed_df.to_csv(filepaths_final_closed_df)\n",
    "#final_table_to_write['Task ID'] = final_table_to_write['Task ID'].astype(float)\n",
    "#final_table_to_write['Task ID'] = final_table_to_write['Task ID'].astype(np.int64)\n",
    "\n",
    "final_table_to_write = final_table_to_write.append(final_closed_df)\n",
    "#final_table_to_write = final_table_to_write.append([final_closed_df, \\\n",
    "#                                                    final_oto_umb_table_new_to_write, \\\n",
    "#                                                    umr_mto_table_new_to_write, \\\n",
    "#                                                    umr_otm_table_final_new_to_write])\n",
    "\n",
    "#Closed Ends\n",
    "\n",
    "#UMB Carry Forward Begins\n",
    "umb_carry_forward_columns_to_select_from_meo_df = ['ViewData.BreakID', \\\n",
    "                                                   'ViewData.Task Business Date', \\\n",
    "                                                   'ViewData.Side0_UniqueIds', \\\n",
    "                                                   'ViewData.Side1_UniqueIds', \\\n",
    "                                                   'ViewData.Source Combination Code', \\\n",
    "                                                   'ViewData.Task ID']\n",
    "umb_carry_forward_df = umb_carry_forward_df[umb_carry_forward_columns_to_select_from_meo_df]\n",
    "\n",
    "umb_carry_forward_df['Predicted_Status'] = 'UMB'\n",
    "# Change added on 20-12-2020 as per Pratik. Since there are a lot of UMB Carry Forwards, we will include them in our final status and show them as predicted UMBs. Therefore the ML_flah will be ML instead of Not_covered_by_ML\n",
    "# umb_carry_forward_df['Predicted_action'] = 'UMB_Carry_Forward'\n",
    "# umb_carry_forward_df['ML_flag'] = 'Not_Covered_by_ML'\n",
    "umb_carry_forward_df['Predicted_action'] = 'UMB'\n",
    "umb_carry_forward_df['ML_flag'] = 'ML'\n",
    "umb_carry_forward_df['SetupID'] = setup_code \n",
    "umb_carry_forward_df['Final_predicted_break'] = ''\n",
    "umb_carry_forward_df['PredictedComment'] = ''\n",
    "umb_carry_forward_df['PredictedCategory'] = ''\n",
    "umb_carry_forward_df['probability_UMB'] = ''\n",
    "umb_carry_forward_df['probability_No_pair'] = ''\n",
    "umb_carry_forward_df['probability_UMR'] = ''\n",
    "\n",
    "umb_carry_forward_df[umb_carry_forward_columns_to_select_from_meo_df] = umb_carry_forward_df[umb_carry_forward_columns_to_select_from_meo_df].astype(str)\n",
    "change_names_of_umb_carry_forward_df_mapping_dict = {\n",
    "                                            'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds',\n",
    "                                            'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds',\n",
    "                                            'ViewData.BreakID' : 'BreakID',\n",
    "                                            'ViewData.Task ID' : 'TaskID',\n",
    "                                            'ViewData.Task Business Date' : 'BusinessDate',\n",
    "                                            'ViewData.Source Combination Code' : 'SourceCombinationCode'\n",
    "                                        }\n",
    "\n",
    "umb_carry_forward_df.rename(columns = change_names_of_umb_carry_forward_df_mapping_dict, inplace = True)\n",
    "\n",
    "umb_carry_forward_df['BusinessDate'] = pd.to_datetime(umb_carry_forward_df['BusinessDate'])\n",
    "umb_carry_forward_df['BusinessDate'] = umb_carry_forward_df['BusinessDate'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "umb_carry_forward_df['BusinessDate'] = pd.to_datetime(umb_carry_forward_df['BusinessDate'])\n",
    "\n",
    "umb_carry_forward_df['Side0_UniqueIds'] = umb_carry_forward_df['Side0_UniqueIds'].astype(str)\n",
    "umb_carry_forward_df['Side1_UniqueIds'] = umb_carry_forward_df['Side1_UniqueIds'].astype(str)\n",
    "umb_carry_forward_df['Final_predicted_break'] = umb_carry_forward_df['Final_predicted_break'].astype(str)\n",
    "umb_carry_forward_df['Predicted_action'] = umb_carry_forward_df['Predicted_action'].astype(str)\n",
    "umb_carry_forward_df['probability_No_pair'] = umb_carry_forward_df['probability_No_pair'].astype(str)\n",
    "umb_carry_forward_df['probability_UMB'] = umb_carry_forward_df['probability_UMB'].astype(str)\n",
    "umb_carry_forward_df['probability_UMR'] = umb_carry_forward_df['probability_UMR'].astype(str)\n",
    "umb_carry_forward_df['SourceCombinationCode'] = umb_carry_forward_df['SourceCombinationCode'].astype(str)\n",
    "umb_carry_forward_df['Predicted_Status'] = umb_carry_forward_df['Predicted_Status'].astype(str)\n",
    "umb_carry_forward_df['ML_flag'] = umb_carry_forward_df['ML_flag'].astype(str)\n",
    "\n",
    "\n",
    "umb_carry_forward_df[['BreakID', 'TaskID']] = umb_carry_forward_df[['BreakID', 'TaskID']].astype(float)\n",
    "umb_carry_forward_df[['BreakID', 'TaskID']] = umb_carry_forward_df[['BreakID', 'TaskID']].astype(np.int64)\n",
    "\n",
    "umb_carry_forward_df[['SetupID']] = umb_carry_forward_df[['SetupID']].astype(int)\n",
    "filepaths_umb_carry_forward_df = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\umb_carry_forward_df.csv'\n",
    "umb_carry_forward_df.to_csv(filepaths_umb_carry_forward_df)\n",
    "\n",
    "final_table_to_write = final_table_to_write.append(umb_carry_forward_df)\n",
    "\n",
    "#UMB Carry Forward Ends\n",
    "\n",
    "# Change added on 20-12-2020 to add the following three tables to final_table_to_write:\n",
    "# 1. final_smb_ob_table_copy\n",
    "# 2. final_umb_ob_table_copy\n",
    "# 3. final_mtm_table_copy_new_to_write\n",
    "\n",
    "# Append smb_ob to final_table_to_write\n",
    "if(final_smb_ob_table_copy.shape[0] != 0):\n",
    "    final_table_to_write = final_table_to_write.append(final_smb_ob_table_copy)\n",
    "# Append umb_ob to final_table_to_write\n",
    "if(final_umb_ob_table_copy.shape[0] != 0):\n",
    "    final_table_to_write = final_table_to_write.append(final_umb_ob_table_copy)\n",
    "# Append final_mtm_table_copy_new_to_write to final_table_to_write\n",
    "if(final_mtm_table_copy_new_to_write.shape[0] != 0):\n",
    "    final_table_to_write = final_table_to_write.append(final_mtm_table_copy_new_to_write)\n",
    "\n",
    "\n",
    "filepaths_final_no_pair_table_copy = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_no_pair_table_copy.csv'\n",
    "final_no_pair_table_copy.to_csv(filepaths_final_no_pair_table_copy)\n",
    "\n",
    "filepaths_final_no_pair_table = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_no_pair_table.csv'\n",
    "final_no_pair_table.to_csv(filepaths_final_no_pair_table)\n",
    "\n",
    "filepaths_final_table_to_write = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_table_to_write_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "final_table_to_write.to_csv(filepaths_final_table_to_write)\n",
    "\n",
    "filepaths_meo_df = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\meo_df_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "meo_df.to_csv(filepaths_meo_df)\n",
    "\n",
    "def unlist_comma_separated_single_quote_string_lst(list_obj):\n",
    "    new_list = []\n",
    "    for i in list_obj:\n",
    "        list_i = list(i.replace('\\'','').split(', '))\n",
    "        for j in list_i:\n",
    "            new_list.append(j)\n",
    "    return new_list\n",
    "\n",
    "def get_remaining_breakids(fun_meo_df, fun_final_df_2):\n",
    "    \n",
    "#    fun_meo_df = fun_meo_df[~fun_meo_df['ViewData.Status'].isin(['SMT','HST', 'OC', 'CT', 'Archive','SMR','SPM'])]\n",
    "    fun_meo_df = fun_meo_df[~fun_meo_df['ViewData.Status'].isin(['SMT','HST', 'OC', 'CT', 'Archive','SMR'])]\n",
    "\n",
    "    BreakId_final_df_2 =  unlist_comma_separated_single_quote_string_lst(fun_final_df_2['BreakID'].astype(str).unique().tolist())\n",
    "#    BreakId_final_df_2 =  final_df_2['BreakID'].astype(str).unique().tolist()\n",
    "    \n",
    "    Final_predicted_breakId_final_df_2 =  unlist_comma_separated_single_quote_string_lst(fun_final_df_2['Final_predicted_break'].astype(str).unique().tolist())\n",
    "#    for i in final_df_2['Final_predicted_break'].astype(str).unique().tolist():\n",
    "#        if(',' in i):\n",
    "#            print(i)\n",
    "    BreakId_meo_df =  unlist_comma_separated_single_quote_string_lst(fun_meo_df['ViewData.BreakID'].astype(str).unique().tolist())\n",
    "    all_breakids_in_final_df_2 = set(BreakId_final_df_2).union(set(Final_predicted_breakId_final_df_2))        \n",
    "    fun_unpredicted_breakids = list(set(BreakId_meo_df) - set(all_breakids_in_final_df_2)) \n",
    "#    meo_df[meo_df['ViewData.BreakID'].isin(unpredicted_breakids)]['ViewData.Status'].value_counts()\n",
    "    return(fun_unpredicted_breakids)\n",
    "\n",
    "unpredicted_breakids = get_remaining_breakids(fun_meo_df = meo_df, fun_final_df_2 = final_table_to_write)\n",
    "#unpredicted_breakids_Predicted_Status = meo_df[meo_df['ViewData.BreakID'] == ]  \n",
    "BusinessDate_df_to_append_value = final_table_to_write['BusinessDate'].iloc[1]\n",
    "\n",
    "df_to_append= meo_df[meo_df['ViewData.BreakID'].isin(unpredicted_breakids)][['ViewData.BreakID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.Task ID','ViewData.Status','ViewData.Source Combination Code']]\n",
    "change_names_of_df_to_append_mapping_dict = {\n",
    "                                            'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds',\n",
    "                                            'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds',\n",
    "                                            'ViewData.BreakID' : 'BreakID',\n",
    "                                            'ViewData.Task ID' : 'TaskID',\n",
    "                                            'ViewData.Status' : 'Predicted_Status',\n",
    "                                            'ViewData.Source Combination Code' : 'SourceCombinationCode'\n",
    "                                        }\n",
    "\n",
    "df_to_append.rename(columns = change_names_of_df_to_append_mapping_dict, inplace = True)\n",
    "#df_to_append = pd.DataFrame()\n",
    "\n",
    "df_to_append['BusinessDate'] = BusinessDate_df_to_append_value \n",
    "df_to_append['Final_predicted_break'] = ''\n",
    "df_to_append['ML_flag'] = 'ML'\n",
    "df_to_append['Predicted_Status'] = df_to_append['Predicted_Status'].apply(lambda x : x.strip())\n",
    "df_to_append.loc[df_to_append['Predicted_Status'] != 'OB', 'Predicted_Status'] = df_to_append['Predicted_Status'] + '_Not_Covered_by_ML'\n",
    "df_to_append.loc[df_to_append['Predicted_Status'] != 'OB', 'Predicted_action'] = df_to_append['Predicted_Status'] + '_Not_Covered_by_ML'\n",
    "df_to_append.loc[df_to_append['Predicted_Status'] == 'OB', 'Predicted_Status'] = df_to_append['Predicted_Status']\n",
    "df_to_append.loc[df_to_append['Predicted_Status'] == 'OB', 'Predicted_action'] = 'No-Pair'\n",
    "df_to_append['SetupID'] = setup_code\n",
    "df_to_append['probability_No_pair'] = ''\n",
    "df_to_append['probability_UMR'] = ''\n",
    "df_to_append['probability_UMB'] = ''\n",
    "if(setup_code == '125' or setup_code == '123'):\n",
    "    df_to_append['probability_UMT'] = ''\n",
    "df_to_append['PredictedComment'] = ''\n",
    "df_to_append['PredictedCategory'] = ''\n",
    "\n",
    "filepaths_df_to_append = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\df_to_append_setup_' + setup_code + '_date_' + str(date_i) + '.csv'\n",
    "df_to_append.to_csv(filepaths_df_to_append)\n",
    "\n",
    "final_table_to_write = final_table_to_write.append(df_to_append)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data_dict = final_table_to_write.to_dict(\"records\")\n",
    "#coll_1_for_writing_prediction_data = db_1_for_MEO_data['MLPrediction_Cash']\n",
    "#coll_1_for_writing_prediction_data.insert_many(data_dict) \n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Comment\n",
    "\n",
    "#Start of Commenting\n",
    "os.chdir('D:\\\\ViteosModel\\\\Abhijeet - Comment')\n",
    "comment_df_final_list = []\n",
    "brk = final_table_to_write.copy()\n",
    "\n",
    "brk = brk[brk['Predicted_action'] == 'No-Pair']\n",
    "#meo_df = pd.read_csv('\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\Soros\\\\meo_df.csv')\n",
    "\n",
    "brk = brk.rename(columns ={'Side0_UniqueIds':'ViewData.Side0_UniqueIds',\n",
    "                         'Side1_UniqueIds':'ViewData.Side1_UniqueIds'})\n",
    "meo_df = meo_df.rename(columns ={'ViewData.B-P Net Amount':'ViewData.Cust Net Amount'\n",
    "                         })\n",
    "brk['ViewData.Side0_UniqueIds'] = brk['ViewData.Side0_UniqueIds'].fillna('AA')\n",
    "brk['ViewData.Side1_UniqueIds'] = brk['ViewData.Side1_UniqueIds'].fillna('BB')\n",
    "\n",
    "\n",
    "brk['ViewData.Side0_UniqueIds'] = brk['ViewData.Side0_UniqueIds'].replace('nan','AA')\n",
    "brk['ViewData.Side1_UniqueIds'] = brk['ViewData.Side1_UniqueIds'].replace('nan','BB')\n",
    "\n",
    "\n",
    "def fid1(a,b,c):\n",
    "    if a=='No-Pair':\n",
    "        if b =='AA':\n",
    "            return c\n",
    "        else:\n",
    "            return b\n",
    "    else:\n",
    "        return '12345'\n",
    "\n",
    "\n",
    "\n",
    "brk['final_ID'] = brk.apply(lambda row : fid1(row['Predicted_action'],row['ViewData.Side0_UniqueIds'],row['ViewData.Side1_UniqueIds']),axis =1 )\n",
    "\n",
    "\n",
    "brk['final_ID'].value_counts()\n",
    "\n",
    "\n",
    "side0_id = list(set(brk[brk['ViewData.Side1_UniqueIds'] =='BB']['ViewData.Side0_UniqueIds']))\n",
    "side1_id = list(set(brk[brk['ViewData.Side0_UniqueIds'] =='AA']['ViewData.Side1_UniqueIds']))\n",
    "\n",
    "meo1 = meo_df[meo_df['ViewData.Side0_UniqueIds'].isin(side0_id)]\n",
    "meo2 = meo_df[meo_df['ViewData.Side1_UniqueIds'].isin(side1_id)]\n",
    "\n",
    "\n",
    "frames = [meo1, meo2]\n",
    "\n",
    "df1 = pd.concat(frames)\n",
    "df1 = df1.reset_index()\n",
    "df1 = df1.drop('index', axis = 1)\n",
    "\n",
    "\n",
    "# ### Duplicate OB removal\n",
    "df1 = df1.drop_duplicates()\n",
    "\n",
    "df1['ViewData.Side0_UniqueIds'] = df1['ViewData.Side0_UniqueIds'].fillna('AA')\n",
    "df1['ViewData.Side1_UniqueIds'] = df1['ViewData.Side1_UniqueIds'].fillna('BB')\n",
    "\n",
    "def fid(a,b):\n",
    "   \n",
    "    if ( b=='BB'):\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "        \n",
    "\n",
    "\n",
    "df1['final_ID'] = df1.apply(lambda row: fid(row['ViewData.Side0_UniqueIds'],row['ViewData.Side1_UniqueIds']),axis =1)\n",
    "\n",
    "df1 = df1.sort_values(['final_ID','ViewData.Business Date'], ascending = [True, True])\n",
    "\n",
    "uni2 = df1.groupby(['final_ID','ViewData.Task Business Date']).last().reset_index()\n",
    "\n",
    "uni2 = uni2.sort_values(['final_ID','ViewData.Task Business Date'], ascending = [True, True])\n",
    "\n",
    "\n",
    "# #### Trade date vs Settle date and future dated trade\n",
    "df2 = uni2.copy()\n",
    "\n",
    "import datetime\n",
    "\n",
    "df2['ViewData.Settle Date'] = pd.to_datetime(df2['ViewData.Settle Date'])\n",
    "df2['ViewData.Trade Date'] = pd.to_datetime(df2['ViewData.Trade Date'])\n",
    "df2['ViewData.Task Business Date'] = pd.to_datetime(df2['ViewData.Task Business Date'])\n",
    "\n",
    "df2['ViewData.Task Business Date1'] = df2['ViewData.Task Business Date'].dt.date\n",
    "\n",
    "df2['ViewData.Settle Date1'] = df2['ViewData.Settle Date'].dt.date\n",
    "df2['ViewData.Trade Date1'] = df2['ViewData.Trade Date'].dt.date\n",
    "\n",
    "df2['ViewData.SettlevsTrade Date'] = (df2['ViewData.Settle Date1'] - df2['ViewData.Trade Date1']).dt.days\n",
    "df2['ViewData.SettlevsTask Date'] = (df2['ViewData.Task Business Date1'] - df2['ViewData.Settle Date1']).dt.days\n",
    "df2['ViewData.TaskvsTrade Date'] = (df2['ViewData.Task Business Date1'] - df2['ViewData.Trade Date1']).dt.days\n",
    "\n",
    "\n",
    "# ### Cleannig of the 4 variables in this\n",
    "df = pd.read_excel('Mapping variables for variable cleaning.xlsx', sheet_name='General')\n",
    "\n",
    "def make_dict(row):\n",
    "    keys_l = str(row['Keys']).lower()\n",
    "    keys_s = keys_l.split(', ')\n",
    "    keys = tuple(keys_s)\n",
    "    return keys\n",
    "\n",
    "df['tuple'] = df.apply(make_dict, axis=1)\n",
    "\n",
    "clean_map_dict = df.set_index('tuple')['Value'].to_dict()\n",
    "\n",
    "df2['ViewData.Transaction Type'] = df2['ViewData.Transaction Type'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df2['ViewData.Asset Type Category'] = df2['ViewData.Asset Type Category'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df2['ViewData.Investment Type'] = df2['ViewData.Investment Type'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df2['ViewData.Prime Broker'] = df2['ViewData.Prime Broker'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "\n",
    "def clean_mapping(item):\n",
    "    item1 = item.split()\n",
    "    \n",
    "    \n",
    "    ttype = []\n",
    "    \n",
    "    \n",
    "    for x in item1:\n",
    "        ttype1 = []\n",
    "        for key, value in clean_map_dict.items():\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "            if x in key:\n",
    "                a = value\n",
    "                ttype1.append(a)\n",
    "           \n",
    "        if len(ttype1)==0:\n",
    "            ttype1.append(x)\n",
    "        ttype = ttype + ttype1\n",
    "        \n",
    "    return ' '.join(ttype)\n",
    "        \n",
    "df2['ViewData.Transaction Type1'] = df2['ViewData.Transaction Type'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df2['ViewData.Asset Type Category1'] = df2['ViewData.Asset Type Category'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df2['ViewData.Investment Type1'] = df2['ViewData.Investment Type'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df2['ViewData.Prime Broker1'] = df2['ViewData.Prime Broker'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "\n",
    "def is_num(item):\n",
    "    try:\n",
    "        float(item)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_date_format(item):\n",
    "    try:\n",
    "        parse(item, fuzzy=False)\n",
    "        return True\n",
    "    \n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def date_edge_cases(item):\n",
    "    if len(item) == 5 and item[2] =='/' and is_num(item[:2]) and is_num(item[3:]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def comb_clean(x):\n",
    "    k = []\n",
    "    for item in x.split():\n",
    "        if ((is_num(item)==False) and (is_date_format(item)==False) and (date_edge_cases(item)==False)):\n",
    "            k.append(item)\n",
    "    return ' '.join(k)\n",
    "\n",
    "df2['ViewData.Transaction Type1'] = df2['ViewData.Transaction Type1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "\n",
    "df2['ViewData.Asset Type Category1'] = df2['ViewData.Asset Type Category1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "df2['ViewData.Investment Type1'] = df2['ViewData.Investment Type1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "df2['ViewData.Prime Broker1'] = df2['ViewData.Prime Broker1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "\n",
    "df2['ViewData.Transaction Type1'] = df2['ViewData.Transaction Type1'].apply(lambda x : 'paydown' if x=='pay down' else x)\n",
    "\n",
    "# ### Cleaning of Description\n",
    "com = pd.read_csv('desc cat with naveen oaktree.csv')\n",
    "cat_list = list(set(com['Pairing']))\n",
    "\n",
    "def descclean(com,cat_list):\n",
    "    cat_all1 = []\n",
    "    list1 = cat_list\n",
    "    m = 0\n",
    "    if (type(com) == str):\n",
    "        com = com.lower()\n",
    "        com1 =  re.split(\"[,/. \\-!?:]+\", com)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for item in list1:\n",
    "            if (type(item) == str):\n",
    "                item = item.lower()\n",
    "                item1 = item.split(' ')\n",
    "                lst3 = [value for value in item1 if value in com1] \n",
    "                if len(lst3) == len(item1):\n",
    "                    cat_all1.append(item)\n",
    "                    m = m+1\n",
    "            \n",
    "                else:\n",
    "                    m = m\n",
    "            else:\n",
    "                    m = 0\n",
    "    else:\n",
    "        m = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    if m >0 :\n",
    "        return list(set(cat_all1))\n",
    "    else:\n",
    "        if ((type(com)==str)):\n",
    "            if (len(com1)<4):\n",
    "                if ((len(com1)==1) & com1[0].startswith('20')== True):\n",
    "                    return 'swap id'\n",
    "                else:\n",
    "                    return com\n",
    "            else:\n",
    "                return 'NA'\n",
    "        else:\n",
    "            return 'NA'\n",
    "            \n",
    "df2['desc_cat'] = df2['ViewData.Description'].apply(lambda x : descclean(x,cat_list))\n",
    "\n",
    "def currcln(x):\n",
    "    if (type(x)==list):\n",
    "        return x\n",
    "      \n",
    "    else:\n",
    "       \n",
    "        \n",
    "        if x == 'NA':\n",
    "            return \"NA\"\n",
    "        elif (('dollar' in x) | ('dollars' in x )):\n",
    "            return 'dollar'\n",
    "        elif (('pound' in x) | ('pounds' in x)):\n",
    "            return 'pound'\n",
    "        elif ('yen' in x):\n",
    "            return 'yen'\n",
    "        elif ('euro' in x) :\n",
    "            return 'euro'\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "df2['desc_cat'] = df2['desc_cat'].apply(lambda x : currcln(x))\n",
    "\n",
    "com = com.drop(['var','Catogery'], axis = 1)\n",
    "\n",
    "com = com.drop_duplicates()\n",
    "\n",
    "com['Pairing'] = com['Pairing'].apply(lambda x : x.lower())\n",
    "com['replace'] = com['replace'].apply(lambda x : x.lower())\n",
    "\n",
    "def catcln1(cat,df):\n",
    "    ret = []\n",
    "    if (type(cat)==list):\n",
    "        \n",
    "        if 'equity swap settlement' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'equity swap' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'swap settlement' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'swap unwind' in cat:\n",
    "            ret.append('swap unwind')\n",
    "        #return 'swap unwind'\n",
    "   \n",
    "    \n",
    "        else:\n",
    "        \n",
    "       \n",
    "            for item in cat:\n",
    "            \n",
    "                a = df[df['Pairing']==item]['replace'].values[0]\n",
    "                if a not in ret:\n",
    "                    ret.append(a)\n",
    "        return list(set(ret))\n",
    "      \n",
    "    else:\n",
    "        return cat\n",
    "\n",
    "df2['new_desc_cat'] = df2['desc_cat'].apply(lambda x : catcln1(x,com))\n",
    "\n",
    "comp = ['inc','stk','corp ','llc','pvt','plc']\n",
    "df2['new_desc_cat'] = df2['new_desc_cat'].apply(lambda x : 'Company' if x in comp else x)\n",
    "\n",
    "def desccat(x):\n",
    "    if isinstance(x, list):\n",
    "        \n",
    "        if 'equity swap settlement' in x:\n",
    "            return 'swap settlement'\n",
    "        elif 'collateral transfer' in x:\n",
    "            return 'collateral transfer'\n",
    "        elif 'dividend' in x:\n",
    "            return 'dividend'\n",
    "        elif (('loan' in x) & ('option' in x)):\n",
    "            return 'option loan'\n",
    "        \n",
    "        elif (('interest' in x) & ('corp' in x) ):\n",
    "            return 'corp loan'\n",
    "        elif (('interest' in x) & ('loan' in x) ):\n",
    "            return 'interest'\n",
    "        else:\n",
    "            return x[0]\n",
    "    else:\n",
    "        if x == 'db_int':\n",
    "            return 'interest'\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "df2['new_desc_cat'] = df2['new_desc_cat'].apply(lambda x : desccat(x))\n",
    "\n",
    "# #### Prime Broker Creation\n",
    "df2['new_pb'] = df2['ViewData.Mapped Custodian Account'].apply(lambda x : x.split('_')[0] if type(x)==str else x)\n",
    "\n",
    "new_pb_mapping = {'GSIL':'GS','CITIGM':'CITI','JPMNA':'JPM'}\n",
    "\n",
    "def new_pf_mapping(x):\n",
    "    if x=='GSIL':\n",
    "        return 'GS'\n",
    "    elif x == 'CITIGM':\n",
    "        return 'CITI'\n",
    "    elif x == 'JPMNA':\n",
    "        return 'JPM'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "df2['new_pb'] = df2['new_pb'].apply(lambda x : new_pf_mapping(x))\n",
    "\n",
    "df2['ViewData.Prime Broker1'] = df2['ViewData.Prime Broker1'].fillna('kkk')\n",
    "\n",
    "df2['new_pb1'] = df2.apply(lambda x : x['new_pb'] if x['ViewData.Prime Broker1']=='kkk' else x['ViewData.Prime Broker1'],axis = 1)\n",
    "\n",
    "df2['new_pb1'] = df2['new_pb1'].apply(lambda x : x.lower())\n",
    "\n",
    "# #### Cancelled Trade Removal\n",
    "\n",
    "trade_types = ['buy','sell','cover short', 'sell short', 'forward', 'forwardfx', 'spotfx']\n",
    "\n",
    "dfkk = df2[df2['ViewData.Transaction Type1'].isin(trade_types)]\n",
    "\n",
    "dfk_nontrade = df2[~df2['ViewData.Transaction Type1'].isin(trade_types)]\n",
    "\n",
    "dffk2 = dfkk[dfkk['ViewData.Side0_UniqueIds']=='AA']\n",
    "dffk3 = dfkk[dfkk['ViewData.Side1_UniqueIds']=='BB']\n",
    "\n",
    "dffk4 = dfk_nontrade[dfk_nontrade['ViewData.Side0_UniqueIds']=='AA']\n",
    "dffk5 = dfk_nontrade[dfk_nontrade['ViewData.Side1_UniqueIds']=='BB']\n",
    "# #### Geneva side\n",
    "def canceltrade(x,y):\n",
    "    if x =='buy' and y>0:\n",
    "        k = 1\n",
    "    elif x =='sell' and y<0:\n",
    "        k = 1\n",
    "    else:\n",
    "        k = 0\n",
    "    return k\n",
    "\n",
    "dffk3['cancel_marker'] = dffk3.apply(lambda x : canceltrade(x['ViewData.Transaction Type1'],x['ViewData.Accounting Net Amount']), axis = 1)\n",
    "\n",
    "def cancelcomment(x,y):\n",
    "    com1 = 'This is original of cancelled trade with tran id'\n",
    "    com2 = 'on settle date'\n",
    "    com = com1 + ' ' +  str(x) + ' ' + com2 + str(y)\n",
    "    return com\n",
    "\n",
    "def cancelcomment1(x,y):\n",
    "    com1 = 'This is cancelled trade with tran id'\n",
    "    com2 = 'on settle date'\n",
    "    com = com1 + ' ' +  str(x) + ' ' + com2 + str(y)\n",
    "    return com\n",
    "\n",
    "if dffk3[dffk3['cancel_marker'] == 1].shape[0]!=0:\n",
    "    cancel_trade = list(set(dffk3[dffk3['cancel_marker'] == 1]['ViewData.Transaction ID']))\n",
    "    if len(cancel_trade)>0:\n",
    "        km = dffk3[dffk3['cancel_marker'] != 1]\n",
    "        original = km[km['ViewData.Transaction ID'].isin(cancel_trade)]\n",
    "        original['predicted category'] = 'Original of Cancelled trade'\n",
    "        if(original.shape[0]!=0):\n",
    "            original['predicted comment'] = original.apply(lambda x : cancelcomment(x['ViewData.Transaction ID'],x['ViewData.Settle Date1']), axis = 1)\n",
    "        cancellation = dffk3[dffk3['cancel_marker'] == 1]\n",
    "        cancellation['predicted category'] = 'Cancelled trade'\n",
    "        cancellation['predicted comment'] =  cancellation.apply(lambda x : cancelcomment1(x['ViewData.Transaction ID'],x['ViewData.Settle Date1']), axis = 1)\n",
    "        cancel_fin = pd.concat([original,cancellation])\n",
    "        sel_col_1 = ['final_ID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']\n",
    "        cancel_fin = cancel_fin[sel_col_1]\n",
    "        cancel_fin.to_csv('Comment file oaktree 2 sep testing p1.csv')\n",
    "        comment_df_final_list.append(cancel_fin)\n",
    "        dffk3 = dffk3[~dffk3['ViewData.Transaction ID'].isin(cancel_trade)]\n",
    "        \n",
    "    else:\n",
    "        cancellation = dffk3[dffk3['cancel_marker'] == 1]\n",
    "        cancellation['predicted category'] = 'Cancelled trade'\n",
    "        cancellation['predicted comment'] =  cancellation.apply(lambda x : cancelcomment1(x['ViewData.Transaction ID'],x['ViewData.Settle Date1']), axis = 1)\n",
    "        cancel_fin = pd.concat([original,cancellation])\n",
    "        sel_col_1 = ['final_ID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']\n",
    "        cancel_fin = cancel_fin[sel_col_1]\n",
    "        cancel_fin.to_csv('Comment file oaktree 2 sep testing no original p2.csv')\n",
    "        comment_df_final_list.append(cancel_fin)\n",
    "        dffk3 = dffk3[~dffk3['ViewData.Transaction ID'].isin(cancel_trade)]\n",
    "else:\n",
    "    dffk3 = dffk3.copy()\n",
    "\n",
    "# #### Broker side\n",
    "dffk2['cancel_marker'] = dffk2.apply(lambda x : canceltrade(x['ViewData.Transaction Type1'],x['ViewData.Cust Net Amount']), axis = 1)\n",
    "\n",
    "def amountelim(row):\n",
    "   \n",
    "   \n",
    "   \n",
    "    if (row['SideA.ViewData.Mapped Custodian Account'] == row['SideB.ViewData.Mapped Custodian Account']):\n",
    "        a = 1\n",
    "    else:\n",
    "        a = 0\n",
    "        \n",
    "    if ((row['SideB.ViewData.Cust Net Amount']) == -(row['SideA.ViewData.Cust Net Amount'])):\n",
    "        b = 1\n",
    "    else:\n",
    "        b = 0\n",
    "    \n",
    "    if (row['SideA.ViewData.Fund'] == row['SideB.ViewData.Fund']):\n",
    "        c = 1\n",
    "    else:\n",
    "        c = 0\n",
    "        \n",
    "    if (row['SideA.ViewData.Currency'] == row['SideB.ViewData.Currency']):\n",
    "        d = 1\n",
    "    else:\n",
    "        d = 0\n",
    "    \n",
    "    if (row['SideA.ViewData.Settle Date1'] == row['SideB.ViewData.Settle Date1']):\n",
    "        e = 1\n",
    "    else:\n",
    "        e = 0\n",
    "        \n",
    "    if (row['SideA.ViewData.Transaction Type1'] == row['SideB.ViewData.Transaction Type1']):\n",
    "        f = 1\n",
    "    else:\n",
    "        f = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.Quantity'] == row['SideA.ViewData.Quantity']):\n",
    "        g = 1\n",
    "    else:\n",
    "        g = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.ISIN'] == row['SideA.ViewData.ISIN']):\n",
    "        h = 1\n",
    "    else:\n",
    "        h = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.CUSIP'] == row['SideA.ViewData.CUSIP']):\n",
    "        i = 1\n",
    "    else:\n",
    "        i = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.Ticker'] == row['SideA.ViewData.Ticker']):\n",
    "        j = 1\n",
    "    else:\n",
    "        j = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.Investment ID'] == row['SideA.ViewData.Investment ID']):\n",
    "        k = 1\n",
    "    else:\n",
    "        k = 0\n",
    "        \n",
    "    return a, b, c ,d, e,f,g,h,i,j,k\n",
    "    \n",
    "\n",
    "from pandas import merge\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cancelcomment2(y):\n",
    "    com1 = 'This is original of cancelled trade'\n",
    "    com2 = 'on settle date'\n",
    "    com = com1 + ' '  + com2 +' ' + str(y)\n",
    "    return com\n",
    "\n",
    "def cancelcomment3(y):\n",
    "    com1 = 'This is cancelled trade'\n",
    "    com2 = 'on settle date'\n",
    "    com = com1 + ' ' + com2 + ' ' + str(y)\n",
    "    return com\n",
    "\n",
    "if dffk2[dffk2['cancel_marker'] == 1].shape[0]!=0:\n",
    "    dummy = dffk2[dffk2['cancel_marker']!=1]\n",
    "    dummy1 = dffk2[dffk2['cancel_marker']==1]\n",
    "\n",
    "\n",
    "    pool =[]\n",
    "    key_index =[]\n",
    "    training_df =[]\n",
    "    call1 = []\n",
    "\n",
    "    appended_data = []\n",
    "\n",
    "    no_pair_ids = []\n",
    "#max_rows = 5\n",
    "\n",
    "    k = list(set(list(set(dummy['ViewData.Task Business Date1']))))\n",
    "    k1 = k\n",
    "\n",
    "    for d in tqdm(k1):\n",
    "        aa1 = dummy[dummy['ViewData.Task Business Date1']==d]\n",
    "        bb1 = dummy1[dummy1['ViewData.Task Business Date1']==d]\n",
    "        aa1['marker'] = 1\n",
    "        bb1['marker'] = 1\n",
    "    \n",
    "        aa1 = aa1.reset_index()\n",
    "        aa1 = aa1.drop('index',1)\n",
    "        bb1 = bb1.reset_index()\n",
    "        bb1 = bb1.drop('index', 1)\n",
    "        #print(aa1.shape)\n",
    "        #print(bb1.shape)\n",
    "    \n",
    "        aa1.columns = ['SideB.' + x  for x in aa1.columns] \n",
    "        bb1.columns = ['SideA.' + x  for x in bb1.columns]\n",
    "    \n",
    "        cc1 = pd.merge(aa1,bb1, left_on = 'SideB.marker', right_on = 'SideA.marker', how = 'outer')\n",
    "        appended_data.append(cc1)\n",
    "        cancel_broker = pd.concat(appended_data)\n",
    "        cancel_broker[['map_match','amt_match','fund_match','curr_match','sd_match','ttype_match','Qnt_match','isin_match','cusip_match','ticker_match','Invest_id']] = cancel_broker.apply(lambda row : amountelim(row), axis = 1,result_type=\"expand\")\n",
    "        elim1 = cancel_broker[(cancel_broker['map_match']==1) & (cancel_broker['curr_match']==1)  & ((cancel_broker['isin_match']==1) |(cancel_broker['cusip_match']==1)| (cancel_broker['ticker_match']==1) | (cancel_broker['Invest_id']==1))]\n",
    "        if elim1.shape[0]!=0:\n",
    "            id_listA = list(set(elim1['SideA.final_ID']))\n",
    "            c1 = dummy\n",
    "            c2 = dummy1[dummy1['final_ID'].isin(id_listA)]\n",
    "            c1['predicted category'] = 'Cancelled trade'\n",
    "            c2['predicted category'] = 'Original of Cancelled trade'\n",
    "            c1['predicted comment'] =  c1.apply(lambda x : cancelcomment2(x['ViewData.Settle Date1']),axis = 1)\n",
    "            c2['predicted comment'] = c2.apply(lambda x : cancelcomment3(x['ViewData.Settle Date1']),axis = 1)\n",
    "            cancel_fin = pd.concat([c1,c2])\n",
    "            cancel_fin = cancel_fin.reset_index()\n",
    "            cancel_fin = cancel_fin.drop('index', axis = 1)\n",
    "            sel_col_1 = ['final_ID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']\n",
    "            cancel_fin = cancel_fin[sel_col_1]\n",
    "            comment_df_final_list.append(cancel_fin)\n",
    "            cancel_fin.to_csv('Comment file oaktree 2 sep testing p3.csv')\n",
    "            id_listB = list(set(c1['final_ID']))\n",
    "            comb = id_listB + id_listA\n",
    "            dffk2 = dffk2[~dffk2['final_ID'].isin(comb)]\n",
    "            \n",
    "            \n",
    "            \n",
    "   \n",
    "        \n",
    "    else:\n",
    "        c1 = dummy\n",
    "        c1['predicted category'] = 'Cancelled trade'\n",
    "        c1['predicted comment'] =  c1.apply(lambda x : cancelcomment2(x['ViewData.Settle Date1']),axis = 1)\n",
    "        sel_col_1 = ['final_ID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']\n",
    "        cancel_fin = c1[sel_col_1]\n",
    "        comment_df_final_list.append(cancel_fin)\n",
    "        cancel_fin.to_csv('Comment file oaktree 2 sep testing no original p4.csv')\n",
    "        id_listB = list(set(c1['final_ID']))\n",
    "        comb = id_listB\n",
    "        dffk2 = dffk2[~dffk2['final_ID'].isin(comb)]\n",
    "        \n",
    "else:\n",
    "    dffk2 = dffk2.copy()\n",
    "\n",
    "\n",
    "# #### Finding Pairs in Up and down\n",
    "sel_col = ['ViewData.Currency', \n",
    "       'ViewData.Accounting Net Amount', 'ViewData.Age', 'ViewData.Asset Type Category1',\n",
    "       \n",
    "        'ViewData.Cust Net Amount',\n",
    "       'ViewData.BreakID', 'ViewData.Business Date', 'ViewData.Cancel Amount',\n",
    "       'ViewData.Cancel Flag', 'ViewData.ClusterID', 'ViewData.Commission',\n",
    "       'ViewData.CUSIP',  \n",
    "       'ViewData.Description',  'ViewData.Fund',\n",
    "        'ViewData.Has Attachments',\n",
    "       'ViewData.InternalComment1', 'ViewData.InternalComment2',\n",
    "       'ViewData.InternalComment3', 'ViewData.Investment ID',\n",
    "       'ViewData.Investment Type1', \n",
    "       'ViewData.ISIN', 'ViewData.Keys', \n",
    "       'ViewData.Mapped Custodian Account', 'ViewData.Department',\n",
    "       \n",
    "        'ViewData.Portfolio ID',\n",
    "       'ViewData.Portolio', 'ViewData.Price', 'ViewData.Prime Broker1',\n",
    "        \n",
    "       'ViewData.Quantity',  'ViewData.Rule And Key',\n",
    "       'ViewData.SEDOL', 'ViewData.Settle Date', \n",
    "       'ViewData.Status', 'ViewData.Strategy', 'ViewData.System Comments',\n",
    "       'ViewData.Ticker', 'ViewData.Trade Date', 'ViewData.Trade Expenses',\n",
    "       'ViewData.Transaction ID',\n",
    "       'ViewData.Transaction Type1', 'ViewData.Underlying Cusip',\n",
    "       'ViewData.Underlying Investment ID', 'ViewData.Underlying ISIN',\n",
    "       'ViewData.Underlying Sedol', 'ViewData.Underlying Ticker',\n",
    "       'ViewData.UserTran1', 'ViewData.UserTran2', \n",
    "       'ViewData.Side0_UniqueIds', 'ViewData.Side1_UniqueIds',\n",
    "       'ViewData.Task Business Date', 'final_ID',\n",
    "        'ViewData.Task Business Date1',\n",
    "       'ViewData.Settle Date1', 'ViewData.Trade Date1',\n",
    "       'ViewData.SettlevsTrade Date', 'ViewData.SettlevsTask Date',\n",
    "       'ViewData.TaskvsTrade Date','new_desc_cat', 'ViewData.Custodian', 'ViewData.Net Amount Difference Absolute', 'new_pb1'\n",
    "      ]\n",
    "\n",
    "dff4 = dffk2[sel_col]\n",
    "dff5 = dffk3[sel_col]\n",
    "\n",
    "dff6 = dffk4[sel_col]\n",
    "dff7 = dffk5[sel_col]\n",
    "\n",
    "dff4 = pd.concat([dff4,dff6])\n",
    "dff4 = dff4.reset_index()\n",
    "dff4 = dff4.drop('index', axis = 1)\n",
    "\n",
    "dff5 = pd.concat([dff5,dff7])\n",
    "dff5 = dff5.reset_index()\n",
    "dff5 = dff5.drop('index', axis = 1)\n",
    "\n",
    "# #### M cross N code\n",
    "\n",
    "###################### loop 3 ###############################\n",
    "pool =[]\n",
    "key_index =[]\n",
    "training_df =[]\n",
    "call1 = []\n",
    "\n",
    "appended_data = []\n",
    "\n",
    "no_pair_ids = []\n",
    "#max_rows = 5\n",
    "\n",
    "k = list(set(list(set(dff5['ViewData.Task Business Date1'])) + list(set(dff4['ViewData.Task Business Date1']))))\n",
    "k1 = k\n",
    "\n",
    "for d in tqdm(k1):\n",
    "    aa1 = dff4[dff4['ViewData.Task Business Date1']==d]\n",
    "    bb1 = dff5[dff5['ViewData.Task Business Date1']==d]\n",
    "    aa1['marker'] = 1\n",
    "    bb1['marker'] = 1\n",
    "    \n",
    "    aa1 = aa1.reset_index()\n",
    "    aa1 = aa1.drop('index',1)\n",
    "    bb1 = bb1.reset_index()\n",
    "    bb1 = bb1.drop('index', 1)\n",
    "    print(aa1.shape)\n",
    "    print(bb1.shape)\n",
    "    \n",
    "    aa1.columns = ['SideB.' + x  for x in aa1.columns] \n",
    "    bb1.columns = ['SideA.' + x  for x in bb1.columns]\n",
    "    \n",
    "    cc1 = pd.merge(aa1,bb1, left_on = 'SideB.marker', right_on = 'SideA.marker', how = 'outer')\n",
    "    appended_data.append(cc1)\n",
    "\n",
    "df_213_1 = pd.concat(appended_data)\n",
    "\n",
    "def amountelim(row):\n",
    "   \n",
    "   \n",
    "   \n",
    "    if (row['SideA.ViewData.Mapped Custodian Account'] == row['SideB.ViewData.Mapped Custodian Account']):\n",
    "        a = 1\n",
    "    else:\n",
    "        a = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.Cust Net Amount'] == row['SideA.ViewData.Accounting Net Amount']):\n",
    "        b = 1\n",
    "    else:\n",
    "        b = 0\n",
    "    \n",
    "    if (row['SideA.ViewData.Fund'] == row['SideB.ViewData.Fund']):\n",
    "        c = 1\n",
    "    else:\n",
    "        c = 0\n",
    "        \n",
    "    if (row['SideA.ViewData.Currency'] == row['SideB.ViewData.Currency']):\n",
    "        d = 1\n",
    "    else:\n",
    "        d = 0\n",
    "    \n",
    "    if (row['SideA.ViewData.Settle Date1'] == row['SideB.ViewData.Settle Date1']):\n",
    "        e = 1\n",
    "    else:\n",
    "        e = 0\n",
    "        \n",
    "    if (row['SideA.ViewData.Transaction Type1'] == row['SideB.ViewData.Transaction Type1']):\n",
    "        f = 1\n",
    "    else:\n",
    "        f = 0\n",
    "        \n",
    "    return a, b, c ,d, e,f\n",
    "    \n",
    "df_213_1[['map_match','amt_match','fund_match','curr_match','sd_match','ttype_match']] = df_213_1.apply(lambda row : amountelim(row), axis = 1,result_type=\"expand\")\n",
    "\n",
    "df_213_1['key_match_sum'] = df_213_1['map_match'] + df_213_1['sd_match'] + df_213_1['curr_match']\n",
    "elim1 = df_213_1[(df_213_1['amt_match']==1) & (df_213_1['key_match_sum']>=2)]\n",
    "\n",
    "# - putting updown comments\n",
    "\n",
    "def updownat(a,b,c,d,e):\n",
    "    if a == 0:\n",
    "        k = 'mapped custodian account'\n",
    "    elif b==0:\n",
    "        k = 'currency'\n",
    "    elif c ==0 :\n",
    "        k = 'Settle Date'\n",
    "    elif d == 0:\n",
    "        k = 'fund'    \n",
    "    elif e == 0:\n",
    "        k = 'transaction type'\n",
    "    else :\n",
    "        k = 'Investment type'\n",
    "        \n",
    "    com = 'up/down at'+ ' ' + k\n",
    "    return com\n",
    "\n",
    "if elim1.shape[0]!=0:\n",
    "    elim1['SideA.predicted category'] = 'Updown'\n",
    "    elim1['SideB.predicted category'] = 'Updown'\n",
    "    elim1['SideA.predicted comment'] = elim1.apply(lambda x : updownat(x['map_match'],x['curr_match'],x['sd_match'],x['fund_match'],x['ttype_match']), axis = 1)\n",
    "    elim1['SideB.predicted comment'] = elim1.apply(lambda x : updownat(x['map_match'],x['curr_match'],x['sd_match'],x['fund_match'],x['ttype_match']), axis = 1)\n",
    "    elim_col = ['final_ID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']\n",
    "    \n",
    "    sideA_col = []\n",
    "    sideB_col = []\n",
    "#    elim_col = list(elim1.columns)\n",
    "\n",
    "    for items in elim_col:\n",
    "        item = 'SideA.'+items\n",
    "        sideA_col.append(item)\n",
    "        item = 'SideB.'+items\n",
    "        sideB_col.append(item)\n",
    "        \n",
    "    elim2 = elim1[sideA_col]\n",
    "    elim3 = elim1[sideA_col]\n",
    "    \n",
    "    elim2 = elim2.rename(columns= {'SideA.final_ID':'final_ID',\n",
    "                              'SideA.ViewData.Side0_UniqueIds' : 'ViewData.Side0_UniqueIds',\n",
    "                              'SideA.ViewData.Side1_UniqueIds' : 'ViewData.Side1_UniqueIds',\n",
    "                              'SideA.predicted category':'predicted category',\n",
    "                              'SideA.predicted comment':'predicted comment'})\n",
    "    elim3 = elim3.rename(columns= {'SideB.final_ID':'final_ID',\n",
    "                              'SideB.ViewData.Side0_UniqueIds' : 'ViewData.Side0_UniqueIds',\n",
    "                              'SideB.ViewData.Side1_UniqueIds' : 'ViewData.Side1_UniqueIds',                                   \n",
    "                              'SideB.predicted category':'predicted category',\n",
    "                              'SideB.predicted comment':'predicted comment'})\n",
    "    frames = [elim2,elim3]\n",
    "    elim = pd.concat(frames)\n",
    "    elim = elim.reset_index()\n",
    "    elim = elim.drop('index', axis = 1)\n",
    "    elim.to_csv('Comment file oaktree 2 sep testing p5.csv')\n",
    "    comment_df_final_list.append(elim)\n",
    "    \n",
    "    id_listB = list(set(elim1['SideB.final_ID'])) \n",
    "    id_listA = list(set(elim1['SideA.final_ID']))\n",
    "    \n",
    "    df_213_1 = df_213_1[~df_213_1['SideB.final_ID'].isin(id_listB)]\n",
    "    df_213_1 = df_213_1[~df_213_1['SideA.final_ID'].isin(id_listA)]\n",
    "    \n",
    "    id_listB = list(set(df_213_1['SideB.final_ID'])) \n",
    "    id_listA = list(set(df_213_1['SideA.final_ID']))\n",
    "    \n",
    "    dff4 = dff4[dff4['final_ID'].isin(id_listB)]\n",
    "    dff5 = dff5[dff5['final_ID'].isin(id_listA)]\n",
    "    \n",
    "else:\n",
    "    dff4 = dff4.copy()\n",
    "    dff5 = dff5.copy()\n",
    "    \n",
    "frames = [dff4,dff5]\n",
    "\n",
    "data = pd.concat(frames)\n",
    "data = data.reset_index()\n",
    "data = data.drop('index', axis = 1)\n",
    "data['new_pb2'] = data.apply(lambda x : 'Geneva' if x['ViewData.Side0_UniqueIds'] != 'AA' else x['new_pb1'], axis = 1)\n",
    "\n",
    "Pre_final = [\n",
    "    \n",
    "'ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.BreakID',\n",
    " 'ViewData.Currency',\n",
    " 'ViewData.Custodian',\n",
    "     'ViewData.ISIN',\n",
    " 'ViewData.Mapped Custodian Account',\n",
    "  'ViewData.Net Amount Difference Absolute',\n",
    "  'ViewData.Portolio',\n",
    " 'ViewData.Settle Date',\n",
    "  'ViewData.Trade Date',\n",
    " 'ViewData.Transaction Type1',\n",
    "'new_desc_cat',\n",
    "    'ViewData.Department',\n",
    " 'ViewData.Accounting Net Amount',\n",
    " 'ViewData.Asset Type Category1',\n",
    " 'ViewData.CUSIP',\n",
    " 'ViewData.Commission',\n",
    " 'ViewData.Fund',\n",
    " 'ViewData.Investment ID',\n",
    " 'ViewData.Investment Type1',\n",
    " 'ViewData.Price',\n",
    " 'ViewData.Prime Broker1',\n",
    " 'ViewData.Quantity',\n",
    "'ViewData.InternalComment2', 'ViewData.Description','new_pb2','new_pb1'\n",
    "]\n",
    "\n",
    "\n",
    "data = data[Pre_final]\n",
    "\n",
    "df_mod1 = data.copy()\n",
    "\n",
    "df_mod1['ViewData.Custodian'] = df_mod1['ViewData.Custodian'].fillna('AA')\n",
    "df_mod1['ViewData.Portolio'] = df_mod1['ViewData.Portolio'].fillna('bb')\n",
    "df_mod1['ViewData.Settle Date'] = df_mod1['ViewData.Settle Date'].fillna(0)\n",
    "df_mod1['ViewData.Trade Date'] = df_mod1['ViewData.Trade Date'].fillna(0)\n",
    "df_mod1['ViewData.Accounting Net Amount'] = df_mod1['ViewData.Accounting Net Amount'].fillna(0)\n",
    "df_mod1['ViewData.Asset Type Category1'] = df_mod1['ViewData.Asset Type Category1'].fillna('CC')\n",
    "df_mod1['ViewData.CUSIP'] = df_mod1['ViewData.CUSIP'].fillna('DD')\n",
    "df_mod1['ViewData.Fund'] = df_mod1['ViewData.Fund'].fillna('EE')\n",
    "df_mod1['ViewData.Investment ID'] = df_mod1['ViewData.Investment ID'].fillna('FF')\n",
    "df_mod1['ViewData.Investment Type1'] = df_mod1['ViewData.Investment Type1'].fillna('GG')\n",
    "#df_mod1['ViewData.Knowledge Date'] = df_mod1['ViewData.Knowledge Date'].fillna(0)\n",
    "df_mod1['ViewData.Price'] = df_mod1['ViewData.Price'].fillna(0)\n",
    "df_mod1['ViewData.Prime Broker1'] = df_mod1['ViewData.Prime Broker1'].fillna(\"HH\")\n",
    "df_mod1['ViewData.Quantity'] = df_mod1['ViewData.Quantity'].fillna(0)\n",
    "#df_mod1['ViewData.Sec Fees'] = df_mod1['ViewData.Sec Fees'].fillna(0)\n",
    "#df_mod1['ViewData.Strike Price'] = df_mod1['ViewData.Strike Price'].fillna(0)\n",
    "df_mod1['ViewData.Commission'] = df_mod1['ViewData.Commission'].fillna(0)\n",
    "df_mod1['ViewData.Transaction Type1'] = df_mod1['ViewData.Transaction Type1'].fillna('kk')\n",
    "df_mod1['ViewData.ISIN'] = df_mod1['ViewData.ISIN'].fillna('mm')\n",
    "df_mod1['new_desc_cat'] = df_mod1['new_desc_cat'].fillna('nn')\n",
    "#df_mod1['Category'] = df_mod1['Category'].fillna('NA')\n",
    "df_mod1['ViewData.Description'] = df_mod1['ViewData.Description'].fillna('nn')\n",
    "df_mod1['ViewData.Department'] = df_mod1['ViewData.Department'].fillna('nn')\n",
    "\n",
    "df_mod1['ViewData.Custodian'] = df_mod1['ViewData.Custodian'].replace('nan','kkk')\n",
    "df_mod1['ViewData.Custodian'] = df_mod1['ViewData.Custodian'].replace('None','kkk')\n",
    "df_mod1['ViewData.Custodian'] = df_mod1['ViewData.Custodian'].replace('','kkk')\n",
    "\n",
    "df_mod1['ViewData.Portolio'] = df_mod1['ViewData.Portolio'].replace('nan','bb')\n",
    "df_mod1['ViewData.Portolio'] = df_mod1['ViewData.Portolio'].replace('None','bb')\n",
    "df_mod1['ViewData.Portolio'] = df_mod1['ViewData.Portolio'].replace('','bb')\n",
    "\n",
    "df_mod1['ViewData.Settle Date'] = df_mod1['ViewData.Settle Date'].replace('nan',0)\n",
    "df_mod1['ViewData.Settle Date'] = df_mod1['ViewData.Settle Date'].replace('None',0)\n",
    "df_mod1['ViewData.Settle Date'] = df_mod1['ViewData.Settle Date'].replace('',0)\n",
    "\n",
    "df_mod1['ViewData.Trade Date'] = df_mod1['ViewData.Trade Date'].replace('nan',0)\n",
    "df_mod1['ViewData.Trade Date'] = df_mod1['ViewData.Trade Date'].replace('None',0)\n",
    "df_mod1['ViewData.Trade Date'] = df_mod1['ViewData.Trade Date'].replace('',0)\n",
    "\n",
    "df_mod1['ViewData.Accounting Net Amount'] = df_mod1['ViewData.Accounting Net Amount'].replace('nan',0)\n",
    "df_mod1['ViewData.Accounting Net Amount'] = df_mod1['ViewData.Accounting Net Amount'].replace('None',0)\n",
    "df_mod1['ViewData.Accounting Net Amount'] = df_mod1['ViewData.Accounting Net Amount'].replace('',0)\n",
    "\n",
    "\n",
    "df_mod1['ViewData.Asset Type Category1'] = df_mod1['ViewData.Asset Type Category1'].replace('nan','CC')\n",
    "df_mod1['ViewData.Asset Type Category1'] = df_mod1['ViewData.Asset Type Category1'].replace('None','CC')\n",
    "df_mod1['ViewData.Asset Type Category1'] = df_mod1['ViewData.Asset Type Category1'].replace('','CC')\n",
    "\n",
    "df_mod1['ViewData.CUSIP'] = df_mod1['ViewData.CUSIP'].replace('nan','DD')\n",
    "df_mod1['ViewData.CUSIP'] = df_mod1['ViewData.CUSIP'].replace('None','DD')\n",
    "df_mod1['ViewData.CUSIP'] = df_mod1['ViewData.CUSIP'].replace('','DD')\n",
    "\n",
    "df_mod1['ViewData.Fund'] = df_mod1['ViewData.Fund'].replace('nan','EE')\n",
    "df_mod1['ViewData.Fund'] = df_mod1['ViewData.Fund'].replace('None','EE')\n",
    "df_mod1['ViewData.Fund'] = df_mod1['ViewData.Fund'].replace('','EE')\n",
    "\n",
    "df_mod1['ViewData.Investment ID'] = df_mod1['ViewData.Investment ID'].replace('nan','FF')\n",
    "df_mod1['ViewData.Investment ID'] = df_mod1['ViewData.Investment ID'].replace('None','FF')\n",
    "df_mod1['ViewData.Investment ID'] = df_mod1['ViewData.Investment ID'].replace('','FF')\n",
    "\n",
    "df_mod1['ViewData.Investment Type1'] = df_mod1['ViewData.Investment Type1'].replace('nan','GG')\n",
    "df_mod1['ViewData.Investment Type1'] = df_mod1['ViewData.Investment Type1'].replace('None','GG')\n",
    "df_mod1['ViewData.Investment Type1'] = df_mod1['ViewData.Investment Type1'].replace('','GG')\n",
    "\n",
    "df_mod1['ViewData.Price'] = df_mod1['ViewData.Price'].replace('nan',0)\n",
    "df_mod1['ViewData.Price'] = df_mod1['ViewData.Price'].replace('None',0)\n",
    "df_mod1['ViewData.Price'] = df_mod1['ViewData.Price'].replace('',0)\n",
    "\n",
    "df_mod1['ViewData.Prime Broker1'] = df_mod1['ViewData.Prime Broker1'].replace('nan','HH')\n",
    "df_mod1['ViewData.Prime Broker1'] = df_mod1['ViewData.Prime Broker1'].replace('None','HH')\n",
    "df_mod1['ViewData.Prime Broker1'] = df_mod1['ViewData.Prime Broker1'].replace('','HH')\n",
    "\n",
    "df_mod1['ViewData.Quantity'] = df_mod1['ViewData.Quantity'].replace('nan',0)\n",
    "df_mod1['ViewData.Quantity'] = df_mod1['ViewData.Quantity'].replace('None',0)\n",
    "df_mod1['ViewData.Quantity'] = df_mod1['ViewData.Quantity'].replace('',0)\n",
    "\n",
    "df_mod1['ViewData.Commission'] = df_mod1['ViewData.Commission'].replace('nan',0)\n",
    "df_mod1['ViewData.Commission'] = df_mod1['ViewData.Commission'].replace('None',0)\n",
    "df_mod1['ViewData.Commission'] = df_mod1['ViewData.Commission'].replace('',0)\n",
    "\n",
    "df_mod1['ViewData.Transaction Type1'] = df_mod1['ViewData.Transaction Type1'].replace('nan','kk')\n",
    "df_mod1['ViewData.Transaction Type1'] = df_mod1['ViewData.Transaction Type1'].replace('None','kk')\n",
    "df_mod1['ViewData.Transaction Type1'] = df_mod1['ViewData.Transaction Type1'].replace('','kk')\n",
    "\n",
    "df_mod1['ViewData.ISIN'] = df_mod1['ViewData.ISIN'].replace('nan','mm')\n",
    "df_mod1['ViewData.ISIN'] = df_mod1['ViewData.ISIN'].replace('None','mm')\n",
    "df_mod1['ViewData.ISIN'] = df_mod1['ViewData.ISIN'].replace('','mm')\n",
    "\n",
    "df_mod1['new_desc_cat'] = df_mod1['new_desc_cat'].replace('nan','nn')\n",
    "df_mod1['new_desc_cat'] = df_mod1['new_desc_cat'].replace('None','nn')\n",
    "df_mod1['new_desc_cat'] = df_mod1['new_desc_cat'].replace('','nn')\n",
    "\n",
    "df_mod1['ViewData.Description'] = df_mod1['ViewData.Description'].replace('nan','nn')\n",
    "df_mod1['ViewData.Description'] = df_mod1['ViewData.Description'].replace('None','nn')\n",
    "df_mod1['ViewData.Description'] = df_mod1['ViewData.Description'].replace('','nn')\n",
    "\n",
    "df_mod1['ViewData.Department'] = df_mod1['ViewData.Department'].replace('nan','nn')\n",
    "df_mod1['ViewData.Department'] = df_mod1['ViewData.Department'].replace('None','nn')\n",
    "df_mod1['ViewData.Department'] = df_mod1['ViewData.Department'].replace('','nn')\n",
    "\n",
    "\n",
    "def fid(a,b):\n",
    "   \n",
    "    if ( b=='BB'):\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "\n",
    "\n",
    "df_mod1['final_ID'] = df_mod1.apply(lambda row: fid(row['ViewData.Side0_UniqueIds'],row['ViewData.Side1_UniqueIds']),axis =1)\n",
    "\n",
    "data2 = df_mod1.copy()\n",
    "\n",
    "\n",
    "# ### Separate Prediction of the Trade and Non trade\n",
    "\n",
    "# #### 1st for Non Trade\n",
    "\n",
    "trade_types = ['buy','sell','cover short', 'sell short', 'forward', 'forwardfx', 'spotfx']\n",
    "\n",
    "data21 = data2[~data2['ViewData.Transaction Type1'].isin(trade_types)]\n",
    "\n",
    "cols = [ \n",
    "  'ViewData.Transaction Type1',\n",
    " 'ViewData.Asset Type Category1',\n",
    "  'new_desc_cat',\n",
    "  'ViewData.Investment Type1',\n",
    " 'new_pb1','new_pb2','ViewData.Department'\n",
    "]\n",
    "data211 = data21[cols]\n",
    "filename = 'finalized_model_oaktree_non trade_v1.sav'\n",
    "\n",
    "clf = pickle.load(open(filename, 'rb'))\n",
    "# Actual class predictions\n",
    "cb_predictions = clf.predict(data211)#.astype(str)\n",
    "# Probabilities for each class\n",
    "#cb_probs = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# #### Testing of Model and final prediction file - Non Trade\n",
    "demo = []\n",
    "for item in cb_predictions:\n",
    "    demo.append(item[0])\n",
    "\n",
    "result_non_trade =data21.copy()\n",
    "result_non_trade = result_non_trade.reset_index()\n",
    "\n",
    "result_non_trade['predicted category'] = pd.Series(demo)\n",
    "result_non_trade['predicted comment'] = 'NA'\n",
    "\n",
    "\n",
    "result_non_trade = result_non_trade.drop('predicted comment', axis = 1)\n",
    "\n",
    "\n",
    "# In[249]:\n",
    "\n",
    "\n",
    "com_temp = pd.read_csv('Soros comment template for delivery.csv')\n",
    "\n",
    "\n",
    "# In[250]:\n",
    "\n",
    "\n",
    "com_temp = com_temp.rename(columns = {'Category ':'predicted category','template':'predicted template'})\n",
    "\n",
    "\n",
    "# In[252]:\n",
    "\n",
    "\n",
    "result_non_trade = pd.merge(result_non_trade,com_temp,on = 'predicted category',how = 'left')\n",
    "\n",
    "\n",
    "# In[256]:\n",
    "\n",
    "\n",
    "def comgen(x,y,z,k):\n",
    "    if x == 'Geneva':\n",
    "        \n",
    "        com = k + ' ' +y + ' ' + str(z)\n",
    "    else:\n",
    "        com = \"Geneva\" + ' ' +y + ' ' + str(z)\n",
    "        \n",
    "    return com\n",
    "\n",
    "\n",
    "# In[257]:\n",
    "result_non_trade['new_pb2'] = result_non_trade['new_pb2'].astype(str)\n",
    "result_non_trade['predicted template'] = result_non_trade['predicted template'].astype(str)\n",
    "result_non_trade['ViewData.Settle Date'] = result_non_trade['ViewData.Settle Date'].astype(str)\n",
    "result_non_trade['new_pb1'] = result_non_trade['new_pb1'].astype(str)\n",
    "\n",
    "result_non_trade['predicted comment'] = result_non_trade.apply(lambda x : comgen(x['new_pb2'],x['predicted template'],x['ViewData.Settle Date'],x['new_pb1']), axis = 1)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "result_non_trade = result_non_trade[['final_ID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']]\n",
    "\n",
    "\n",
    "# In[260]:\n",
    "\n",
    "\n",
    "result_non_trade.to_csv('Comment file soros 2 sep testing p6.csv')\n",
    "comment_df_final_list.append(result_non_trade)\n",
    "\n",
    "\n",
    "\n",
    "# #### For Trade Model\n",
    "\n",
    "data22 = data2[data2['ViewData.Transaction Type1'].isin(trade_types)]\n",
    "\n",
    "data222 = data22[cols]\n",
    "\n",
    "filename = 'finalized_model_oaktree_trade_v1.sav'\n",
    "\n",
    "clf = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# Actual class predictions\n",
    "cb_predictions = clf.predict(data222)#.astype(str)\n",
    "# Probabilities for each class\n",
    "#cb_probs = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "demo = []\n",
    "for item in cb_predictions:\n",
    "    demo.append(item[0])\n",
    "\n",
    "result_trade =data22.copy()\n",
    "\n",
    "result_trade = result_trade.reset_index()\n",
    "\n",
    "result_trade['predicted category'] = pd.Series(demo)\n",
    "result_trade['predicted comment'] = 'NA'\n",
    "\n",
    "result_trade = result_trade.drop('predicted comment', axis = 1)\n",
    "\n",
    "com_temp = pd.read_csv('oaktree Comment template for delivery new.csv')\n",
    "\n",
    "com_temp = com_temp.rename(columns = {'Category':'predicted category','template':'predicted template'})\n",
    "\n",
    "result_trade = pd.merge(result_trade,com_temp,on = 'predicted category',how = 'left')\n",
    "\n",
    "def comgen(x,y,z,k):\n",
    "    if x == 'Geneva':\n",
    "        \n",
    "        com = k + ' ' +y + ' ' + str(z)\n",
    "    else:\n",
    "        com = \"Geneva\" + ' ' +y + ' ' + str(z)\n",
    "        \n",
    "    return com\n",
    "\n",
    "result_trade['new_pb2'] = result_trade['new_pb2'].astype(str)\n",
    "result_trade['predicted template'] = result_trade['predicted template'].astype(str)\n",
    "result_trade['ViewData.Settle Date'] = result_trade['ViewData.Settle Date'].astype(str)\n",
    "result_trade['new_pb1'] = result_trade['new_pb1'].astype(str)\n",
    "\n",
    "result_trade['predicted comment'] = result_trade.apply(lambda x : comgen(x['new_pb2'],x['predicted template'],x['ViewData.Settle Date'],x['new_pb1']), axis = 1)\n",
    "\n",
    "\n",
    "result_trade = result_trade[['final_ID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']]\n",
    "\n",
    "result_trade.to_csv('Comment file oaktree 2 sep testing p7.csv')\n",
    "comment_df_final_list.append(result_trade)\n",
    "\n",
    "comment_df_final = pd.concat(comment_df_final_list)\n",
    "\n",
    "change_col_names_comment_df_final_dict = {\n",
    "                                        'ViewData.Side0_UniqueIds' : 'Side0_UniqueIds',\n",
    "                                        'ViewData.Side1_UniqueIds' : 'Side1_UniqueIds',\n",
    "                                        'predicted category' : 'PredictedCategory',\n",
    "                                        'predicted comment' : 'PredictedComment'\n",
    "                                        }\n",
    "\n",
    "comment_df_final.rename(columns = change_col_names_comment_df_final_dict, inplace = True)\n",
    "\n",
    "comment_df_final_side0 = comment_df_final[comment_df_final['Side1_UniqueIds'] == 'BB']\n",
    "comment_df_final_side1 = comment_df_final[comment_df_final['Side0_UniqueIds'] == 'AA']\n",
    "\n",
    "final_df = final_table_to_write.merge(comment_df_final_side0, on = 'Side0_UniqueIds', how = 'left')\n",
    "final_df['PredictedComment'] = final_df['PredictedComment_y'].fillna(final_df['PredictedComment_x'])\n",
    "final_df.drop(['final_ID','PredictedComment_x','PredictedComment_y','Side1_UniqueIds_y'], axis = 1, inplace = True)\n",
    "final_df.rename(columns = {'Side1_UniqueIds_x' : 'Side1_UniqueIds'}, inplace = True)\n",
    "\n",
    "\n",
    "final_df_2 = final_df.merge(comment_df_final_side1, on = 'Side1_UniqueIds', how = 'left')\n",
    "final_df_2['PredictedComment'] = final_df_2['PredictedComment_y'].fillna(final_df_2['PredictedComment_x'])\n",
    "final_df_2.drop(['final_ID','PredictedComment_x','PredictedComment_y','Side0_UniqueIds_y'], axis = 1, inplace = True)\n",
    "#Drop these columns for Oaktree data\n",
    "#final_df_2.drop(['SideA.ViewData.Side0_UniqueIds_x','SideA.ViewData.Side1_UniqueIds_x','SideA.final_ID_x','SideA.predicted category_x','SideA.predicted comment_x','SideA.ViewData.Side0_UniqueIds_y','SideA.ViewData.Side1_UniqueIds_y','SideA.final_ID_y','SideA.predicted category_y','SideA.predicted comment_y'], axis = 1, inplace = True)\n",
    "\n",
    "final_df_2.rename(columns = {'Side0_UniqueIds_x' : 'Side0_UniqueIds'}, inplace = True)\n",
    "\n",
    "    \n",
    "#    Added more checks for database\n",
    "\n",
    "final_df_2['Side1_UniqueIds'] = final_df_2['Side1_UniqueIds'].astype(str)\n",
    "final_df_2['Side0_UniqueIds'] = final_df_2['Side0_UniqueIds'].astype(str)\n",
    "final_df_2['BreakID'] = final_df_2['BreakID'].astype(str)\n",
    "final_df_2['Final_predicted_break'] = final_df_2['Final_predicted_break'].astype(str)\n",
    "#final_df_2['probability_UMT'] = final_df_2['probability_UMT'].astype(str)\n",
    "final_df_2['probability_UMR'] = final_df_2['probability_UMR'].astype(str)\n",
    "final_df_2['probability_UMB'] = final_df_2['probability_UMB'].astype(str)\n",
    "final_df_2['probability_No_pair'] = final_df_2['probability_No_pair'].astype(str)\n",
    "\n",
    "final_df_2['Side1_UniqueIds'] = final_df_2['Side1_UniqueIds'].map(lambda x:x.lstrip('[').rstrip(']'))\n",
    "final_df_2['Side0_UniqueIds'] = final_df_2['Side0_UniqueIds'].map(lambda x:x.lstrip('[').rstrip(']'))\n",
    "final_df_2['BreakID'] = final_df_2['BreakID'].map(lambda x:x.lstrip('[').rstrip(']'))\n",
    "final_df_2['Final_predicted_break'] = final_df_2['Final_predicted_break'].map(lambda x:x.lstrip('[').rstrip(']'))\n",
    "\n",
    "cols_to_remove_newline_char_from = ['Side1_UniqueIds','Side0_UniqueIds','BreakID']\n",
    "final_df_2['Side1_UniqueIds'] = final_df_2['Side1_UniqueIds'].replace('\\\\n','',regex = True)\n",
    "final_df_2['Side0_UniqueIds'] = final_df_2['Side0_UniqueIds'].replace('\\\\n','',regex = True)\n",
    "final_df_2['Side1_UniqueIds'] = final_df_2['Side1_UniqueIds'].replace('BB','')\n",
    "final_df_2['Side0_UniqueIds'] = final_df_2['Side0_UniqueIds'].replace('AA','')\n",
    "final_df_2['Side1_UniqueIds'] = final_df_2['Side1_UniqueIds'].replace('None','')\n",
    "final_df_2['Side0_UniqueIds'] = final_df_2['Side0_UniqueIds'].replace('None','')\n",
    "final_df_2['Side1_UniqueIds'] = final_df_2['Side1_UniqueIds'].replace('nan','')\n",
    "final_df_2['Side0_UniqueIds'] = final_df_2['Side0_UniqueIds'].replace('nan','')\n",
    "\n",
    "final_df_2['probability_No_pair'] = final_df_2['probability_No_pair'].replace('None','')\n",
    "final_df_2['probability_No_pair'] = final_df_2['probability_No_pair'].replace('nan','')\n",
    "\n",
    "#final_df_2['probability_UMT'] = final_df_2['probability_UMT'].replace('None','')\n",
    "#final_df_2['probability_UMT'] = final_df_2['probability_UMT'].replace('nan','')\n",
    "\n",
    "final_df_2['probability_UMR'] = final_df_2['probability_UMR'].replace('None','')\n",
    "final_df_2['probability_UMR'] = final_df_2['probability_UMR'].replace('nan','')\n",
    "\n",
    "final_df_2['probability_UMB'] = final_df_2['probability_UMB'].replace('None','')\n",
    "final_df_2['probability_UMB'] = final_df_2['probability_UMB'].replace('nan','')\n",
    "\n",
    "final_df_2['BreakID'] = final_df_2['BreakID'].replace('\\\\n','',regex = True)\n",
    "\n",
    "final_df_2['PredictedComment'] = final_df_2['PredictedComment'].astype(str)\n",
    "final_df_2['PredictedComment'] = final_df_2['PredictedComment'].replace('nan','')\n",
    "final_df_2['PredictedComment'] = final_df_2['PredictedComment'].replace('None','')\n",
    "final_df_2['PredictedComment'] = final_df_2['PredictedComment'].replace('NA','')\n",
    "\n",
    "final_df_2['BreakID'] = final_df_2['BreakID'].replace('\\.0','',regex = True)\n",
    "\n",
    "final_df_2_UMR_record_with_predicted_comment = final_df_2[((final_df_2['PredictedComment']!='') & (final_df_2['Predicted_Status'] == 'UMR'))]\n",
    "if(final_df_2_UMR_record_with_predicted_comment.shape[0] != 0):\n",
    "    final_df_2 = final_df_2[~((final_df_2['PredictedComment']!='') & (final_df_2['Predicted_Status'] == 'UMR'))]\n",
    "\n",
    "    Side0_id_of_OB_record_to_remove_corresponding_to_UMR_record_with_predicted_comment = final_df_2_UMR_record_with_predicted_comment['Side0_UniqueIds']\n",
    "    Side1_id_of_OB_record_to_remove_corresponding_to_UMR_record_with_predicted_comment = final_df_2_UMR_record_with_predicted_comment['Side1_UniqueIds']\n",
    "\n",
    "    final_df_2 = final_df_2[~((final_df_2['Side0_UniqueIds'].isin(Side0_id_of_OB_record_to_remove_corresponding_to_UMR_record_with_predicted_comment)) & (final_df_2['Predicted_Status'] == 'OB'))]\n",
    "    final_df_2 = final_df_2[~((final_df_2['Side1_UniqueIds'].isin(Side1_id_of_OB_record_to_remove_corresponding_to_UMR_record_with_predicted_comment)) & (final_df_2['Predicted_Status'] == 'OB'))]\n",
    "\n",
    "    final_df_2_UMR_record_with_predicted_comment['PredictedComment'] = ''       \n",
    "    final_df_2 = final_df_2.append(final_df_2_UMR_record_with_predicted_comment)\n",
    "\n",
    "\n",
    "final_df_2_UMT_record_with_predicted_comment = final_df_2[((final_df_2['PredictedComment']!='') & (final_df_2['Predicted_Status'] == 'UMT'))]\n",
    "if(final_df_2_UMT_record_with_predicted_comment.shape[0] != 0):\n",
    "    final_df_2 = final_df_2[~((final_df_2['PredictedComment']!='') & (final_df_2['Predicted_Status'] == 'UMT'))]\n",
    "    \n",
    "    Side0_id_of_OB_record_to_remove_corresponding_to_UMT_record_with_predicted_comment = final_df_2_UMT_record_with_predicted_comment['Side0_UniqueIds']\n",
    "    Side1_id_of_OB_record_to_remove_corresponding_to_UMT_record_with_predicted_comment = final_df_2_UMT_record_with_predicted_comment['Side1_UniqueIds']\n",
    "    \n",
    "    final_df_2 = final_df_2[~((final_df_2['Side0_UniqueIds'].isin(Side0_id_of_OB_record_to_remove_corresponding_to_UMT_record_with_predicted_comment)) & (final_df_2['Predicted_Status'] == 'OB'))]\n",
    "    final_df_2 = final_df_2[~((final_df_2['Side1_UniqueIds'].isin(Side1_id_of_OB_record_to_remove_corresponding_to_UMT_record_with_predicted_comment)) & (final_df_2['Predicted_Status'] == 'OB'))]\n",
    "\n",
    "    final_df_2_UMT_record_with_predicted_comment['PredictedComment'] = ''\n",
    "    final_df_2 = final_df_2.append(final_df_2_UMT_record_with_predicted_comment)\n",
    "\n",
    "final_df_2['BreakID'] = final_df_2['BreakID'].astype(str)\n",
    "final_df_2['BusinessDate'] = pd.to_datetime(final_df_2['BusinessDate'])\n",
    "final_df_2['BusinessDate'] = final_df_2['BusinessDate'].map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "final_df_2['BusinessDate'] = pd.to_datetime(final_df_2['BusinessDate'])\n",
    "\n",
    "final_df_2[['SetupID']] = final_df_2[['SetupID']].astype(int)\n",
    "\n",
    "final_df_2[['TaskID']] = final_df_2[['TaskID']].astype(float)\n",
    "final_df_2[['TaskID']] = final_df_2[['TaskID']].astype(np.int64)\n",
    "\n",
    "\n",
    "final_df_2.drop(['SideA.ViewData.Side0_UniqueIds_x',\\\n",
    "                    'SideA.ViewData.Side1_UniqueIds_x', \\\n",
    "                    'SideA.final_ID_x', \\\n",
    "                    'SideA.predicted category_x', \\\n",
    "                    'SideA.predicted comment_x', \\\n",
    "                    'SideA.ViewData.Side0_UniqueIds_y', \\\n",
    "                    'SideA.ViewData.Side1_UniqueIds_y', \\\n",
    "                    'SideA.final_ID_y', \\\n",
    "                    'SideA.predicted category_y', \\\n",
    "                    'SideA.predicted comment_y'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "#Fixing 'Not_Covered_by_ML' Statuses\n",
    "Search_term = 'not_covered_by_ml'\n",
    "\n",
    "final_df_2_Covered_by_ML_df = final_df_2[~final_df_2['Predicted_Status'].str.lower().str.endswith(Search_term)]\n",
    "\n",
    "final_df_2_Not_Covered_by_ML_df = final_df_2[final_df_2['Predicted_Status'].str.lower().str.endswith(Search_term)]\n",
    "\n",
    "def get_first_term_before_separator(single_string, separator):\n",
    "    return(single_string.split(separator)[0])\n",
    "\n",
    "final_df_2_Not_Covered_by_ML_df['Predicted_Status'] = final_df_2_Not_Covered_by_ML_df['Predicted_Status'].apply(lambda x : get_first_term_before_separator(x,'_'))\n",
    "final_df_2_Not_Covered_by_ML_df['ML_flag'] = 'Not_Covered_by_ML'\n",
    "\n",
    "final_df_2 = final_df_2_Covered_by_ML_df.append(final_df_2_Not_Covered_by_ML_df)\n",
    "\n",
    "def ui_action_column(param_final_df):\n",
    "    param_final_df.loc[((param_final_df['ML_flag'] == 'Not_Covered_by_ML')),'ActionType'] = 'No Prediction'    \n",
    "    param_final_df.loc[((param_final_df['Predicted_Status'] == 'OB') & (param_final_df['PredictedComment'] == '') & (param_final_df['ML_flag'] == 'ML')),'ActionType'] = 'No Action'\n",
    "    param_final_df.loc[((param_final_df['Predicted_Status'] == 'OB') & (param_final_df['PredictedComment'] != '') & (param_final_df['ML_flag'] == 'ML')),'ActionType'] = 'COMMENT'\n",
    "    param_final_df.loc[((param_final_df['Predicted_Status'] == 'UCB') & (param_final_df['PredictedComment'] == '') & (param_final_df['ML_flag'] == 'ML')),'ActionType'] = 'CLOSE'\n",
    "    param_final_df.loc[((param_final_df['Predicted_Status'].isin(['UMB','UMR','UMT'])) & (param_final_df['PredictedComment'] == '') & (param_final_df['ML_flag'] == 'ML')),'ActionType'] = 'PAIR'\n",
    "    param_final_df.loc[((param_final_df['Predicted_Status'].isin(['UMB','UMR','UMT'])) & (param_final_df['PredictedComment'] != '') & (param_final_df['ML_flag'] == 'ML')),'ActionType'] = 'PAIR WITH COMMENT'\n",
    "    param_final_df['ActionType'] = param_final_df['ActionType'].astype(str)\n",
    "    return(param_final_df)    \n",
    "\n",
    "final_df_2= ui_action_column(final_df_2)\n",
    "\n",
    "filepaths_final_df_2 = '\\\\\\\\vitblrdevcons01\\\\Raman  Strategy ML 2.0\\\\All_Data\\\\' + client + '\\\\final_df_2_setup_' + setup_code + '_date_' + str(date_i) + '_2.csv'\n",
    "final_df_2.to_csv(filepaths_final_df_2)\n",
    "\n",
    "\n",
    "#End of Commenting\n",
    "\n",
    "#data_dict = final_table_to_write.to_dict(\"records\")\n",
    "data_dict = final_df_2.to_dict(\"records_final\")\n",
    "coll_1_for_writing_prediction_data = db_1_for_MEO_data['MLPrediction_Cash']\n",
    "coll_1_for_writing_prediction_data.insert_many(data_dict) \n",
    "\n",
    "print(setup_code)\n",
    "print(date_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020-12-15T00:00:00    1553\n",
       "2019-04-22T00:00:00       1\n",
       "2020-03-04T00:00:00       1\n",
       "Name: ViewData.Task Business Date, dtype: int64"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meo_df['ViewData.Task Business Date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
