{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,1,5,10,15,16,17,18,30,34,35,37,39,41,42,47,50,51,53,56,58,61,62,64,65,66,67,68,69,70,71,72,74,78,82,83,85,87,88,97,101,114,117,125,126,128,130,135,147,148,149,150,151,152,157,162,164,170,172,198,206,212,214,229,242,245,252,260,267,268,269,270,278,284,285,286,290,309,316,351,353,357,358,360,362) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "uni2 = pd.read_csv('Lombard/249/ReconDB.HST_RecData_249_01_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni2['ViewData.Side1_UniqueIds'] = uni2['ViewData.Side1_UniqueIds'].fillna('BB')\n",
    "uni2['ViewData.Side0_UniqueIds'] = uni2['ViewData.Side0_UniqueIds'].fillna('AA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtm(x,y):\n",
    "    if ((x !='AA') & (y !='BB')):\n",
    "        y1 = y.split(',')\n",
    "        x1 = x.split(',')\n",
    "        return pd.Series([len(x1),len(y1)], index=['len_0', 'len_1'])\n",
    "    elif ((x !='AA') & (y =='BB')):\n",
    "        x1 = x.split(',')\n",
    "        \n",
    "        return pd.Series([len(x1),0], index=['len_0', 'len_1'])\n",
    "    elif ((x =='AA') & (y !='BB')):\n",
    "        y1 = y.split(',')\n",
    "        \n",
    "        return pd.Series([0,len(y1)], index=['len_0', 'len_1'])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        \n",
    "        return pd.Series([0,0], index=['len_0', 'len_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni2[['len_0','len_1']] = uni2.apply(lambda x : mtm(x['ViewData.Side0_UniqueIds'],x['ViewData.Side1_UniqueIds']),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtm_mark(x,y):\n",
    "    if ((x>1) &(y>1)):\n",
    "        return 'MTM'\n",
    "    elif((x==1) &(y==1)):\n",
    "        return 'OTO'\n",
    "    elif((x>1) &(y==1)):\n",
    "        return 'MTO'\n",
    "    elif((x==1) &(y>1)):\n",
    "        return 'OTM'\n",
    "    else:\n",
    "        return 'OB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni2['MTM_mark'] = uni2.apply(lambda x : mtm_mark(x['len_0'],x['len_1']),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni3 = uni2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation filters applied. Custodian account | Currency | Description\n",
    "\n",
    "dfk = uni3.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'])['ViewData.Net Amount Difference'].apply(list).reset_index()\n",
    "dfk1 = uni3.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'])['len_0'].sum().reset_index()\n",
    "dfk2 = uni3.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'])['len_1'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.merge(dfk,dfk1, on = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'], how = 'left')\n",
    "df_merge = pd.merge(df_merge,dfk2, on = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['single_sided'] = df_merge.apply(lambda x : 1 if ((x['len_0']==0) | (x['len_1']==0)) else 0, axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['len_amount'] = df_merge['ViewData.Net Amount Difference'].apply(lambda x : len(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We seaparte single sided and double sided reconcialiation here. 1 being single sided. We reconcile single sided first\n",
    "df_merge1 = df_merge[df_merge['single_sided']==1]\n",
    "df_merge2 = df_merge[df_merge['single_sided']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three common functions\n",
    "def subSum(numbers,total):\n",
    "    length = len(numbers)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if length <16:\n",
    "      \n",
    "            \n",
    "      \n",
    "        \n",
    "        for index,number in enumerate(numbers):\n",
    "            if np.isclose(number, total, atol=5.0).any():\n",
    "                return [number]\n",
    "                print(34567)\n",
    "            subset = subSum(numbers[index+1:],total-number)\n",
    "            if subset:\n",
    "                #print(12345)\n",
    "                return [number] + subset\n",
    "        return []\n",
    "    else:\n",
    "        return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amt_marker(x,y,z):\n",
    "    if type(y)==list:\n",
    "        if ((x in y) & ((z<16) & (z>=2))) :\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mark(x,z,k):\n",
    "    \n",
    "   \n",
    "    if ((x>1) & (x<16)):\n",
    "        if ((k<6.0) & (z==0)):\n",
    "            return 1\n",
    "        elif ((k==0.0) & (z!=0)):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "if df_merge1.shape[0]!=0:\n",
    "    df_merge1['zero_list'] = df_merge1['ViewData.Net Amount Difference'].apply(lambda x : subSum(x,0))\n",
    "    df_merge1['len_zero_list'] = df_merge1['zero_list'].apply(lambda x : len(x))\n",
    "    df_merge1 = df_merge1.drop(['len_0','len_1','single_sided','ViewData.Net Amount Difference'], axis = 1)\n",
    "    uni4 = pd.merge(uni3, df_merge1, on = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'], how = 'left' )\n",
    "    uni4['amt_marker'] = uni4.apply(lambda x : amt_marker(x['ViewData.Net Amount Difference'],x['zero_list'] ,x['len_zero_list']) , axis = 1)\n",
    "\n",
    "    if uni4[uni4['amt_marker'] != 0].shape[0]!=0:\n",
    "        k = uni4[(uni4['amt_marker'] == 1)]\n",
    "        k['predicted status'] = 'UCB'\n",
    "        k['predicted action'] = 'Close'\n",
    "        k['predicted category'] = 'one sided close'\n",
    "        k['predicted comment'] = 'Match'\n",
    "        sel_col_1 = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']\n",
    "        k = k[sel_col_1]\n",
    "        k.to_csv('prediction result lombard 249 P1.csv')\n",
    "        uni5 = uni4[(uni4['amt_marker'] == 0)]\n",
    "    \n",
    "        \n",
    "        uni5 = uni5.drop(['len_0','len_1','len_amount', 'zero_list', 'len_zero_list','amt_marker'], axis = 1)\n",
    "        dummy = uni5.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency'])['ViewData.Net Amount Difference'].apply(list).reset_index()\n",
    "        dummy['len_amount'] = dummy['ViewData.Net Amount Difference'].apply(lambda x : len(x))\n",
    "        dummy['zero_list'] = dummy['ViewData.Net Amount Difference'].apply(lambda x : subSum(x,0))\n",
    "        dummy['len_zero_list'] = dummy['zero_list'].apply(lambda x : len(x))\n",
    "        dummy['diff_len'] = dummy['len_amount'] - dummy['len_zero_list']\n",
    "        dummy['zero_list_sum'] = dummy['zero_list'].apply(lambda x : round(abs(sum(x)),2))\n",
    "    #dummy = pd.merge(dummy, pos , on = ['Custodian Account','Currency','Ticker1'], how = 'left')\n",
    "        dummy['remove_mark'] = dummy.apply(lambda x :remove_mark(x['len_zero_list'],x['diff_len'],x['zero_list_sum']),axis = 1)\n",
    "        dummy = dummy[['ViewData.Mapped Custodian Account','ViewData.Currency',  'zero_list', 'len_zero_list', 'remove_mark']]\n",
    "        uni4 = pd.merge(uni3, dummy, on = ['ViewData.Mapped Custodian Account','ViewData.Currency'], how = 'left')\n",
    "        uni4['amt_marker'] = uni4.apply(lambda x : amt_marker(x['ViewData.Net Amount Difference'],x['zero_list'] ,x['len_zero_list']) , axis = 1)\n",
    "        \n",
    "        \n",
    "        if uni4[(uni4['amt_marker'] != 0) & (uni4['remove_mark'] != 0) ].shape[0]!=0:\n",
    "            k = uni4[(uni4['amt_marker'] != 0) & (uni4['remove_mark'] != 0)]\n",
    "            k['predicted status'] = 'UCB'\n",
    "            k['predicted action'] = 'Close'\n",
    "            k['predicted category'] = 'one sided close'\n",
    "            k['predicted comment'] = 'Match'\n",
    "            sel_col_1 = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']\n",
    "            k = k[sel_col_1]\n",
    "            k.to_csv('prediction result lombard 249 P2.csv')\n",
    "            uni5 = uni4[(uni4['amt_marker'] == 0)]\n",
    "            uni5 = uni5.drop(['zero_list', 'len_zero_list', 'remove_mark','amt_marker'], axis = 1)\n",
    "            remain_df1 = uni5.copy()\n",
    "        else:\n",
    "            uni5 = uni4.copy()\n",
    "            uni5 = uni5.drop(['zero_list', 'len_zero_list', 'remove_mark','amt_marker'], axis = 1)\n",
    "            remain_df1 = uni5.copy()\n",
    "            \n",
    "        \n",
    "    \n",
    "    else:\n",
    "        uni5 = uni4.copy()\n",
    "        uni5 = uni5.drop(['len_0','len_1','len_amount', 'zero_list', 'len_zero_list','amt_marker'], axis = 1)\n",
    "        remain_df1 = uni5.copy()\n",
    "else:\n",
    "    m = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45487, 370)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remain_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViewData.Mapped Custodian Account</th>\n",
       "      <th>ViewData.Currency</th>\n",
       "      <th>ViewData.Description</th>\n",
       "      <th>ViewData.Net Amount Difference</th>\n",
       "      <th>len_0</th>\n",
       "      <th>len_1</th>\n",
       "      <th>single_sided</th>\n",
       "      <th>len_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>BNP - ECF-BNP</td>\n",
       "      <td>USD</td>\n",
       "      <td>GOLAR LNG ORD</td>\n",
       "      <td>[-200.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>BNP - ECF-BNP</td>\n",
       "      <td>USD</td>\n",
       "      <td>HUR    7.500 07/24/22 CVT</td>\n",
       "      <td>[1832.48999999929]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>BNP - ECF-BNP</td>\n",
       "      <td>USD</td>\n",
       "      <td>VGR    5.500 04/15/20 CVT</td>\n",
       "      <td>[689.8399999998511, -70835.63, -70835.63, -839...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>Barclays - CRV-BARC</td>\n",
       "      <td>USD</td>\n",
       "      <td>CCLX  11.500 04/01/23 '23</td>\n",
       "      <td>[-0.0100000000093132, -265266.67]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>Barclays - CRV-BARC</td>\n",
       "      <td>USD</td>\n",
       "      <td>CLEVELAND CLIFFS ORD</td>\n",
       "      <td>[-18240.0, -2262260.0, 29.41, 12245.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13020</td>\n",
       "      <td>UBS - VOL-UBS-ISDA</td>\n",
       "      <td>GBP</td>\n",
       "      <td>TT ELECTRONICS ORD</td>\n",
       "      <td>[80289.09, 1.52999999999884]</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13023</td>\n",
       "      <td>UBS - VOL-UBS-ISDA</td>\n",
       "      <td>GBP</td>\n",
       "      <td>UNITE GROUP REIT</td>\n",
       "      <td>[198070.78, 29.609999999986]</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13027</td>\n",
       "      <td>UBS - VOL-UBS-ISDA</td>\n",
       "      <td>GBP</td>\n",
       "      <td>WILLIAM HILL ORD</td>\n",
       "      <td>[8052.86]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13029</td>\n",
       "      <td>UBS - VOL-UBS-ISDA</td>\n",
       "      <td>GBP</td>\n",
       "      <td>XP POWER ORD</td>\n",
       "      <td>[952213.68, -934111.09, 18102.5900000001, -103...</td>\n",
       "      <td>18</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13033</td>\n",
       "      <td>UBS - VOL-UBS-ISDA</td>\n",
       "      <td>USD</td>\n",
       "      <td>CENTAMIN ORD</td>\n",
       "      <td>[3946.08]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2026 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ViewData.Mapped Custodian Account ViewData.Currency  \\\n",
       "13                        BNP - ECF-BNP               USD   \n",
       "14                        BNP - ECF-BNP               USD   \n",
       "92                        BNP - ECF-BNP               USD   \n",
       "155                 Barclays - CRV-BARC               USD   \n",
       "156                 Barclays - CRV-BARC               USD   \n",
       "...                                 ...               ...   \n",
       "13020                UBS - VOL-UBS-ISDA               GBP   \n",
       "13023                UBS - VOL-UBS-ISDA               GBP   \n",
       "13027                UBS - VOL-UBS-ISDA               GBP   \n",
       "13029                UBS - VOL-UBS-ISDA               GBP   \n",
       "13033                UBS - VOL-UBS-ISDA               USD   \n",
       "\n",
       "            ViewData.Description  \\\n",
       "13                 GOLAR LNG ORD   \n",
       "14     HUR    7.500 07/24/22 CVT   \n",
       "92     VGR    5.500 04/15/20 CVT   \n",
       "155    CCLX  11.500 04/01/23 '23   \n",
       "156         CLEVELAND CLIFFS ORD   \n",
       "...                          ...   \n",
       "13020         TT ELECTRONICS ORD   \n",
       "13023           UNITE GROUP REIT   \n",
       "13027           WILLIAM HILL ORD   \n",
       "13029               XP POWER ORD   \n",
       "13033               CENTAMIN ORD   \n",
       "\n",
       "                          ViewData.Net Amount Difference  len_0  len_1  \\\n",
       "13                                              [-200.0]      1      1   \n",
       "14                                    [1832.48999999929]      1      1   \n",
       "92     [689.8399999998511, -70835.63, -70835.63, -839...      4      1   \n",
       "155                    [-0.0100000000093132, -265266.67]      2      1   \n",
       "156               [-18240.0, -2262260.0, 29.41, 12245.0]      4      1   \n",
       "...                                                  ...    ...    ...   \n",
       "13020                       [80289.09, 1.52999999999884]     10     12   \n",
       "13023                       [198070.78, 29.609999999986]      1      4   \n",
       "13027                                          [8052.86]      1      1   \n",
       "13029  [952213.68, -934111.09, 18102.5900000001, -103...     18     81   \n",
       "13033                                          [3946.08]      1      1   \n",
       "\n",
       "       single_sided  len_amount  \n",
       "13                0           1  \n",
       "14                0           1  \n",
       "92                0           4  \n",
       "155               0           2  \n",
       "156               0           4  \n",
       "...             ...         ...  \n",
       "13020             0           2  \n",
       "13023             0           2  \n",
       "13027             0           1  \n",
       "13029             0          30  \n",
       "13033             0           1  \n",
       "\n",
       "[2026 rows x 8 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ViewData.Account Name', 'ViewData.Account Type',\n",
       "       'ViewData.Account Type Difference', 'ViewData.Accounting Account Type',\n",
       "       'ViewData.Accounting Activity Code',\n",
       "       'ViewData.Accounting Asset Type Category',\n",
       "       'ViewData.Accounting Base Currency',\n",
       "       'ViewData.Accounting Base Net Amount',\n",
       "       'ViewData.Accounting Bloomberg_Yellow_Key',\n",
       "       'ViewData.Accounting Break Tag',\n",
       "       ...\n",
       "       'ViewData.Value Date', 'ViewData.Value Date Difference',\n",
       "       'ViewData.Workflow Remark', 'ViewData.Workflow Status',\n",
       "       'ViewData._Actions', 'ViewData._GroupID', 'ViewData._ID', 'len_0',\n",
       "       'len_1', 'MTM_mark'],\n",
       "      dtype='object', length=370)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remain_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "remain_df1 = remain_df1.drop(['len_0','len_1', 'MTM_mark'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni3 = remain_df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ViewData.Mapped Custodian Account', 'ViewData.Currency',\n",
       "       'ViewData.Description', 'ViewData.Net Amount Difference', 'len_0',\n",
       "       'len_1', 'single_sided', 'len_amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "if df_merge2.shape[0]!=0:\n",
    "    df_merge2['zero_list'] = df_merge2['ViewData.Net Amount Difference'].apply(lambda x : subSum(x,0))\n",
    "    df_merge2['len_zero_list'] = df_merge2['zero_list'].apply(lambda x : len(x))\n",
    "    df_merge2 = df_merge2.drop(['len_0','len_1','single_sided','ViewData.Net Amount Difference'], axis = 1)\n",
    "    uni4 = pd.merge(uni3, df_merge2, on = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'], how = 'left' )\n",
    "    uni4['amt_marker'] = uni4.apply(lambda x : amt_marker(x['ViewData.Net Amount Difference'],x['zero_list'] ,x['len_zero_list']) , axis = 1)\n",
    "\n",
    "    if uni4[uni4['amt_marker'] != 0].shape[0]!=0:\n",
    "        k = uni4[(uni4['amt_marker'] == 1)]\n",
    "        k['predicted status'] = 'UCB'\n",
    "        k['predicted action'] = 'Close'\n",
    "        k['predicted category'] = 'one sided close'\n",
    "        k['predicted comment'] = 'Match'\n",
    "        sel_col_1 = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']\n",
    "        k = k[sel_col_1]\n",
    "        k.to_csv('prediction result lombard 249 P3.csv')\n",
    "        uni5 = uni4[(uni4['amt_marker'] == 0)]\n",
    "    \n",
    "        \n",
    "        uni5 = uni5.drop(['len_amount', 'zero_list', 'len_zero_list','amt_marker'], axis = 1)\n",
    "        dummy = uni5.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency'])['ViewData.Net Amount Difference'].apply(list).reset_index()\n",
    "        dummy['len_amount'] = dummy['ViewData.Net Amount Difference'].apply(lambda x : len(x))\n",
    "        dummy['zero_list'] = dummy['ViewData.Net Amount Difference'].apply(lambda x : subSum(x,0))\n",
    "        dummy['len_zero_list'] = dummy['zero_list'].apply(lambda x : len(x))\n",
    "        dummy['diff_len'] = dummy['len_amount'] - dummy['len_zero_list']\n",
    "        dummy['zero_list_sum'] = dummy['zero_list'].apply(lambda x : round(abs(sum(x)),2))\n",
    "    #dummy = pd.merge(dummy, pos , on = ['Custodian Account','Currency','Ticker1'], how = 'left')\n",
    "        dummy['remove_mark'] = dummy.apply(lambda x :remove_mark(x['len_zero_list'],x['diff_len'],x['zero_list_sum']),axis = 1)\n",
    "        dummy = dummy[['ViewData.Mapped Custodian Account','ViewData.Currency',  'zero_list', 'len_zero_list', 'remove_mark']]\n",
    "        uni4 = pd.merge(uni3, dummy, on = ['ViewData.Mapped Custodian Account','ViewData.Currency'], how = 'left')\n",
    "        uni4['amt_marker'] = uni4.apply(lambda x : amt_marker(x['ViewData.Net Amount Difference'],x['zero_list'] ,x['len_zero_list']) , axis = 1)\n",
    "        \n",
    "        \n",
    "        if uni4[(uni4['amt_marker'] != 0) & (uni4['remove_mark'] != 0) ].shape[0]!=0:\n",
    "            k = uni4[(uni4['amt_marker'] != 0) & (uni4['remove_mark'] != 0)]\n",
    "            k['predicted status'] = 'UCB'\n",
    "            k['predicted action'] = 'Close'\n",
    "            k['predicted category'] = 'one sided close'\n",
    "            k['predicted comment'] = 'Match'\n",
    "            sel_col_1 = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted category','predicted comment']\n",
    "            k = k[sel_col_1]\n",
    "            k.to_csv('prediction result lombard 249 P4.csv')\n",
    "            uni5 = uni4[(uni4['amt_marker'] == 0)]\n",
    "            uni5 = uni5.drop(['zero_list', 'len_zero_list', 'remove_mark','amt_marker'], axis = 1)\n",
    "            remain_df1 = uni5.copy()\n",
    "        else:\n",
    "            uni5 = uni4.copy()\n",
    "            uni5 = uni5.drop(['zero_list', 'len_zero_list', 'remove_mark','amt_marker'], axis = 1)\n",
    "            remain_df2 = uni5.copy()\n",
    "            \n",
    "        \n",
    "    \n",
    "    else:\n",
    "        uni5 = uni4.copy()\n",
    "        uni5 = uni5.drop(['len_amount', 'zero_list', 'len_zero_list','amt_marker'], axis = 1)\n",
    "        remain_df2 = uni5.copy()\n",
    "else:\n",
    "    m = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ViewData.Account Name', 'ViewData.Account Type',\n",
       "       'ViewData.Account Type Difference', 'ViewData.Accounting Account Type',\n",
       "       'ViewData.Accounting Activity Code',\n",
       "       'ViewData.Accounting Asset Type Category',\n",
       "       'ViewData.Accounting Base Currency',\n",
       "       'ViewData.Accounting Base Net Amount',\n",
       "       'ViewData.Accounting Bloomberg_Yellow_Key',\n",
       "       'ViewData.Accounting Break Tag',\n",
       "       ...\n",
       "       'ViewData.UserTran1', 'ViewData.UserTran2',\n",
       "       'ViewData.UserTran2 Difference', 'ViewData.Value Date',\n",
       "       'ViewData.Value Date Difference', 'ViewData.Workflow Remark',\n",
       "       'ViewData.Workflow Status', 'ViewData._Actions', 'ViewData._GroupID',\n",
       "       'ViewData._ID'],\n",
       "      dtype='object', length=367)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni5.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M cross n architecture for UMB finding using desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_match(x,y):\n",
    "    if ((type(x) == str) & (type(y)==str)):\n",
    "        x = x.lower()\n",
    "        y = y.lower()\n",
    "        x1 =  re.split(\"[,/. \\- !?:]+\", x)\n",
    "        y1 =  re.split(\"[,/. \\- !?:]+\", y)\n",
    "        if len(x1)<len(y1):\n",
    "            lst3 = [value for value in x1 if value in y1]\n",
    "            \n",
    "            if len(lst3)>0:\n",
    "                score = len(lst3)/len(x1)\n",
    "            else:\n",
    "                score = 0\n",
    "        else:\n",
    "            lst3 = [value for value in y1 if value in x1]\n",
    "            \n",
    "            if len(lst3)>0:\n",
    "                \n",
    "                score = len(lst3)/len(y1)\n",
    "            else:\n",
    "                score = 0\n",
    "    else:\n",
    "        score = 2\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding OB for the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.Mapped Custodian Account','ViewData.Fund','ViewData.Task Business Date',\n",
    " 'ViewData.Currency',\n",
    " 'ViewData.Asset Type Category',\n",
    " 'ViewData.Transaction Type',\n",
    " 'ViewData.Investment Type',\n",
    " 'ViewData.Prime Broker',\n",
    " 'ViewData.Ticker',\n",
    " 'ViewData.Sec Fees',\n",
    " 'ViewData.Settle Date',\n",
    " 'ViewData.Trade Date',\n",
    " 'ViewData.Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni5 = uni5[filter_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni5[['len_0','len_1']] = uni5.apply(lambda x : mtm(x['ViewData.Side0_UniqueIds'],x['ViewData.Side1_UniqueIds']),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni5['MTM_mark'] = uni5.apply(lambda x : mtm_mark(x['len_0'],x['len_1']),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OB     41737\n",
       "OTO     1739\n",
       "MTM     1059\n",
       "OTM      655\n",
       "MTO      258\n",
       "Name: MTM_mark, dtype: int64"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni5['MTM_mark'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "ob_df= uni5[uni5['MTM_mark'] == 'OB']\n",
    "side_0 = ob_df[ob_df['ViewData.Side1_UniqueIds']=='BB']\n",
    "side_0['final_id'] = side_0['ViewData.Side0_UniqueIds']\n",
    "side_1 = ob_df[ob_df['ViewData.Side0_UniqueIds']=='AA']\n",
    "side_1['final_id'] = side_1['ViewData.Side1_UniqueIds']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "side0_otm= uni5[uni5['MTM_mark'] != 'OB']\n",
    "side0_otm['final_id'] = side0_otm['ViewData.Side0_UniqueIds'].astype(str) + '|' + side0_otm['ViewData.Side1_UniqueIds'].astype(str) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_0 = pd.concat([side_0,side0_otm], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_1 = side_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/367 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(5, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(4, 19)\n",
      "(6, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                         | 4/367 [00:00<00:11, 32.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n",
      "(2, 19)\n",
      "(7, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▉                                         | 8/367 [00:00<00:10, 33.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n",
      "(12, 19)\n",
      "(4, 19)\n",
      "(4, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(4, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                       | 12/367 [00:00<00:10, 34.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(10, 19)\n",
      "(15, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▊                                       | 16/367 [00:00<00:09, 35.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 19)\n",
      "(16, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(6, 19)\n",
      "(5, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                      | 20/367 [00:00<00:09, 34.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(5, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|██▋                                      | 24/367 [00:00<00:09, 35.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(4, 19)\n",
      "(7, 19)\n",
      "(14, 19)\n",
      "(14, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▏                                     | 29/367 [00:00<00:09, 37.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(5, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(4, 19)\n",
      "(4, 19)\n",
      "(4, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▋                                     | 33/367 [00:00<00:08, 37.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 19)\n",
      "(4, 19)\n",
      "(14, 19)\n",
      "(9, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                    | 37/367 [00:01<00:08, 36.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(18, 19)\n",
      "(39, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                    | 41/367 [00:01<00:08, 36.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 19)\n",
      "(23, 19)\n",
      "(3, 19)\n",
      "(6, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                    | 45/367 [00:01<00:08, 36.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                   | 49/367 [00:01<00:09, 34.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(3, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(5, 19)\n",
      "(4, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                   | 53/367 [00:01<00:09, 34.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(5, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▎                                  | 57/367 [00:01<00:08, 35.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(5, 19)\n",
      "(8, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|██████▊                                  | 61/367 [00:01<00:08, 35.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 19)\n",
      "(13, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▎                                 | 65/367 [00:01<00:09, 33.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(0, 19)\n",
      "(3, 19)\n",
      "(8, 19)\n",
      "(8, 19)\n",
      "(13, 19)\n",
      "(8, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▊                                 | 70/367 [00:01<00:08, 35.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(4, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(4, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▎                                | 74/367 [00:02<00:08, 35.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 19)\n",
      "(7, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(24, 19)\n",
      "(22, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████▋                                | 78/367 [00:02<00:08, 34.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(11, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████▎                               | 83/367 [00:02<00:07, 36.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(13, 19)\n",
      "(22, 19)\n",
      "(26, 19)\n",
      "(21, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|█████████▋                               | 87/367 [00:02<00:07, 37.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(7, 19)\n",
      "(12, 19)\n",
      "(29, 19)\n",
      "(35, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(5, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████▎                              | 92/367 [00:02<00:07, 38.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(6, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██████████▋                              | 96/367 [00:02<00:06, 38.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(6, 19)\n",
      "(1, 19)\n",
      "(8, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(5, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|███████████                             | 101/367 [00:02<00:06, 39.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|███████████▌                            | 106/367 [00:02<00:06, 40.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(4, 19)\n",
      "(6, 19)\n",
      "(8, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(10, 19)\n",
      "(7, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████                            | 111/367 [00:02<00:06, 40.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(4, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(3, 19)\n",
      "(5, 19)\n",
      "(4, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████████▋                           | 116/367 [00:03<00:06, 40.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(1, 19)\n",
      "(10, 19)\n",
      "(8, 19)\n",
      "(22, 19)\n",
      "(17, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(3, 19)\n",
      "(5, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|█████████████▏                          | 121/367 [00:03<00:06, 39.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 19)\n",
      "(10, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(2, 19)\n",
      "(4, 19)\n",
      "(9, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|█████████████▌                          | 125/367 [00:03<00:06, 38.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n",
      "(6, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████                          | 129/367 [00:03<00:06, 38.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(7, 19)\n",
      "(4, 19)\n",
      "(5, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████▌                         | 134/367 [00:03<00:05, 39.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(25, 19)\n",
      "(28, 19)\n",
      "(3, 19)\n",
      "(6, 19)\n",
      "(0, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████▏                        | 139/367 [00:03<00:05, 40.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(5, 19)\n",
      "(26, 19)\n",
      "(31, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███████████████▋                        | 144/367 [00:03<00:05, 40.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n",
      "(4, 19)\n",
      "(6, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████████████████▏                       | 149/367 [00:03<00:05, 39.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(10, 19)\n",
      "(11, 19)\n",
      "(3, 19)\n",
      "(10, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(12, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████▊                       | 154/367 [00:04<00:05, 40.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(4, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(10, 19)\n",
      "(9, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|█████████████████▎                      | 159/367 [00:04<00:05, 39.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|█████████████████▊                      | 163/367 [00:04<00:05, 39.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|██████████████████▏                     | 167/367 [00:04<00:05, 39.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|██████████████████▋                     | 171/367 [00:04<00:05, 38.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(3, 19)\n",
      "(37, 19)\n",
      "(31, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████                     | 175/367 [00:04<00:05, 36.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|███████████████████▌                    | 179/367 [00:04<00:05, 35.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(0, 19)\n",
      "(11, 19)\n",
      "(16, 19)\n",
      "(2, 19)\n",
      "(6, 19)\n",
      "(3, 19)\n",
      "(5, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████▉                    | 183/367 [00:04<00:05, 35.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(6, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|████████████████████▍                   | 187/367 [00:04<00:05, 35.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(5, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████▉                   | 192/367 [00:05<00:04, 38.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 19)\n",
      "(13, 19)\n",
      "(1, 19)\n",
      "(4, 19)\n",
      "(5, 19)\n",
      "(6, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n",
      "(6, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████████████████████▍                  | 197/367 [00:05<00:04, 39.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████▉                  | 201/367 [00:05<00:04, 37.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 19)\n",
      "(16, 19)\n",
      "(9, 19)\n",
      "(5, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(6193, 19)\n",
      "(36357, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████▎                 | 205/367 [00:13<01:39,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n",
      "(6, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████▊                 | 209/367 [00:13<01:08,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n",
      "(6, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(7, 19)\n",
      "(7, 19)\n",
      "(3, 19)\n",
      "(6, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████▏                | 213/367 [00:13<00:48,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(5, 19)\n",
      "(2, 19)\n",
      "(5, 19)\n",
      "(2, 19)\n",
      "(4, 19)\n",
      "(0, 19)\n",
      "(5, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|███████████████████████▊                | 218/367 [00:13<00:33,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(10, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n",
      "(5, 19)\n",
      "(2, 19)\n",
      "(5, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|████████████████████████▎               | 223/367 [00:13<00:23,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(2, 19)\n",
      "(28, 19)\n",
      "(23, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(4, 19)\n",
      "(5, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████▋               | 227/367 [00:13<00:17,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(2, 19)\n",
      "(4, 19)\n",
      "(4, 19)\n",
      "(25, 19)\n",
      "(24, 19)\n",
      "(3, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████▏              | 231/367 [00:13<00:13, 10.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(7, 19)\n",
      "(8, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|█████████████████████████▌              | 235/367 [00:14<00:10, 13.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n",
      "(6, 19)\n",
      "(2, 19)\n",
      "(6, 19)\n",
      "(2, 19)\n",
      "(7, 19)\n",
      "(18, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████████████████████████              | 239/367 [00:14<00:07, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(3, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(8, 19)\n",
      "(10, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████████████████████████▍             | 243/367 [00:14<00:06, 19.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(6, 19)\n",
      "(8, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████▉             | 247/367 [00:14<00:05, 22.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████▎            | 251/367 [00:14<00:04, 25.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(4, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(5, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|███████████████████████████▊            | 255/367 [00:14<00:03, 28.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(6, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████▎           | 260/367 [00:14<00:03, 31.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|████████████████████████████▉           | 265/367 [00:14<00:02, 34.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(4, 19)\n",
      "(2, 19)\n",
      "(4, 19)\n",
      "(6, 19)\n",
      "(6, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████▍          | 270/367 [00:14<00:02, 36.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(3, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(8, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████▉          | 275/367 [00:15<00:02, 36.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(1, 19)\n",
      "(13, 19)\n",
      "(12, 19)\n",
      "(5, 19)\n",
      "(9, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|██████████████████████████████▍         | 279/367 [00:15<00:02, 37.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(2, 19)\n",
      "(8, 19)\n",
      "(9, 19)\n",
      "(5, 19)\n",
      "(11, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|██████████████████████████████▉         | 284/367 [00:15<00:02, 37.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 19)\n",
      "(5, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(5, 19)\n",
      "(7, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████████████████████████████▍        | 289/367 [00:15<00:02, 38.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(8, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(4, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████▉        | 293/367 [00:15<00:01, 38.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(4, 19)\n",
      "(6, 19)\n",
      "(6, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████████████████████████████▎       | 297/367 [00:15<00:01, 37.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 19)\n",
      "(5, 19)\n",
      "(1, 19)\n",
      "(7, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(4, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████████████████████████████▊       | 301/367 [00:15<00:01, 38.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|█████████████████████████████████▎      | 306/367 [00:15<00:01, 38.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(3, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████▉      | 311/367 [00:15<00:01, 39.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n",
      "(6, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(0, 19)\n",
      "(0, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████▎     | 315/367 [00:16<00:01, 38.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(3, 19)\n",
      "(3, 19)\n",
      "(5, 19)\n",
      "(0, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|██████████████████████████████████▊     | 319/367 [00:16<00:01, 38.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(4, 19)\n",
      "(4, 19)\n",
      "(2, 19)\n",
      "(6, 19)\n",
      "(7, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|███████████████████████████████████▏    | 323/367 [00:16<00:01, 37.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 19)\n",
      "(19, 19)\n",
      "(4, 19)\n",
      "(9, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████▋    | 327/367 [00:16<00:01, 37.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 19)\n",
      "(11, 19)\n",
      "(14, 19)\n",
      "(11, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████    | 331/367 [00:16<00:00, 36.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|████████████████████████████████████▌   | 335/367 [00:16<00:00, 36.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 19)\n",
      "(9, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(5, 19)\n",
      "(6, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|████████████████████████████████████▉   | 339/367 [00:16<00:00, 36.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(5, 19)\n",
      "(39, 19)\n",
      "(38, 19)\n",
      "(1, 19)\n",
      "(3, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████▍  | 344/367 [00:16<00:00, 37.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n",
      "(2, 19)\n",
      "(3, 19)\n",
      "(1, 19)\n",
      "(6, 19)\n",
      "(7, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████▉  | 348/367 [00:16<00:00, 38.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(4, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████▎ | 352/367 [00:17<00:00, 36.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 19)\n",
      "(31, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n",
      "(4, 19)\n",
      "(5, 19)\n",
      "(2, 19)\n",
      "(2, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|██████████████████████████████████████▊ | 356/367 [00:17<00:00, 36.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(5, 19)\n",
      "(5, 19)\n",
      "(1, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|███████████████████████████████████████▏| 360/367 [00:17<00:00, 36.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "(5, 19)\n",
      "(1, 19)\n",
      "(2, 19)\n",
      "(15, 19)\n",
      "(10, 19)\n",
      "(2, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|███████████████████████████████████████▋| 364/367 [00:17<00:00, 34.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n",
      "(0, 19)\n",
      "(1, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 367/367 [00:17<00:00, 20.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from pandas import merge\n",
    "from tqdm import tqdm\n",
    "\n",
    "pool =[]\n",
    "key_index =[]\n",
    "training_df =[]\n",
    "call1 = []\n",
    "\n",
    "appended_data = []\n",
    "\n",
    "no_pair_ids = []\n",
    "#max_rows = 5\n",
    "\n",
    "k = list(set(list(set(umb_0['ViewData.Task Business Date'])) + list(set(umb_1['ViewData.Task Business Date']))))\n",
    "k1 = k\n",
    "\n",
    "for d in tqdm(k1):\n",
    "    aa1 = umb_0[umb_0['ViewData.Task Business Date']==d]\n",
    "    bb1 = umb_1[umb_1['ViewData.Task Business Date']==d]\n",
    "#     aa1['marker'] = 1\n",
    "#     bb1['marker'] = 1\n",
    "    \n",
    "    aa1 = aa1.reset_index()\n",
    "    aa1 = aa1.drop('index',1)\n",
    "    bb1 = bb1.reset_index()\n",
    "    bb1 = bb1.drop('index', 1)\n",
    "    print(aa1.shape)\n",
    "    print(bb1.shape)\n",
    "    \n",
    "    aa1.columns = ['SideB.' + x  for x in aa1.columns] \n",
    "    bb1.columns = ['SideA.' + x  for x in bb1.columns]\n",
    "    \n",
    "    cc1 = pd.merge(aa1,bb1, left_on = ['SideB.ViewData.Mapped Custodian Account','SideB.ViewData.Currency'], right_on = ['SideA.ViewData.Mapped Custodian Account','SideA.ViewData.Currency'], how = 'outer')\n",
    "    appended_data.append(cc1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbmn = pd.concat(appended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-526-a65681017c59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mumbmn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'desc_score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mumbmn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mdesc_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SideA.ViewData.Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SideB.ViewData.Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6911\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6912\u001b[0m         )\n\u001b[1;32m-> 6913\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6915\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m                 result = reduction.reduce(\n\u001b[1;32m--> 285\u001b[1;33m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m                 )\n\u001b[0;32m    287\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.reduce\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-526-a65681017c59>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mumbmn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'desc_score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mumbmn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mdesc_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SideA.ViewData.Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SideB.ViewData.Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4706\u001b[0m         \u001b[1;31m# use this, e.g. DatetimeIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4707\u001b[0m         \u001b[1;31m# Things like `Series._get_value` (via .at) pass the EA directly here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4708\u001b[1;33m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_values\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4709\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mExtensionArray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4710\u001b[0m             \u001b[1;31m# GH 20882, 21257\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minternal\u001b[0m \u001b[0mrepr\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \"\"\"\n\u001b[1;32m--> 564\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_formatting_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "umbmn['desc_score'] =  umbmn.apply(lambda x : desc_match(x['SideA.ViewData.Description'],x['SideB.ViewData.Description']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input for UMB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now remove those pair where one side is absent\n",
    "ab_a = umbmn[(umbmn['SideB.final_id'].isna()) | (umbmn['SideA.final_id'].isna())]\n",
    "ab_b = umbmn[~((umbmn['SideB.final_id'].isna()) | (umbmn['SideA.final_id'].isna()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbk = ab_b[['SideB.ViewData.Side0_UniqueIds', 'SideB.ViewData.Side1_UniqueIds','SideB.final_id', 'SideA.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds','SideA.final_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_umb = ['SideB.ViewData.Asset Type Category',\n",
    "        'SideB.ViewData.Fund',\n",
    "       'SideB.ViewData.Investment Type',\n",
    "        'SideB.ViewData.Ticker','SideB.ViewData.Transaction Type',\n",
    "      'SideB.ViewData.Mapped Custodian Account','SideB.ViewData.Currency', 'SideA.ViewData.Asset Type Category',\n",
    "        'SideA.ViewData.Fund','SideA.ViewData.Investment Type',\n",
    "        'SideA.ViewData.Ticker','SideA.ViewData.Transaction Type',\n",
    "      'SideA.ViewData.Mapped Custodian Account','SideA.ViewData.Currency', 'desc_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_file = ab_b[col_umb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_vars = [ \n",
    "      \n",
    "#       'SideB.ViewData.Asset Type Category',\n",
    "#         'SideB.ViewData.Fund',\n",
    "#        'SideB.ViewData.Investment Type',\n",
    "#         'SideB.ViewData.Ticker','SideB.ViewData.Transaction Type',\n",
    "#       'SideB.ViewData.Currency', 'SideB.ViewData.Mapped Custodian Account','SideA.ViewData.Asset Type Category',\n",
    "#         'SideA.ViewData.Fund','SideA.ViewData.Investment Type',\n",
    "#         'SideA.ViewData.Ticker','SideA.ViewData.Transaction Type',\n",
    "#       'SideA.ViewData.Currency', 'SideA.ViewData.Mapped Custodian Account'\n",
    "#        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "umb_file['SideB.ViewData.Asset Type Category'] = umb_file['SideB.ViewData.Asset Type Category'].fillna('AA')\n",
    "umb_file['SideB.ViewData.Fund'] = umb_file['SideB.ViewData.Fund'].fillna('BB')\n",
    "umb_file['SideB.ViewData.Investment Type'] = umb_file['SideB.ViewData.Investment Type'].fillna('CC')\n",
    "umb_file['SideB.ViewData.Ticker'] = umb_file['SideB.ViewData.Ticker'].fillna('DD')\n",
    "umb_file['SideB.ViewData.Transaction Type'] = umb_file['SideB.ViewData.Transaction Type'].fillna('EE')\n",
    "umb_file['SideB.ViewData.Currency'] = umb_file['SideB.ViewData.Currency'].fillna('FF')\n",
    "umb_file['SideB.ViewData.Mapped Custodian Account'] = umb_file['SideB.ViewData.Mapped Custodian Account'].fillna('GG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "umb_file['SideA.ViewData.Asset Type Category'] = umb_file['SideA.ViewData.Asset Type Category'].fillna('aa')\n",
    "umb_file['SideA.ViewData.Fund'] = umb_file['SideA.ViewData.Fund'].fillna('bb')\n",
    "umb_file['SideA.ViewData.Investment Type'] = umb_file['SideA.ViewData.Investment Type'].fillna('cc')\n",
    "umb_file['SideA.ViewData.Ticker'] = umb_file['SideA.ViewData.Ticker'].fillna('dd')\n",
    "umb_file['SideA.ViewData.Transaction Type'] = umb_file['SideA.ViewData.Transaction Type'].fillna('ee')\n",
    "umb_file['SideA.ViewData.Currency'] = umb_file['SideA.ViewData.Currency'].fillna('ff')\n",
    "umb_file['SideA.ViewData.Mapped Custodian Account'] = umb_file['SideA.ViewData.Mapped Custodian Account'].fillna('gg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in list(umb_file.columns):\n",
    "    \n",
    "    x1 = item.split('.')\n",
    "    if 'desc_score' not in x1:\n",
    "    \n",
    "        if x1[0]=='SideB':\n",
    "            m = 'ViewData.' + 'Accounting'+ \" \" + x1[2]\n",
    "            umb_file = umb_file.rename(columns = {item:m})\n",
    "        else:\n",
    "            m = 'ViewData.' + 'B-P'+ \" \" + x1[2]\n",
    "            umb_file =umb_file.rename(columns = {item:m})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model_lombard_249_umb_v3.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_predictions = clf.predict(umb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = []\n",
    "for item in cb_predictions:\n",
    "    demo.append(item[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_file['predicted'] = pd.Series(demo)\n",
    "#result['Actual'] = pd.Series(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OB     3249393\n",
       "UMB      46018\n",
       "Name: predicted, dtype: int64"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umb_file['predicted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ViewData.Accounting Asset Type Category', 'ViewData.Accounting Fund',\n",
       "       'ViewData.Accounting Investment Type', 'ViewData.Accounting Ticker',\n",
       "       'ViewData.Accounting Transaction Type',\n",
       "       'ViewData.Accounting Mapped Custodian Account',\n",
       "       'ViewData.Accounting Currency', 'ViewData.B-P Asset Type Category',\n",
       "       'ViewData.B-P Fund', 'ViewData.B-P Investment Type',\n",
       "       'ViewData.B-P Ticker', 'ViewData.B-P Transaction Type',\n",
       "       'ViewData.B-P Mapped Custodian Account', 'ViewData.B-P Currency',\n",
       "       'desc_score', 'predicted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umb_file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbpred = pd.concat([umbk,umb_file], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbpred =umbpred.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-22589358c7d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# We first segragate ab_a, then next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mside0_ab_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mab_a\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mab_a\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SideB.final_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mside1_ab_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mab_a\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mab_a\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SideB.final_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mside0_ab_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mside1_ab_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ab_a' is not defined"
     ]
    }
   ],
   "source": [
    "# We first segragate ab_a, then next\n",
    "side0_ab_a = ab_a[ab_a['SideB.final_id'].isna()]\n",
    "side1_ab_a = ab_a[~ab_a['SideB.final_id'].isna()]\n",
    "\n",
    "if ((side0_ab_a.shape[0]!=0) & (side1_ab_a.shape[0]!=0)):\n",
    "    list_id_0_ab_a = list(set(ab_a['SideA.final_id']))\n",
    "    list_id_1_ab_a = list(set(ab_a['SideB.final_id']))\n",
    "    side0_ob = umb_0[umb_0['final_id'].isin(list_id_1_ab_a)]\n",
    "    side1_ob = umb_1[umb_1['final_id'].isin(list_id_0_ab_a)]\n",
    "    ob_1st_set = pd.concat([side0_ob,side1_ob], axis = 0)\n",
    "    ob_1st_set = ob_1st_set.reset_index()\n",
    "    ob_1st_set = ob_1st_set.drop('index', axis = 1)\n",
    "elif (side0_ab_a.shape[0]!=0):\n",
    "    list_id_0_ab_a = list(set(ab_a['SideA.final_id']))\n",
    "    \n",
    "    side0_ob = umb_0[umb_0['final_id'].isin(list_id_1_ab_a)]\n",
    "    \n",
    "    ob_1st_set = side0_ob.copy()\n",
    "    ob_1st_set = ob_1st_set.reset_index()\n",
    "    ob_1st_set = ob_1st_set.drop('index', axis = 1)\n",
    "else:\n",
    "    list_id_1_ab_a = list(set(ab_a['SideB.final_id']))\n",
    "    \n",
    "    side1_ob = umb_1[umb_1['final_id'].isin(list_id_0_ab_a)]\n",
    "    \n",
    "    ob_1st_set = side1_ob.copy()\n",
    "    ob_1st_set = ob_1st_set.reset_index()\n",
    "    ob_1st_set = ob_1st_set.drop('index', axis = 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will segragate IDs on both sides which were just OB.\n",
    "k1 = umbpred.groupby('SideB.final_id')['predicted'].apply(list).reset_index()\n",
    "k2 = umbpred.groupby('SideA.final_id')['predicted'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ob_umb(x):\n",
    "    x1 = list(set(x))\n",
    "    if ((len(x1)==1) & ('OB' in x1)):\n",
    "        return 'OB'\n",
    "    elif ((len(x1)==1) & ('UMB' in x1)):\n",
    "        return 'UMB'\n",
    "    else:\n",
    "        return 'Both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1['State'] = k1['predicted'].apply(lambda x : ob_umb(x) )\n",
    "k2['State'] = k2['predicted'].apply(lambda x : ob_umb(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_id_0_k1 = list(set(k1[k1['State']=='OB']['SideB.final_id']))\n",
    "list_id_1_k2 = list(set(k2[k2['State']=='OB']['SideA.final_id']))\n",
    "side0_ob = umb_0[umb_0['final_id'].isin(list_id_0_k1)]\n",
    "side1_ob = umb_1[umb_1['final_id'].isin(list_id_1_k2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_2nd_set = pd.concat([side0_ob,side1_ob], axis = 0)\n",
    "ob_2nd_set = ob_2nd_set.reset_index()\n",
    "ob_2nd_set = ob_2nd_set.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31320, 19)"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob_2nd_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((ob_1st_set.shape[0]!=0) & (ob_2nd_set.shape[0]!=0)):\n",
    "    ob_for_comment = pd.concat([ob_1st_set,ob_2nd_set], axis = 0)\n",
    "    ob_for_comment = ob_for_comment.reset_index()\n",
    "    ob_for_comment = ob_for_comment.drop('index', axis = 1)\n",
    "elif ((ob_1st_set.shape[0]!=0)):\n",
    "    ob_for_comment = ob_1st_set.copy()\n",
    "    ob_for_comment = ob_for_comment.reset_index()\n",
    "    ob_for_comment = ob_for_comment.drop('index', axis = 1)\n",
    "else:\n",
    "    elif ((ob_2nd_set.shape[0]!=0)):\n",
    "    ob_for_comment = ob_2nd_set.copy()\n",
    "    ob_for_comment = ob_for_comment.reset_index()\n",
    "    ob_for_comment = ob_for_comment.drop('index', axis = 1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_for_comment.to_csv('Ob for comment daily lombard 249.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbpred = umbpred[~umbpred['SideB.final_id'].isin(list_id_0_k1)]\n",
    "umbpred = umbpred[~umbpred['SideA.final_id'].isin(list_id_1_k2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difficult part segragation of UMBs in OTO, OTM, MTO and MTM\n",
    "# OTO\n",
    "\n",
    "import collections\n",
    "\n",
    "def umb_counter(x):\n",
    "    counter=collections.Counter(x)\n",
    "    if counter['UMB'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return counter['UMB']\n",
    "        \n",
    "k1['umb_counter'] = k1['predicted'].apply(lambda x : umb_counter(x) )\n",
    "k2['umb_counter'] = k2['predicted'].apply(lambda x : umb_counter(x) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_id_0_k1 = list(set(k1[k1['umb_counter']==1]['SideB.final_id']))\n",
    "list_id_1_k2 = list(set(k2[k2['umb_counter']==1]['SideA.final_id']))\n",
    "if ((len(list_id_0_k1)>0) & (len(list_id_1_k2)>0)):\n",
    "    umb_oto = umbpred[(umbpred['SideB.final_id'].isin(list_id_0_k1)) & (umbpred['SideA.final_id'].isin(list_id_1_k2))]\n",
    "    umbpred = umbpred[~umbpred['SideB.final_id'].isin(list_id_0_k1)]\n",
    "    umbpred = umbpred[~umbpred['SideA.final_id'].isin(list_id_1_k2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now We write the hierarchy for many to one\n",
    "k3 = umbpred.groupby('SideB.final_id')['SideA.final_id'].apply(list).reset_index()\n",
    "k4 = umbpred.groupby('SideA.final_id')['SideB.final_id'].apply(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3['id_len'] = k3['SideA.final_id'].apply(lambda x : len(x) )\n",
    "k4['id_len'] = k4['SideB.final_id'].apply(lambda x : len(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return round((len(lst3)/len(lst1)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(li1, li2):\n",
    "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2]\n",
    "    return li_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ob_stage_df = pd.DataFrame()\n",
    "umb_mtm = pd.DataFrame()\n",
    "umb_mtm_list = []\n",
    "umb_otm_list = []\n",
    "\n",
    "for i, row in k3.iterrows():\n",
    "    k5 = k3.copy()\n",
    "    mid = row['SideB.final_id']\n",
    "    #print(mid)\n",
    "    midlist = row['SideA.final_id']\n",
    "    midlen = row['id_len']\n",
    "    k6 = k5[(k5['id_len']<midlen+3) & (k5['id_len']>midlen-4)]\n",
    "    k6['match'] = k6['SideA.final_id'].apply(lambda x:intersection_(x,midlist) )\n",
    "    \n",
    "    \n",
    "    k7 =list(set(k6[k6['match']>0.7]['SideB.final_id']))\n",
    "    if len(k7)>0:\n",
    "        \n",
    "        set_for_int = list((k6[k6['match']>0.7]['SideA.final_id']))\n",
    "        k8 = list(reduce(set.intersection, [set(item) for item in set_for_int]))\n",
    "        int1 = umbpred[umbpred['SideB.final_id'].isin(k7)]\n",
    "        k9 =list(set(int1['SideA.final_id']))\n",
    "        if ((len(k8)>0) & (len(k9)>0)):\n",
    "            umb_mtm_list_temp = []\n",
    "            umb_mtm_list_temp.append(k7)\n",
    "            umb_mtm_list_temp.append(k8)\n",
    "            umb_mtm_list.append(umb_mtm_list_temp)\n",
    "            \n",
    "            k10 = Diff(k9,k8)\n",
    "            umbpred = umbpred[~umbpred['SideB.final_id'].isin(k7)]\n",
    "            k11 = list(set(umbpred['SideA.final_id']))\n",
    "            k12 = Diff(k10,k11)\n",
    "            ob_3rd_set = umb_1[umb_1['final_id'].isin(k12)]\n",
    "            ob_stage_df = pd.concat([ob_stage_df,ob_3rd_set], axis = 0)\n",
    "            ob_stage_df = ob_stage_df.reset_index()\n",
    "            ob_stage_df = ob_stage_df.drop('index', axis = 1)\n",
    "            \n",
    "            int1 = umbpred[umbpred['SideA.final_id'].isin(k8)]\n",
    "            \n",
    "            k15 =list(set(int1['SideB.final_id']))\n",
    "            k16 = Diff(k15,k7)\n",
    "            umbpred = umbpred[~umbpred['SideA.final_id'].isin(k8)]\n",
    "            k17 = list(set(umbpred['SideB.final_id']))\n",
    "            k18 = Diff(k16,k17)\n",
    "            ob_4th_set = umb_0[umb_0['final_id'].isin(k18)]\n",
    "            ob_stage_df = pd.concat([ob_stage_df,ob_4th_set], axis = 0)\n",
    "            ob_stage_df = ob_stage_df.reset_index()\n",
    "            ob_stage_df = ob_stage_df.drop('index', axis = 1)\n",
    "        \n",
    "            k3 = umbpred.groupby('SideB.final_id')['SideA.final_id'].apply(list).reset_index()\n",
    "            k3['id_len'] = k3['SideA.final_id'].apply(lambda x : len(x) )\n",
    "            \n",
    "    else:\n",
    "        #k8 = list(reduce(set.intersection, [set(item) for item in set_for_int]))\n",
    "#         int1 = k3[k3['SideB.final_id']==mid]\n",
    "#         print(int1.shape[0])\n",
    "        mk = list(mid)\n",
    "  \n",
    "        if (len(midlist)>0):\n",
    "            print(1)\n",
    "            umb_otm_list_temp = []\n",
    "            umb_otm_list_temp.append(mid)\n",
    "            umb_otm_list_temp.append(midlist)\n",
    "            umb_otm_list.append(umb_otm_list_temp)\n",
    "            #k10 = Diff(midlist,mk) \n",
    "            knn = umbpred[umbpred['SideA.final_id'].isin(midlist)]\n",
    "            k11 = list(set(knn['SideB.final_id']))\n",
    "            k12 = Diff(k11,mk)\n",
    "            \n",
    "            umbpred = umbpred[~umbpred['SideA.final_id'].isin(midlist)]\n",
    "            k13 = list(set(umbpred['SideB.final_id']))\n",
    "            k14 = Diff(k12,k13)\n",
    "            \n",
    "            ob_4th_set = umb_0[umb_0['final_id'].isin(k14)]\n",
    "            ob_stage_df = pd.concat([ob_stage_df,ob_4th_set], axis = 0)\n",
    "            ob_stage_df = ob_stage_df.reset_index()\n",
    "            ob_stage_df = ob_stage_df.drop('index', axis = 1)\n",
    "        \n",
    "            k3 = umbpred.groupby('SideB.final_id')['SideA.final_id'].apply(list).reset_index()\n",
    "            k3['id_len'] = k3['SideA.final_id'].apply(lambda x : len(x) )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_mtm = pd.DataFrame(umb_mtm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_otm = pd.DataFrame(umb_otm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_otm.columns = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds']\n",
    "umb_mtm.columns = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_otm['predicted status'] = 'UMB'\n",
    "umb_otm['predicted action'] = 'UMB one to many'\n",
    "umb_otm['predicted category'] = 'UMB'\n",
    "umb_otm['predicted comment'] = 'Difference in amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_mtm['predicted status'] = 'UMB'\n",
    "umb_mtm['predicted action'] = 'UMB one to many'\n",
    "umb_mtm['predicted category'] = 'UMB'\n",
    "umb_mtm['predicted comment'] = 'Difference in amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_oto1 = umb_oto[['SideB.final_id','SideA.final_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_oto1 = umb_oto1.rename(columns = {'SideB.final_id':'ViewData.Side0_UniqueIds',\n",
    "                                     'SideA.final_id':'ViewData.Side1_UniqueIds'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "umb_final = pd.concat([umb_otm,umb_mtm,umb_oto1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_final = umb_final.reset_index()\n",
    "umb_final = umb_final.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_final.to_csv('umb lombard 249 v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276800, 19)"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob_stage_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = ob_stage_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4786, 19)"
      ]
     },
     "execution_count": 913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_for_comment = pd.read_csv('Ob for comment daily lombard 249.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_for_comment = ob_for_comment.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_for_comment_model = pd.concat([ob_for_comment,k], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_for_comment_model = ob_for_comment_model.reset_index()\n",
    "ob_for_comment_model = ob_for_comment_model.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_for_comment_model.to_csv('Ob for comment daily lombard 249 v1 final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we take all OBs to single side model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from dateutil.parser import parse\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = ob_for_comment_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('Ob for comment daily lombard 249 v1 final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ViewData.Side0_UniqueIds', 'ViewData.Side1_UniqueIds',\n",
       "       'ViewData.Mapped Custodian Account', 'ViewData.Fund',\n",
       "       'ViewData.Task Business Date', 'ViewData.Currency',\n",
       "       'ViewData.Asset Type Category', 'ViewData.Transaction Type',\n",
       "       'ViewData.Investment Type', 'ViewData.Prime Broker', 'ViewData.Ticker',\n",
       "       'ViewData.Sec Fees', 'ViewData.Settle Date', 'ViewData.Trade Date',\n",
       "       'ViewData.Description', 'len_0', 'len_1', 'MTM_mark', 'final_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = pd.read_excel('Mapping variables for variable cleaning.xlsx', sheet_name='General')\n",
    "def make_dict(row):\n",
    "    keys_l = str(row['Keys']).lower()\n",
    "    keys_s = keys_l.split(', ')\n",
    "    keys = tuple(keys_s)\n",
    "    return keys\n",
    "df['tuple'] = df.apply(make_dict, axis=1)\n",
    "clean_map_dict = df.set_index('tuple')['Value'].to_dict()\n",
    "\n",
    "df3['ViewData.Transaction Type'] = df3['ViewData.Transaction Type'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df3['ViewData.Asset Type Category'] = df3['ViewData.Asset Type Category'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df3['ViewData.Investment Type'] = df3['ViewData.Investment Type'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df3['ViewData.Prime Broker'] = df3['ViewData.Prime Broker'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "\n",
    "def clean_mapping(item):\n",
    "    item1 = item.split()\n",
    "    \n",
    "    \n",
    "    ttype = []\n",
    "    \n",
    "    \n",
    "    for x in item1:\n",
    "        ttype1 = []\n",
    "        for key, value in clean_map_dict.items():\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "            if x in key:\n",
    "                a = value\n",
    "                ttype1.append(a)\n",
    "           \n",
    "        if len(ttype1)==0:\n",
    "            ttype1.append(x)\n",
    "        ttype = ttype + ttype1\n",
    "        \n",
    "    return ' '.join(ttype)\n",
    "\n",
    "df3['ViewData.Transaction Type1'] = df3['ViewData.Transaction Type'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df3['ViewData.Asset Type Category1'] = df3['ViewData.Asset Type Category'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df3['ViewData.Investment Type1'] = df3['ViewData.Investment Type'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df3['ViewData.Prime Broker1'] = df3['ViewData.Prime Broker'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "\n",
    "def is_num(item):\n",
    "    try:\n",
    "        float(item)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_date_format(item):\n",
    "    try:\n",
    "        parse(item, fuzzy=False)\n",
    "        return True\n",
    "    \n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def date_edge_cases(item):\n",
    "    if len(item) == 5 and item[2] =='/' and is_num(item[:2]) and is_num(item[3:]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def comb_clean(x):\n",
    "    k = []\n",
    "    for item in x.split():\n",
    "        if ((is_num(item)==False) and (is_date_format(item)==False) and (date_edge_cases(item)==False)):\n",
    "            k.append(item)\n",
    "    return ' '.join(k)\n",
    "\n",
    "df3['ViewData.Transaction Type1'] = df3['ViewData.Transaction Type1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "df3['ViewData.Asset Type Category1'] = df3['ViewData.Asset Type Category1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "df3['ViewData.Investment Type1'] = df3['ViewData.Investment Type1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "df3['ViewData.Prime Broker1'] = df3['ViewData.Prime Broker1'].apply(lambda x : comb_clean(x) if type(x)==str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "com = pd.read_csv('desc cat with naveen oaktree.csv')\n",
    "cat_list = list(set(com['Pairing']))\n",
    "\n",
    "def descclean(com,cat_list):\n",
    "    cat_all1 = []\n",
    "    list1 = cat_list\n",
    "    m = 0\n",
    "    if (type(com) == str):\n",
    "        com = com.lower()\n",
    "        com1 =  re.split(\"[,/. \\-!?:]+\", com)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for item in list1:\n",
    "            if (type(item) == str):\n",
    "                item = item.lower()\n",
    "                item1 = item.split(' ')\n",
    "                lst3 = [value for value in item1 if value in com1] \n",
    "                if len(lst3) == len(item1):\n",
    "                    cat_all1.append(item)\n",
    "                    m = m+1\n",
    "            \n",
    "                else:\n",
    "                    m = m\n",
    "            else:\n",
    "                    m = 0\n",
    "    else:\n",
    "        m = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    if m >0 :\n",
    "        return list(set(cat_all1))\n",
    "    else:\n",
    "        if ((type(com)==str)):\n",
    "            if (len(com1)<4):\n",
    "                if ((len(com1)==1) & com1[0].startswith('20')== True):\n",
    "                    return 'swap id'\n",
    "                else:\n",
    "                    return com\n",
    "            else:\n",
    "                return 'NA'\n",
    "        else:\n",
    "            return 'NA'\n",
    "\n",
    "df3['desc_cat'] = df3['ViewData.Description'].apply(lambda x : descclean(x,cat_list))\n",
    "\n",
    "def currcln(x):\n",
    "    if (type(x)==list):\n",
    "        return x\n",
    "      \n",
    "    else:\n",
    "       \n",
    "        \n",
    "        if x == 'NA':\n",
    "            return \"NA\"\n",
    "        elif (('dollar' in x) | ('dollars' in x )):\n",
    "            return 'dollar'\n",
    "        elif (('pound' in x) | ('pounds' in x)):\n",
    "            return 'pound'\n",
    "        elif ('yen' in x):\n",
    "            return 'yen'\n",
    "        elif ('euro' in x) :\n",
    "            return 'euro'\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['desc_cat'] = df3['desc_cat'].apply(lambda x : currcln(x))\n",
    "com = com.drop(['var','Catogery'], axis = 1)\n",
    "com['Pairing'] = com['Pairing'].apply(lambda x : x.lower())\n",
    "com['replace'] = com['replace'].apply(lambda x : x.lower())\n",
    "\n",
    "def catcln1(cat,df):\n",
    "    ret = []\n",
    "    if (type(cat)==list):\n",
    "        \n",
    "        if 'equity swap settlement' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'equity swap' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'swap settlement' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'swap unwind' in cat:\n",
    "            ret.append('swap unwind')\n",
    "        #return 'swap unwind'\n",
    "   \n",
    "    \n",
    "        else:\n",
    "        \n",
    "       \n",
    "            for item in cat:\n",
    "            \n",
    "                a = df[df['Pairing']==item]['replace'].values[0]\n",
    "                if a not in ret:\n",
    "                    ret.append(a)\n",
    "        return list(set(ret))\n",
    "      \n",
    "    else:\n",
    "        return cat\n",
    "    \n",
    "df3['new_desc_cat'] = df3['desc_cat'].apply(lambda x : catcln1(x,com))\n",
    "\n",
    "comp = ['inc','stk','corp ','llc','pvt','plc']\n",
    "df3['new_desc_cat'] = df3['new_desc_cat'].apply(lambda x : 'Company' if x in comp else x)\n",
    "def desccat(x):\n",
    "    if isinstance(x, list):\n",
    "        \n",
    "        if 'equity swap settlement' in x:\n",
    "            return 'swap settlement'\n",
    "        elif 'collateral transfer' in x:\n",
    "            return 'collateral transfer'\n",
    "        elif 'dividend' in x:\n",
    "            return 'dividend'\n",
    "        elif (('loan' in x) & ('option' in x)):\n",
    "            return 'option loan'\n",
    "        \n",
    "        elif (('interest' in x) & ('corp' in x) ):\n",
    "            return 'corp loan'\n",
    "        elif (('interest' in x) & ('loan' in x) ):\n",
    "            return 'interest'\n",
    "        else:\n",
    "            return x[0]\n",
    "    else:\n",
    "        if x == 'db_int':\n",
    "            return 'interest'\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "df3['new_desc_cat'] = df3['new_desc_cat'].apply(lambda x : desccat(x))\n",
    "\n",
    "df3['new_pb'] = df3['ViewData.Mapped Custodian Account'].apply(lambda x : x.split('_')[0] if type(x)==str else x)\n",
    "new_pb_mapping = {'GSIL':'GS','CITIGM':'CITI','JPMNA':'JPM'}\n",
    "def new_pf_mapping(x):\n",
    "    if x=='GSIL':\n",
    "        return 'GS'\n",
    "    elif x == 'CITIGM':\n",
    "        return 'CITI'\n",
    "    elif x == 'JPMNA':\n",
    "        return 'JPM'\n",
    "    else:\n",
    "        return x\n",
    "df3['new_pb'] = df3['new_pb'].apply(lambda x : new_pf_mapping(x))\n",
    "df3['ViewData.Prime Broker1'] = df3['ViewData.Prime Broker1'].fillna('kkk')\n",
    "df3['new_pb1'] = df3.apply(lambda x : x['new_pb'] if x['ViewData.Prime Broker1']=='kkk' else x['ViewData.Prime Broker1'],axis = 1)\n",
    "df3['new_pb1'] = df3['new_pb1'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['ViewData.Settle Date'] = pd.to_datetime(df3['ViewData.Settle Date'])\n",
    "days = [1,30,31,29]\n",
    "df3['monthend marker'] = df3['ViewData.Settle Date'].apply(lambda x : 1 if x.day in days else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['comm_marker'] = 'zero'\n",
    "df3['new_pb2'] = df3.apply(lambda x : 'Geneva' if x['ViewData.Side0_UniqueIds'] != 'AA' else x['new_pb1'], axis = 1)\n",
    "df3['new_pb2'] = df3['new_pb2'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ViewData.Side0_UniqueIds', 'ViewData.Side1_UniqueIds',\n",
       "       'ViewData.Mapped Custodian Account', 'ViewData.Fund',\n",
       "       'ViewData.Task Business Date', 'ViewData.Currency',\n",
       "       'ViewData.Asset Type Category', 'ViewData.Transaction Type',\n",
       "       'ViewData.Investment Type', 'ViewData.Prime Broker', 'ViewData.Ticker',\n",
       "       'ViewData.Sec Fees', 'ViewData.Settle Date', 'ViewData.Trade Date',\n",
       "       'ViewData.Description', 'len_0', 'len_1', 'MTM_mark', 'final_id',\n",
       "       'ViewData.Transaction Type1', 'ViewData.Asset Type Category1',\n",
       "       'ViewData.Investment Type1', 'ViewData.Prime Broker1', 'desc_cat',\n",
       "       'new_desc_cat', 'new_pb', 'new_pb1', 'comm_marker', 'new_pb2',\n",
       "       'monthend marker'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ViewData.Transaction Type1',\n",
    " 'ViewData.Asset Type Category1',\n",
    " 'ViewData.Investment Type1',\n",
    " 'new_desc_cat',\n",
    " 'new_pb2',\n",
    " 'new_pb1',\n",
    " 'comm_marker',\n",
    " 'monthend marker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df4['ViewData.Transaction Type1'] = df4['ViewData.Transaction Type1'].fillna('aa')\n",
    "df4['ViewData.Asset Type Category1'] = df4['ViewData.Asset Type Category1'].fillna('aa')\n",
    "df4['ViewData.Investment Type1'] = df4['ViewData.Investment Type1'].fillna('aa')\n",
    "df4['new_desc_cat'] = df4['new_desc_cat'].fillna('aa')\n",
    "df4['new_pb2'] = df4['new_pb2'].fillna('aa')\n",
    "df4['new_pb1'] = df4['new_pb1'].fillna('aa')\n",
    "df4['comm_marker'] = df4['comm_marker'].fillna('aa')\n",
    "df4['monthend marker'] = df4['monthend marker'].fillna('aa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model_lombard_249_v1.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViewData.Transaction Type1       41496\n",
       "ViewData.Asset Type Category1    41496\n",
       "ViewData.Investment Type1        41496\n",
       "new_desc_cat                     41496\n",
       "new_pb2                          41496\n",
       "new_pb1                          41496\n",
       "comm_marker                      41496\n",
       "monthend marker                  41496\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_predictions = clf.predict(df4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = []\n",
    "for item in cb_predictions:\n",
    "    demo.append(item[0])\n",
    "df3['predicted category'] = pd.Series(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df3['predicted category'] .value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.rename(columns = {'index':'Category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['template'] = 'to book' + ' ' + temp['Category'] + ' ' + \"on sd \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.drop('predicted category', axis =1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('lobard 249 comment template for delivery.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('lombard 1st comment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ViewData.Side0_UniqueIds', 'ViewData.Side1_UniqueIds',\n",
       "       'ViewData.Mapped Custodian Account', 'ViewData.Fund',\n",
       "       'ViewData.Task Business Date', 'ViewData.Currency',\n",
       "       'ViewData.Asset Type Category', 'ViewData.Transaction Type',\n",
       "       'ViewData.Investment Type', 'ViewData.Prime Broker', 'ViewData.Ticker',\n",
       "       'ViewData.Sec Fees', 'ViewData.Settle Date', 'ViewData.Trade Date',\n",
       "       'ViewData.Description', 'len_0', 'len_1', 'MTM_mark', 'final_id',\n",
       "       'ViewData.Transaction Type1', 'ViewData.Asset Type Category1',\n",
       "       'ViewData.Investment Type1', 'ViewData.Prime Broker1', 'desc_cat',\n",
       "       'new_desc_cat', 'new_pb', 'new_pb1', 'comm_marker', 'new_pb2',\n",
       "       'monthend marker', 'predicted category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_temp = pd.read_csv('lobard 249 comment template for delivery.csv')\n",
    "com_temp = com_temp.rename(columns = {'Category':'predicted category','template':'predicted template'})\n",
    "result_non_trade = df3.copy()\n",
    "result_non_trade = pd.merge(result_non_trade,com_temp,on = 'predicted category',how = 'left')\n",
    "def comgen(x,y,z,k):\n",
    "    if x == 'Geneva':\n",
    "        \n",
    "        com = k + ' ' +y + ' ' + str(z)\n",
    "    else:\n",
    "        com = \"Geneva\" + ' ' +y + ' ' + str(z)\n",
    "        \n",
    "    return com\n",
    "\n",
    "result_non_trade['predicted comment'] = result_non_trade.apply(lambda x : comgen(x['new_pb2'],x['predicted template'],x['ViewData.Settle Date'],x['new_pb1']), axis = 1)\n",
    "result_non_trade = result_non_trade[['ViewData.Side0_UniqueIds','ViewData.Side0_UniqueIds','predicted category','predicted comment']]\n",
    "result_non_trade.to_csv('Comment file for lombard 249.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni3.drop(['zero_list', 'diff_len', 'remove_mark', 'sel_mark'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy['remove_mark'] = dummy.apply(lambda x :remove_mark(x['zero_list_len'] ,x['diff_len'], x['zero_list_sum']),axis = 1)\n",
    "# dummy = dummy[['ViewData.Task Business Date','Custodian Account', 'Currency', 'zero_list',  'diff_len', 'remove_mark','sel_mark']]\n",
    "df3 = pd.merge(uni3, dummy, on = ['ViewData.Task Business Date','Custodian Account','Currency'], how = 'left')\n",
    "df4 = df3[(df3['sel_mark']==1) & (df3['sel_mark']==1)]\n",
    "df4['Predicted Category'] = 'Match'\n",
    "df4['Predicted Comment'] = 'Match'\n",
    "df5 = df3[~((df3['sel_mark']==1) & (df3['sel_mark']==1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['remove_mark'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning of description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni3['ViewData.Description'] = uni3['ViewData.Description'].apply(lambda x : x.lower() if type(x)==str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar = ['u.s. dollars','us dollar','']\n",
    "pound = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = uni3.groupby(['ViewData.Task Business Date','Custodian Account','Currency','Ticker1'])['Net Amount Difference1'].apply(list).reset_index()\n",
    "dummy1 = uni3.groupby(['ViewData.Task Business Date','Custodian Account','Currency','Ticker1'])['ViewData.Side0_UniqueIds'].count().reset_index()\n",
    "dummy = pd.merge(dummy, dummy1 , on = ['ViewData.Task Business Date','Custodian Account','Currency','Ticker1'], how = 'left')\n",
    "dummy2 = uni3.groupby(['ViewData.Task Business Date','Custodian Account','Currency','Ticker1'])['ViewData.Side1_UniqueIds'].count().reset_index()\n",
    "dummy = pd.merge(dummy, dummy2 , on = ['ViewData.Task Business Date','Custodian Account','Currency','Ticker1'], how = 'left')\n",
    "dummy['sel_mark'] = dummy.apply(lambda x : 0 if ((x['ViewData.Side0_UniqueIds']==0) | (x['ViewData.Side1_UniqueIds']==0)) else 1, axis =1 )\n",
    "dummy = dummy[dummy['sel_mark']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleannig of the 4 variables in this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Mapping variables for variable cleaning.xlsx', sheet_name='General')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(row):\n",
    "    keys_l = str(row['Keys']).lower()\n",
    "    keys_s = keys_l.split(', ')\n",
    "    keys = tuple(keys_s)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tuple'] = df.apply(make_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_map_dict = df.set_index('tuple')['Value'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ViewData.Transaction Type'] = df2['ViewData.Transaction Type'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df2['ViewData.Asset Type Category'] = df2['ViewData.Asset Type Category'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df2['ViewData.Investment Type'] = df2['ViewData.Investment Type'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df2['ViewData.Prime Broker'] = df2['ViewData.Prime Broker'].apply(lambda x : x.lower() if type(x)==str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mapping(item):\n",
    "    item1 = item.split()\n",
    "    \n",
    "    \n",
    "    ttype = []\n",
    "    \n",
    "    \n",
    "    for x in item1:\n",
    "        ttype1 = []\n",
    "        for key, value in clean_map_dict.items():\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "            if x in key:\n",
    "                a = value\n",
    "                ttype1.append(a)\n",
    "           \n",
    "        if len(ttype1)==0:\n",
    "            ttype1.append(x)\n",
    "        ttype = ttype + ttype1\n",
    "        \n",
    "    return ' '.join(ttype)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ViewData.Transaction Type1'] = df2['ViewData.Transaction Type'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df2['ViewData.Asset Type Category1'] = df2['ViewData.Asset Type Category'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df2['ViewData.Investment Type1'] = df2['ViewData.Investment Type'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df2['ViewData.Prime Broker1'] = df2['ViewData.Prime Broker'].apply(lambda x : clean_mapping(x) if type(x)==str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_num(item):\n",
    "    try:\n",
    "        float(item)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_date_format(item):\n",
    "    try:\n",
    "        parse(item, fuzzy=False)\n",
    "        return True\n",
    "    \n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def date_edge_cases(item):\n",
    "    if len(item) == 5 and item[2] =='/' and is_num(item[:2]) and is_num(item[3:]):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "from dateutil.parser import parse\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_clean(x):\n",
    "    k = []\n",
    "    for item in x.split():\n",
    "        if ((is_num(item)==False) and (is_date_format(item)==False) and (date_edge_cases(item)==False)):\n",
    "            k.append(item)\n",
    "    return ' '.join(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ViewData.Transaction Type1'] = df2['ViewData.Transaction Type1'].apply(lambda x : comb_clean(x) if type(x)==str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ViewData.Asset Type Category1'] = df2['ViewData.Asset Type Category1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "df2['ViewData.Investment Type1'] = df2['ViewData.Investment Type1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "df2['ViewData.Prime Broker1'] = df2['ViewData.Prime Broker1'].apply(lambda x : comb_clean(x) if type(x)==str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ViewData.Transaction Type1'] = df2['ViewData.Transaction Type1'].apply(lambda x : 'paydown' if x=='pay down' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divclient(x):\n",
    "    if (type(x) == str):\n",
    "        if ('eqswap div client tax' in x) :\n",
    "            return 'eqswap div client tax'\n",
    "        else:\n",
    "            return x\n",
    "    else:\n",
    "        return 'float'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mhreplace(item):\n",
    "    item1 = item.split()\n",
    "    for items in item1:\n",
    "        if items.endswith('mh')==True:\n",
    "            item1.remove(items)\n",
    "    return ' '.join(item1).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compname(x):\n",
    "    m = 0\n",
    "    comp = ['corporate','stk','inc','lp','plc','inc.','inc','corp']\n",
    "    if type(x)==str:\n",
    "        x1 = x.split()\n",
    "        for item in x1:\n",
    "            if item in comp:\n",
    "                m = m+1\n",
    "    else:\n",
    "        m = 0\n",
    "    \n",
    "    if m ==0:\n",
    "        return x\n",
    "    else:\n",
    "        return 'Company'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wht(x):\n",
    "    if type(x)==str:\n",
    "        x1 = x.split()\n",
    "        if x1[0] =='30%':\n",
    "            return 'Wht'\n",
    "        else:\n",
    "            return x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ViewData.Transaction Type1'] = df2['ViewData.Transaction Type1'].apply(lambda x : divclient(x))\n",
    "df2['ViewData.Transaction Type1'] = df2['ViewData.Transaction Type1'].apply(lambda x : mhreplace(x))\n",
    "df2['ViewData.Transaction Type1'] = df2['ViewData.Transaction Type1'].apply(lambda x : compname(x))\n",
    "df2['ViewData.Transaction Type1'] = df2['ViewData.Transaction Type1'].apply(lambda x : divclient(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ViewData.Transaction Type1'] = df3['ViewData.Transaction Type1'].apply( lambda x : item[:2] if '30%' in x else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = pd.read_csv('desc cat with naveen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = list(set(com['Pairing']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def descclean(com,cat_list):\n",
    "    cat_all1 = []\n",
    "    list1 = cat_list\n",
    "    m = 0\n",
    "    if (type(com) == str):\n",
    "        com = com.lower()\n",
    "        com1 =  re.split(\"[,/. \\-!?:]+\", com)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for item in list1:\n",
    "            if (type(item) == str):\n",
    "                item = item.lower()\n",
    "                item1 = item.split(' ')\n",
    "                lst3 = [value for value in item1 if value in com1] \n",
    "                if len(lst3) == len(item1):\n",
    "                    cat_all1.append(item)\n",
    "                    m = m+1\n",
    "            \n",
    "                else:\n",
    "                    m = m\n",
    "            else:\n",
    "                    m = 0\n",
    "    else:\n",
    "        m = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    if m >0 :\n",
    "        return list(set(cat_all1))\n",
    "    else:\n",
    "        if ((type(com)==str)):\n",
    "            if (len(com1)<4):\n",
    "                if ((len(com1)==1) & com1[0].startswith('20')== True):\n",
    "                    return 'swap id'\n",
    "                else:\n",
    "                    return com\n",
    "            else:\n",
    "                return 'NA'\n",
    "        else:\n",
    "            return 'NA'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['desc_cat'] = df2['ViewData.Description'].apply(lambda x : descclean(x,cat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def currcln(x):\n",
    "    if (type(x)==list):\n",
    "        return x\n",
    "      \n",
    "    else:\n",
    "       \n",
    "        \n",
    "        if x == 'NA':\n",
    "            return \"NA\"\n",
    "        elif (('dollar' in x) | ('dollars' in x )):\n",
    "            return 'dollar'\n",
    "        elif (('pound' in x) | ('pounds' in x)):\n",
    "            return 'pound'\n",
    "        elif ('yen' in x):\n",
    "            return 'yen'\n",
    "        elif ('euro' in x) :\n",
    "            return 'euro'\n",
    "        else:\n",
    "            return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['desc_cat'] = df2['desc_cat'].apply(lambda x : currcln(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = com.drop(['var','Catogery'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = com.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com['Pairing'] = com['Pairing'].apply(lambda x : x.lower())\n",
    "com['replace'] = com['replace'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catcln1(cat,df):\n",
    "    ret = []\n",
    "    if (type(cat)==list):\n",
    "        \n",
    "        if 'equity swap settlement' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'equity swap' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'swap settlement' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'swap unwind' in cat:\n",
    "            ret.append('swap unwind')\n",
    "        #return 'swap unwind'\n",
    "   \n",
    "    \n",
    "        else:\n",
    "        \n",
    "       \n",
    "            for item in cat:\n",
    "            \n",
    "                a = df[df['Pairing']==item]['replace'].values[0]\n",
    "                if a not in ret:\n",
    "                    ret.append(a)\n",
    "        return list(set(ret))\n",
    "      \n",
    "    else:\n",
    "        return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['new_desc_cat'] = df2['desc_cat'].apply(lambda x : catcln1(x,com))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = ['inc','stk','corp ','llc','pvt','plc']\n",
    "df2['new_desc_cat'] = df2['new_desc_cat'].apply(lambda x : 'Company' if x in comp else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desccat(x):\n",
    "    if isinstance(x, list):\n",
    "        \n",
    "        if 'equity swap settlement' in x:\n",
    "            return 'swap settlement'\n",
    "        elif 'collateral transfer' in x:\n",
    "            return 'collateral transfer'\n",
    "        elif 'dividend' in x:\n",
    "            return 'dividend'\n",
    "        elif (('loan' in x) & ('option' in x)):\n",
    "            return 'option loan'\n",
    "        \n",
    "        elif (('interest' in x) & ('corp' in x) ):\n",
    "            return 'corp loan'\n",
    "        elif (('interest' in x) & ('loan' in x) ):\n",
    "            return 'interest'\n",
    "        else:\n",
    "            return x[0]\n",
    "    else:\n",
    "        if x == 'db_int':\n",
    "            return 'interest'\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['new_desc_cat'] = df2['new_desc_cat'].apply(lambda x : desccat(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prime Broker Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['new_pb'] = df2['ViewData.Mapped Custodian Account'].apply(lambda x : x.split('_')[0] if type(x)==str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pb_mapping = {'GSIL':'GS','CITIGM':'CITI','JPMNA':'JPM'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pf_mapping(x):\n",
    "    if x=='GSIL':\n",
    "        return 'GS'\n",
    "    elif x == 'CITIGM':\n",
    "        return 'CITI'\n",
    "    elif x == 'JPMNA':\n",
    "        return 'JPM'\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['new_pb'] = df2['new_pb'].apply(lambda x : new_pf_mapping(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['ViewData.Prime Broker1'] = df2['ViewData.Prime Broker1'].fillna('kkk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['new_pb1'] = df2.apply(lambda x : x['new_pb'] if x['ViewData.Prime Broker1']=='kkk' else x['ViewData.Prime Broker1'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['new_pb1'] = df2['new_pb1'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cancelled Trade Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trade_types = ['buy','sell','cover short', 'sell short', 'forward', 'forwardfx', 'spotfx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfkk = df2[df2['ViewData.Transaction Type1'].isin(trade_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfk_nontrade = df2[~df2['ViewData.Transaction Type1'].isin(trade_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dffk2 = dfkk[dfkk['ViewData.Side0_UniqueIds']=='AA']\n",
    "#dffk3 = dfkk[dfkk['ViewData.Side1_UniqueIds']=='BB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dffk4 = dfk_nontrade[dfk_nontrade['ViewData.Side0_UniqueIds']=='AA']\n",
    "#dffk5 = dfk_nontrade[dfk_nontrade['ViewData.Side1_UniqueIds']=='BB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geneva side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canceltrade(x,y):\n",
    "    if x =='buy' and y>0:\n",
    "        k = 1\n",
    "    elif x =='sell' and y<0:\n",
    "        k = 1\n",
    "    else:\n",
    "        k = 0\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dffk3['cancel_marker'] = dffk3.apply(lambda x : canceltrade(x['ViewData.Transaction Type1'],x['ViewData.Accounting Net Amount']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancelcomment(x,y):\n",
    "    com1 = 'This is original of cancelled trade with tran id'\n",
    "    com2 = 'on settle date'\n",
    "    com = com1 + ' ' +  str(x) + ' ' + com2 + str(y)\n",
    "    return com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancelcomment1(x,y):\n",
    "    com1 = 'This is cancelled trade with tran id'\n",
    "    com2 = 'on settle date'\n",
    "    com = com1 + ' ' +  str(x) + ' ' + com2 + str(y)\n",
    "    return com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dffk3[dffk3['cancel_marker'] == 1].shape[0]!=0:\n",
    "#     cancel_trade = list(set(dffk3[dffk3['cancel_marker'] == 1]['ViewData.Transaction ID']))\n",
    "#     if len(cancel_trade)>0:\n",
    "#         km = dffk3[dffk3['cancel_marker'] != 1]\n",
    "#         original = km[km['ViewData.Transaction ID'].isin(cancel_trade)]\n",
    "#         original['predicted category'] = 'Original of Cancelled trade'\n",
    "#         original['predicted comment'] = original.apply(lambda x : cancelcomment(x['ViewData.Transaction ID'],x['ViewData.Settle Date1']), axis = 1)\n",
    "#         cancellation = dffk3[dffk3['cancel_marker'] == 1]\n",
    "#         cancellation['predicted category'] = 'Cancelled trade'\n",
    "#         cancellation['predicted comment'] =  cancellation.apply(lambda x : cancelcomment1(x['ViewData.Transaction ID'],x['ViewData.Settle Date1']), axis = 1)\n",
    "#         cancel_fin = pd.concat([original,cancellation])\n",
    "#         sel_col_1 = ['final_ID','ViewData.Side0_UniqueIds','ViewData.Side0_UniqueIds','predicted category','predicted comment']\n",
    "#         cancel_fin = cancel_fin[sel_col_1]\n",
    "#         cancel_fin.to_csv('Comment file soros 2 sep testing p1.csv')\n",
    "#         dffk3 = dffk3[~dffk3['ViewData.Transaction ID'].isin(cancel_trade)]\n",
    "        \n",
    "#     else:\n",
    "#         cancellation = dffk3[dffk3['cancel_marker'] == 1]\n",
    "#         cancellation['predicted category'] = 'Cancelled trade'\n",
    "#         cancellation['predicted comment'] =  cancellation.apply(lambda x : cancelcomment1(x['ViewData.Transaction ID'],x['ViewData.Settle Date1']), axis = 1)\n",
    "#         cancel_fin = pd.concat([original,cancellation])\n",
    "#         sel_col_1 = ['final_ID','ViewData.Side0_UniqueIds','ViewData.Side0_UniqueIds','predicted category','predicted comment']\n",
    "#         cancel_fin = cancel_fin[sel_col_1]\n",
    "#         cancel_fin.to_csv('Comment file soros 2 sep testing no original p2.csv')\n",
    "#         dffk3 = dffk3[~dffk3['ViewData.Transaction ID'].isin(cancel_trade)]\n",
    "# else:\n",
    "#     dffk3 = dffk3.copy()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broker side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dffk2['cancel_marker'] = dffk2.apply(lambda x : canceltrade(x['ViewData.Transaction Type1'],x['ViewData.Cust Net Amount']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amountelim(row):\n",
    "   \n",
    "   \n",
    "   \n",
    "    if (row['SideA.ViewData.Mapped Custodian Account'] == row['SideB.ViewData.Mapped Custodian Account']):\n",
    "        a = 1\n",
    "    else:\n",
    "        a = 0\n",
    "        \n",
    "    if ((row['SideB.ViewData.Cust Net Amount']) == -(row['SideA.ViewData.Cust Net Amount'])):\n",
    "        b = 1\n",
    "    else:\n",
    "        b = 0\n",
    "    \n",
    "    if (row['SideA.ViewData.Fund'] == row['SideB.ViewData.Fund']):\n",
    "        c = 1\n",
    "    else:\n",
    "        c = 0\n",
    "        \n",
    "    if (row['SideA.ViewData.Currency'] == row['SideB.ViewData.Currency']):\n",
    "        d = 1\n",
    "    else:\n",
    "        d = 0\n",
    "    \n",
    "    if (row['SideA.ViewData.Settle Date1'] == row['SideB.ViewData.Settle Date1']):\n",
    "        e = 1\n",
    "    else:\n",
    "        e = 0\n",
    "        \n",
    "    if (row['SideA.ViewData.Transaction Type1'] == row['SideB.ViewData.Transaction Type1']):\n",
    "        f = 1\n",
    "    else:\n",
    "        f = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.Quantity'] == row['SideA.ViewData.Quantity']):\n",
    "        g = 1\n",
    "    else:\n",
    "        g = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.ISIN'] == row['SideA.ViewData.ISIN']):\n",
    "        h = 1\n",
    "    else:\n",
    "        h = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.CUSIP'] == row['SideA.ViewData.CUSIP']):\n",
    "        i = 1\n",
    "    else:\n",
    "        i = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.Ticker'] == row['SideA.ViewData.Ticker']):\n",
    "        j = 1\n",
    "    else:\n",
    "        j = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.Investment ID'] == row['SideA.ViewData.Investment ID']):\n",
    "        k = 1\n",
    "    else:\n",
    "        k = 0\n",
    "        \n",
    "    return a, b, c ,d, e,f,g,h,i,j,k\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import merge\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancelcomment2(y):\n",
    "    com1 = 'This is original of cancelled trade'\n",
    "    com2 = 'on settle date'\n",
    "    com = com1 + ' '  + com2 +' ' + str(y)\n",
    "    return com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancelcomment3(y):\n",
    "    com1 = 'This is cancelled trade'\n",
    "    com2 = 'on settle date'\n",
    "    com = com1 + ' ' + com2 + ' ' + str(y)\n",
    "    return com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dffk2[dffk2['cancel_marker'] == 1].shape[0]!=0:\n",
    "#     dummy1 = dffk2[dffk2['cancel_marker']!=1]\n",
    "#     dummy1 = dffk2[dffk2['cancel_marker']==1]\n",
    "\n",
    "\n",
    "#     pool =[]\n",
    "#     key_index =[]\n",
    "#     training_df =[]\n",
    "#     call1 = []\n",
    "\n",
    "#     appended_data = []\n",
    "\n",
    "#     no_pair_ids = []\n",
    "# #max_rows = 5\n",
    "\n",
    "#     k = list(set(list(set(dummy['ViewData.Task Business Date1']))))\n",
    "#     k1 = k\n",
    "\n",
    "#     for d in tqdm(k1):\n",
    "#         aa1 = dummy[dummy['ViewData.Task Business Date1']==d]\n",
    "#         bb1 = dummy1[dummy1['ViewData.Task Business Date1']==d]\n",
    "#         aa1['marker'] = 1\n",
    "#         bb1['marker'] = 1\n",
    "    \n",
    "#         aa1 = aa1.reset_index()\n",
    "#         aa1 = aa1.drop('index',1)\n",
    "#         bb1 = bb1.reset_index()\n",
    "#         bb1 = bb1.drop('index', 1)\n",
    "#         #print(aa1.shape)\n",
    "#         #print(bb1.shape)\n",
    "    \n",
    "#         aa1.columns = ['SideB.' + x  for x in aa1.columns] \n",
    "#         bb1.columns = ['SideA.' + x  for x in bb1.columns]\n",
    "    \n",
    "#         cc1 = pd.merge(aa1,bb1, left_on = 'SideB.marker', right_on = 'SideA.marker', how = 'outer')\n",
    "#         appended_data.append(cc1)\n",
    "#         cancel_broker = pd.concat(appended_data)\n",
    "#         cancel_broker[['map_match','amt_match','fund_match','curr_match','sd_match','ttype_match','Qnt_match','isin_match','cusip_match','ticker_match','Invest_id']] = cancel_broker.apply(lambda row : amountelim(row), axis = 1,result_type=\"expand\")\n",
    "#         elim1 = cancel_broker[(cancel_broker['map_match']==1) & (cancel_broker['curr_match']==1)  & ((cancel_broker['isin_match']==1) |(cancel_broker['cusip_match']==1)| (cancel_broker['ticker_match']==1) | (cancel_broker['Invest_id']==1))]\n",
    "#         if elim1.shape[0]!=0:\n",
    "#             id_listA = list(set(elim1['SideA.final_ID']))\n",
    "#             c1 = dummy\n",
    "#             c2 = dummy1[dummy1['final_ID'].isin(id_listA)]\n",
    "#             c1['predicted category'] = 'Cancelled trade'\n",
    "#             c2['predicted category'] = 'Original of Cancelled trade'\n",
    "#             c1['predicted comment'] =  c1.apply(lambda x : cancelcomment2(x['ViewData.Settle Date1']))\n",
    "#             c2['predicted comment'] = c2.apply(lambda x : cancelcomment3(x['ViewData.Settle Date1']))\n",
    "#             cancel_fin = pd.concat([c1,c2])\n",
    "#             cancel_fin = cancel_fin.reset_index()\n",
    "#             cancel_fin = cancel_fin.drop('index', axis = 1)\n",
    "#             sel_col_1 = ['final_ID','ViewData.Side0_UniqueIds','ViewData.Side0_UniqueIds','predicted category','predicted comment']\n",
    "#             cancel_fin = cancel_fin[sel_col_1]\n",
    "#             cancel_fin.to_csv('Comment file soros 2 sep testing p3.csv')\n",
    "#             id_listB = list(set(c1['final_ID']))\n",
    "#             comb = id_listB + id_listA\n",
    "#             dffk2 = dffk2[~dffk2['final_ID'].isin(comb)]\n",
    "            \n",
    "            \n",
    "            \n",
    "   \n",
    "        \n",
    "#     else:\n",
    "#         c1 = dummy\n",
    "#         c1['predicted category'] = 'Cancelled trade'\n",
    "#         c1['predicted comment'] =  c1.apply(lambda x : cancelcomment2(x['ViewData.Settle Date1']))\n",
    "#         sel_col_1 = ['final_ID','ViewData.Side0_UniqueIds','ViewData.Side0_UniqueIds','predicted category','predicted comment']\n",
    "#         cancel_fin = c1[sel_col_1]\n",
    "#         cancel_fin.to_csv('Comment file soros 2 sep testing no original p4.csv')\n",
    "#         id_listB = list(set(c1['final_ID']))\n",
    "#         comb = id_listB\n",
    "#         dffk2 = dffk2[~dffk2['final_ID'].isin(comb)]\n",
    "        \n",
    "# else:\n",
    "#     dffk2 = dffk2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Pairs in Up and down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(columns = {'ViewData.B-P Net Amount':'ViewData.Cust Net Amount'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffk2 = df2[df2['ViewData.Side0_UniqueIds']=='AA']\n",
    "dffk3 = df2[df2['ViewData.Side1_UniqueIds']=='BB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_col = ['final_ID',  'ViewData.Currency',\n",
    "       'ViewData.Accounting Net Amount',\n",
    "       \n",
    "       'ViewData.Asset Type Category', \n",
    "       'ViewData.Cust Net Amount', 'ViewData.BreakID',\n",
    "       'ViewData.ClusterID',\n",
    "       'ViewData.CUSIP', 'ViewData.Description', 'ViewData.Fund',\n",
    "        'ViewData.Investment ID',\n",
    "       'ViewData.Investment Type', \n",
    "       'ViewData.ISIN', 'ViewData.Keys', \n",
    "       'ViewData.Mapped Custodian Account',  'ViewData.Prime Broker',\n",
    "       \n",
    "       'ViewData.Quantity','ViewData.Settle Date1', \n",
    "       'ViewData.Status', 'ViewData.Strategy', \n",
    "       'ViewData.Ticker', 'ViewData.Trade Date1', \n",
    "       'ViewData.Transaction Type', \n",
    "       'ViewData.Side0_UniqueIds', 'ViewData.Side1_UniqueIds', \n",
    "      'ViewData.Task Business Date1','ViewData.InternalComment2'\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffpb = dffk2[sel_col]\n",
    "dffacc = dffk3[sel_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bplist = dffpb.groupby('ViewData.Task Business Date1')['ViewData.Cust Net Amount'].apply(list).reset_index()\n",
    "acclist = dffacc.groupby('ViewData.Task Business Date1')['ViewData.Accounting Net Amount'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updlist = pd.merge(bplist, acclist, on = 'ViewData.Task Business Date1', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updlist['upd_amt'] = updlist.apply(lambda x : [value for value in x['ViewData.Cust Net Amount'] if value in x['ViewData.Accounting Net Amount']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updlist = updlist[['ViewData.Task Business Date1','upd_amt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffpb = pd.merge(dffpb, updlist, on = 'ViewData.Task Business Date1', how = 'left')\n",
    "dffacc = pd.merge(dffacc, updlist, on = 'ViewData.Task Business Date1', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffpb['upd_amt']= dffpb['upd_amt'].fillna('MMM')\n",
    "dffacc['upd_amt']= dffacc['upd_amt'].fillna('MMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updmark(y,x):\n",
    "    if x =='MMM':\n",
    "        return 0\n",
    "    else:\n",
    "        if y in x:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffpb['upd_mark'] = dffpb.apply(lambda x :  updmark(x['ViewData.Cust Net Amount'], x['upd_amt']) , axis= 1)\n",
    "dffacc['upd_mark'] = dffacc.apply(lambda x : updmark(x['ViewData.Accounting Net Amount'], x['upd_amt']) , axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff4 = dffpb[dffpb['upd_mark']==1]\n",
    "dff5 = dffacc[dffacc['upd_mark']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff6 = dffk4[sel_col]\n",
    "#dff7 = dffk5[sel_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff4 = pd.concat([dff4,dff6])\n",
    "# dff4 = dff4.reset_index()\n",
    "# dff4 = dff4.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff5 = pd.concat([dff5,dff7])\n",
    "# dff5 = dff5.reset_index()\n",
    "# dff5 = dff5.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amountelim(row):\n",
    "   \n",
    "    if (row['SideA.ViewData.Mapped Custodian Account'] == row['SideB.ViewData.Mapped Custodian Account']):\n",
    "        a = 1\n",
    "    else:\n",
    "        a = 0\n",
    "        \n",
    "    if (row['SideB.ViewData.Cust Net Amount'] == row['SideA.ViewData.Accounting Net Amount']):\n",
    "        b = 1\n",
    "    else:\n",
    "        b = 0\n",
    "    \n",
    "    if (row['SideA.ViewData.Fund'] == row['SideB.ViewData.Fund']):\n",
    "        c = 1\n",
    "    else:\n",
    "        c = 0\n",
    "        \n",
    "    if (row['SideA.ViewData.Currency'] == row['SideB.ViewData.Currency']):\n",
    "        d = 1\n",
    "    else:\n",
    "        d = 0\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    return a, b, c ,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updownat(a,b,c,d,):\n",
    "    if a == 0:\n",
    "        k = 'mapped custodian account'\n",
    "    elif b==0:\n",
    "        k = 'currency'\n",
    "    elif c ==0 :\n",
    "        k = 'Settle Date'\n",
    "    elif d == 0:\n",
    "        k = 'fund'    \n",
    "  \n",
    "    else :\n",
    "        k = 'Investment type'\n",
    "        \n",
    "    com = 'up/down at'+ ' ' + k\n",
    "    return com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M cross N code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################### loop 3 ###############################\n",
    "from pandas import merge\n",
    "from tqdm import tqdm\n",
    "if ((dff4.shape[0]!=0) & (dff5.shape[0]!=0)):\n",
    "    pool =[]\n",
    "    key_index =[]\n",
    "    training_df =[]\n",
    "    call1 = []\n",
    "\n",
    "    appended_data = []\n",
    "\n",
    "    no_pair_ids = []\n",
    "#max_rows = 5\n",
    "\n",
    "    k = list(set(list(set(dff5['ViewData.Task Business Date1'])) + list(set(dff4['ViewData.Task Business Date1']))))\n",
    "    k1 = k[0:3]\n",
    "\n",
    "    for d in tqdm(k1):\n",
    "        aa1 = dff4[dff4['ViewData.Task Business Date1']==d]\n",
    "        bb1 = dff5[dff5['ViewData.Task Business Date1']==d]\n",
    "        aa1['marker'] = 1\n",
    "        bb1['marker'] = 1\n",
    "    \n",
    "        aa1 = aa1.reset_index()\n",
    "        aa1 = aa1.drop('index',1)\n",
    "        bb1 = bb1.reset_index()\n",
    "        bb1 = bb1.drop('index', 1)\n",
    "        print(aa1.shape)\n",
    "        print(bb1.shape)\n",
    "    \n",
    "        aa1.columns = ['SideB.' + x  for x in aa1.columns] \n",
    "        bb1.columns = ['SideA.' + x  for x in bb1.columns]\n",
    "    \n",
    "        cc1 = pd.merge(aa1,bb1, left_on = 'SideB.marker', right_on = 'SideA.marker', how = 'outer')\n",
    "        appended_data.append(cc1)\n",
    "        \n",
    "    df_213_1 = pd.concat(appended_data)\n",
    "    df_213_1[['map_match','amt_match','fund_match','curr_match']] = df_213_1.apply(lambda row : amountelim(row), axis = 1,result_type=\"expand\")\n",
    "    df_213_1['key_match_sum'] = df_213_1['map_match'] + df_213_1['fund_match'] + df_213_1['curr_match']\n",
    "    elim1 = df_213_1[(df_213_1['amt_match']==1) & (df_213_1['key_match_sum']>=2)]\n",
    "#     if elim1.shape[0]!=0:\n",
    "#         elim1['SideA.predicted category'] = 'Updown'\n",
    "#         elim1['SideB.predicted category'] = 'Updown'\n",
    "#         elim1['SideA.Predicted_action'] = 'No-Pair'\n",
    "#         elim1['SideB.Predicted_action'] = 'No-Pair'\n",
    "#         elim1['SideA.predicted comment'] = elim1.apply(lambda x : updownat(x['map_match'],x['curr_match'],x['sd_match'],x['fund_match'],x['ttype_match']), axis = 1)\n",
    "#         elim1['SideB.predicted comment'] = elim1.apply(lambda x : updownat(x['map_match'],x['curr_match'],x['sd_match'],x['fund_match'],x['ttype_match']), axis = 1)\n",
    "#         elim_col = ['final_ID','ViewData.Side0_UniqueIds','ViewData.Side0_UniqueIds','predicted category','predicted comment','Predicted_action']\n",
    "    \n",
    "    \n",
    "#         elim_col = list(elim1.columns)\n",
    "\n",
    "#         for items in elim_col:\n",
    "#             item = 'SideA.'+items\n",
    "#             sideA_col.append(item)\n",
    "#             item = 'SideB.'+items\n",
    "#             sideB_col.append(item)\n",
    "        \n",
    "#         elim2 = elim1[sideA_col]\n",
    "#         elim3 = elim1[sideB_col]\n",
    "    \n",
    "#         elim2 = elim2.rename(columns= {'SideA.final_ID':'final_ID',\n",
    "#                               'SideA.predicted category':'predicted category',\n",
    "#                               'SideA.predicted comment':'predicted comment'})\n",
    "#         elim3 = elim3.rename(columns= {'SideB.final_ID':'final_ID',\n",
    "#                               'SideB.predicted category':'predicted category',\n",
    "#                               'SideB.predicted comment':'predicted comment'})\n",
    "#         frames = [elim2,elim3]\n",
    "#         elim = pd.concat(frames)\n",
    "#         elim = elim.reset_index()\n",
    "#         elim = elim.drop('index', axis = 1)\n",
    "#         elim.to_csv('Comment file soros 2 sep testing p5.csv')\n",
    "        \n",
    "#         ## TODO : Rohit to write elimination code here\n",
    "        \n",
    "#     else:\n",
    "#         aa_new = aa_new.copy()\n",
    "#         bb_new = bb_new.copy()\n",
    "# else:\n",
    "#     aa_new = aa_new.copy()\n",
    "#     bb_new = bb_new.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elim1.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elim1['SideA.ViewData.InternalComment2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start of the single Side Commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat(frames)\n",
    "# data = data.reset_index()\n",
    "# data = data.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['ViewData.Settle Date'] = pd.to_datetime(data['ViewData.Settle Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days = [1,30,31,29]\n",
    "# data['monthend marker'] = data['ViewData.Settle Date'].apply(lambda x : 1 if x.day in days else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['ViewData.Commission'] = data['ViewData.Commission'].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comfun(x):\n",
    "    if x==\"NA\":\n",
    "        k = 'NA'\n",
    "       \n",
    "    elif x == 0.0:\n",
    "        k = 'zero'\n",
    "    else:\n",
    "        k = 'positive'\n",
    "   \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['comm_marker'] = data['ViewData.Commission'].apply(lambda x : comfun(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new_pb2'] = data.apply(lambda x : 'Geneva' if x['ViewData.Side0_UniqueIds'] != 'AA' else x['new_pb1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre_final = [\n",
    "    \n",
    "'ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.BreakID',\n",
    " \n",
    "\n",
    "\n",
    " 'ViewData.Currency',\n",
    " 'ViewData.Custodian',\n",
    "     'ViewData.ISIN',\n",
    "\n",
    " 'ViewData.Mapped Custodian Account',\n",
    " \n",
    " 'ViewData.Net Amount Difference Absolute',\n",
    " \n",
    " 'ViewData.Portolio',\n",
    " 'ViewData.Settle Date',\n",
    " \n",
    " 'ViewData.Trade Date',\n",
    " 'ViewData.Transaction Type1',\n",
    "'new_desc_cat',\n",
    "    'ViewData.Department',\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " 'ViewData.Accounting Net Amount',\n",
    " 'ViewData.Asset Type Category1',\n",
    " \n",
    " \n",
    " 'ViewData.CUSIP',\n",
    " 'ViewData.Commission',\n",
    " \n",
    " 'ViewData.Fund',\n",
    " \n",
    " \n",
    " 'ViewData.Investment ID',\n",
    " 'ViewData.Investment Type1',\n",
    " \n",
    " \n",
    " 'ViewData.Price',\n",
    " 'ViewData.Prime Broker1',\n",
    "\n",
    " 'ViewData.Quantity',\n",
    " \n",
    "'ViewData.InternalComment2', 'ViewData.Description','new_pb2','new_pb1'\n",
    "   \n",
    " \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[Pre_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod1['ViewData.Custodian'] = df_mod1['ViewData.Custodian'].fillna('AA')\n",
    "df_mod1['ViewData.Portolio'] = df_mod1['ViewData.Portolio'].fillna('bb')\n",
    "df_mod1['ViewData.Settle Date'] = df_mod1['ViewData.Settle Date'].fillna(0)\n",
    "df_mod1['ViewData.Trade Date'] = df_mod1['ViewData.Trade Date'].fillna(0)\n",
    "df_mod1['ViewData.Accounting Net Amount'] = df_mod1['ViewData.Accounting Net Amount'].fillna(0)\n",
    "df_mod1['ViewData.Asset Type Category1'] = df_mod1['ViewData.Asset Type Category1'].fillna('CC')\n",
    "df_mod1['ViewData.CUSIP'] = df_mod1['ViewData.CUSIP'].fillna('DD')\n",
    "df_mod1['ViewData.Fund'] = df_mod1['ViewData.Fund'].fillna('EE')\n",
    "df_mod1['ViewData.Investment ID'] = df_mod1['ViewData.Investment ID'].fillna('FF')\n",
    "df_mod1['ViewData.Investment Type1'] = df_mod1['ViewData.Investment Type1'].fillna('GG')\n",
    "#df_mod1['ViewData.Knowledge Date'] = df_mod1['ViewData.Knowledge Date'].fillna(0)\n",
    "df_mod1['ViewData.Price'] = df_mod1['ViewData.Price'].fillna(0)\n",
    "df_mod1['ViewData.Prime Broker1'] = df_mod1['ViewData.Prime Broker1'].fillna(\"HH\")\n",
    "df_mod1['ViewData.Quantity'] = df_mod1['ViewData.Quantity'].fillna(0)\n",
    "#df_mod1['ViewData.Sec Fees'] = df_mod1['ViewData.Sec Fees'].fillna(0)\n",
    "#df_mod1['ViewData.Strike Price'] = df_mod1['ViewData.Strike Price'].fillna(0)\n",
    "df_mod1['ViewData.Commission'] = df_mod1['ViewData.Commission'].fillna(0)\n",
    "df_mod1['ViewData.Transaction Type1'] = df_mod1['ViewData.Transaction Type1'].fillna('kk')\n",
    "df_mod1['ViewData.ISIN'] = df_mod1['ViewData.ISIN'].fillna('mm')\n",
    "df_mod1['new_desc_cat'] = df_mod1['new_desc_cat'].fillna('nn')\n",
    "#df_mod1['Category'] = df_mod1['Category'].fillna('NA')\n",
    "df_mod1['ViewData.Description'] = df_mod1['ViewData.Description'].fillna('nn')\n",
    "df_mod1['ViewData.Department'] = df_mod1['ViewData.Department'].fillna('nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fid(a,b):\n",
    "   \n",
    "    if ( b=='BB'):\n",
    "        return a\n",
    "    else:\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod1['final_ID'] = df_mod1.apply(lambda row: fid(row['ViewData.Side0_UniqueIds'],row['ViewData.Side1_UniqueIds']),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = df_mod1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Prediction of the Trade and Non trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st for Non Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trade_types = ['buy','sell','cover short', 'sell short', 'forward', 'forwardfx', 'spotfx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data21 = data2[~data2['ViewData.Transaction Type1'].isin(trade_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    " \n",
    "\n",
    "\n",
    " 'ViewData.Transaction Type1',\n",
    " \n",
    " \n",
    "\n",
    " 'ViewData.Asset Type Category1',\n",
    "\n",
    " 'new_desc_cat',\n",
    "\n",
    " 'ViewData.Investment Type1',\n",
    " \n",
    "\n",
    " 'new_pb2','new_pb1'\n",
    " \n",
    " \n",
    "              \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data211 = data2[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model_weiss_catrefine_v8.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual class predictions\n",
    "cb_predictions = clf.predict(data211)#.astype(str)\n",
    "# Probabilities for each class\n",
    "#cb_probs = clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing of Model and final prediction file - Non Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = []\n",
    "for item in cb_predictions:\n",
    "    demo.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_non_trade =data2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_non_trade = result_non_trade.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_non_trade['predicted category'] = pd.Series(demo)\n",
    "result_non_trade['predicted comment'] = 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_non_trade = result_non_trade.drop('predicted comment', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#com_temp = pd.read_csv('Soros comment template for delivery.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#com_temp = com_temp.rename(columns = {'Category ':'predicted category','template':'predicted template'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_non_trade = pd.merge(result_non_trade,com_temp,on = 'predicted category',how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comgen(x,y,z,k):\n",
    "    if x == 'Geneva':\n",
    "        \n",
    "        com = k + ' ' +y + ' ' + str(z)\n",
    "    else:\n",
    "        com = \"Geneva\" + ' ' +y + ' ' + str(z)\n",
    "        \n",
    "    return com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_non_trade['predicted comment'] = result_non_trade.apply(lambda x : comgen(x['new_pb2'],x['predicted template'],x['ViewData.Settle Date'],x['new_pb1']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_non_trade = result_non_trade[['final_ID','ViewData.Side0_UniqueIds','ViewData.Side0_UniqueIds','predicted category','predicted comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_non_trade.to_csv('Comment file Weiss 2 sep testing p6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
