{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function1\n",
    "def subSum(numbers,total):\n",
    "    length = len(numbers)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if length <16:\n",
    "      \n",
    "            \n",
    "      \n",
    "        \n",
    "        for index,number in enumerate(numbers):\n",
    "            if np.isclose(number, total, atol=5.0).any():\n",
    "                return [number]\n",
    "                print(34567)\n",
    "            subset = subSum(numbers[index+1:],total-number)\n",
    "            if subset:\n",
    "                #print(12345)\n",
    "                return [number] + subset\n",
    "        return []\n",
    "    else:\n",
    "        return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amt_marker(x,y,z):\n",
    "    if type(y)==list:\n",
    "        if ((x in y) & ((z<16) & (z>=2))) :\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mark(x,z,k):\n",
    "    \n",
    "   \n",
    "    if ((x>1) & (x<16)):\n",
    "        if ((k<6.0)):\n",
    "            return 1\n",
    "#         elif ((k==0.0) & (z!=0)):\n",
    "#             return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtm(x,y):\n",
    "    if ((pd.isnull(x)==False) & (pd.isnull(y)==False)):\n",
    "        y1 = y.split(',')\n",
    "        x1 = x.split(',')\n",
    "        return pd.Series([len(x1),len(y1)], index=['len_0', 'len_1'])\n",
    "    elif ((pd.isnull(x)==False) & (pd.isnull(y)==True)):\n",
    "        x1 = x.split(',')\n",
    "        \n",
    "        return pd.Series([len(x1),0], index=['len_0', 'len_1'])\n",
    "    elif ((pd.isnull(x)==True) & (pd.isnull(y)==False)):\n",
    "        y1 = y.split(',')\n",
    "        \n",
    "        return pd.Series([0,len(y1)], index=['len_0', 'len_1'])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        \n",
    "        return pd.Series([0,0], index=['len_0', 'len_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtm_mark(x,y):\n",
    "    if ((x>1) &(y>1)):\n",
    "        return 'MTM'\n",
    "    elif((x==1) &(y==1)):\n",
    "        return 'OTO'\n",
    "    elif((x>1) &(y==1)):\n",
    "        return 'MTO'\n",
    "    elif((x==1) &(y>1)):\n",
    "        return 'OTM'\n",
    "    else:\n",
    "        return 'OB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,1,5,10,15,16,17,18,30,34,35,37,39,41,42,47,50,51,53,56,58,61,62,64,65,66,67,68,69,70,71,72,74,78,82,83,85,87,88,97,101,114,117,125,126,128,130,135,147,148,149,150,151,152,157,162,164,170,172,198,206,212,214,229,242,245,252,260,267,268,269,270,278,284,285,286,290,309,316,351,353,357,358,360,362) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "uni2 = pd.read_csv('Lombard/249/ReconDB.HST_RecData_249_01_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = ['ViewData.Task Business Date','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.Mapped Custodian Account','ViewData.BreakID','ViewData.Fund','ViewData.Task Business Date',\n",
    " 'ViewData.Currency',\n",
    " 'ViewData.Asset Type Category',\n",
    " 'ViewData.Transaction Type',\n",
    " 'ViewData.Investment Type',\n",
    " 'ViewData.Prime Broker',\n",
    " 'ViewData.Ticker',\n",
    " 'ViewData.Sec Fees',\n",
    " 'ViewData.Settle Date',\n",
    " 'ViewData.Trade Date',\n",
    " 'ViewData.Description',\n",
    " 'ViewData.CUSIP',\n",
    " 'ViewData.Call Put Indicator',\n",
    " 'ViewData.Cancel Flag',\n",
    " 'ViewData.Commission',\n",
    " 'ViewData.ISIN',\n",
    " 'ViewData.Investment ID',\n",
    " \n",
    " 'ViewData.Interest Amount',\n",
    " 'ViewData.InternalComment1',\n",
    " 'ViewData.InternalComment2',\n",
    " 'ViewData.InternalComment3',\n",
    "             \n",
    "'ViewData.Accounting Net Amount',\n",
    "'ViewData.B-P Net Amount',\n",
    "              'ViewData.Net Amount Difference','ViewData.Status'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni2 = uni2[filter_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45568, 31)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uni2['ViewData.Side1_UniqueIds'] = uni2['ViewData.Side1_UniqueIds'].fillna('BB')\n",
    "#uni2['ViewData.Side0_UniqueIds'] = uni2['ViewData.Side0_UniqueIds'].fillna('AA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uni2[['len_0','len_1']] = uni2.apply(lambda x : mtm(x['ViewData.Side0_UniqueIds'],x['ViewData.Side1_UniqueIds']),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uni2['MTM_mark'] = uni2.apply(lambda x : mtm_mark(x['len_0'],x['len_1']),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We preserve Actual copy of the file and move to make changes on copy\n",
    "uni3 = uni2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reconciliation involving single sides only\n",
    "def common_matching_engine_single(df,filters,columns_to_output, amount_column, dummy_filter,serial_num):\n",
    "    dummy = df.groupby(filters)[amount_column].apply(list).reset_index()\n",
    "    dummy1 = df.groupby(filters)['ViewData.Side0_UniqueIds'].count().reset_index()\n",
    "    dummy = pd.merge(dummy, dummy1 , on = filters, how = 'left')\n",
    "    dummy2 = df.groupby(filters)['ViewData.Side1_UniqueIds'].count().reset_index()\n",
    "    dummy = pd.merge(dummy, dummy2 , on = filters, how = 'left')\n",
    "    dummy['sel_mark'] = dummy.apply(lambda x : 1 if ((x['ViewData.Side0_UniqueIds']==0) | (x['ViewData.Side1_UniqueIds']==0)) else 0, axis =1 )\n",
    "    #print(dummy['sel_mark'].value_counts())\n",
    "    if dummy[dummy['sel_mark']==1].shape[0]!=0:\n",
    "    \n",
    "        dummy['len_amount'] = dummy[amount_column].apply(lambda x : len(x))\n",
    "    \n",
    "        dummy['zero_list'] = dummy[amount_column].apply(lambda x : subSum(x,0))\n",
    "        dummy['zero_list_len'] = dummy['zero_list'].apply(lambda x : len(x))\n",
    "\n",
    "        dummy['diff_len'] = dummy['len_amount'] - dummy['zero_list_len']\n",
    "        dummy['zero_list_sum'] = dummy['zero_list'].apply(lambda x : round(abs(sum(x)),2))\n",
    "    \n",
    "    #dummy = pd.merge(dummy, pos , on = ['Custodian Account','Currency','Ticker1'], how = 'left')\n",
    "        final_cols = filters + dummy_filter\n",
    "    \n",
    "        dummy['remove_mark'] = dummy.apply(lambda x :remove_mark(x['zero_list_len'],x['diff_len'],x['zero_list_sum']),axis = 1)\n",
    "\n",
    "        dummy = dummy[final_cols]\n",
    "        df3 = pd.merge(df, dummy, on = filters, how = 'left')\n",
    "    \n",
    "        df4 = df3[(df3['remove_mark']==1) & (df3['sel_mark']==1)]\n",
    "        #print(df4.shape)\n",
    "    \n",
    "   \n",
    "        if df4.shape[0]!=0:\n",
    "       \n",
    "            df4['predicted status'] = 'close'\n",
    "            df4['predicted action'] = 'close'\n",
    "            df4['predicted category'] = 'close'\n",
    "            df4['predicted comment'] = 'close'\n",
    "            df4 = df4[columns_to_output]\n",
    "        \n",
    "        \n",
    "            string_name = 'p'+ str(serial_num)\n",
    "            filename = 'Lombard/Lombard 249 ' + string_name + '.csv'\n",
    "            df4.to_csv(filename)\n",
    "        \n",
    "            df5 = df3[~((df3['remove_mark']==1) & (df3['sel_mark']==1))]\n",
    "            df5.drop(dummy_filter, axis = 1, inplace = True)\n",
    "            df5 = df5.reset_index()\n",
    "            df5.drop('index', axis = 1, inplace = True)\n",
    "        else:    \n",
    "            df5 = df3[~((df3['remove_mark']==1) & (df3['sel_mark']==1))]\n",
    "            df5.drop(dummy_filter, axis = 1, inplace = True)\n",
    "            df5 = df5.reset_index()\n",
    "            df5.drop('index', axis = 1, inplace = True)\n",
    "    else:\n",
    "        df5 = df.copy()\n",
    "        \n",
    "    return df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reconciliation involving both sides\n",
    "def common_matching_engine_double(df,filters,columns_to_output, amount_column, dummy_filter,serial_num):\n",
    "    dummy = df.groupby(filters)[amount_column].apply(list).reset_index()\n",
    "    dummy1 = df.groupby(filters)['ViewData.Side0_UniqueIds'].count().reset_index()\n",
    "    dummy = pd.merge(dummy, dummy1 , on = filters, how = 'left')\n",
    "    dummy2 = df.groupby(filters)['ViewData.Side1_UniqueIds'].count().reset_index()\n",
    "    dummy = pd.merge(dummy, dummy2 , on = filters, how = 'left')\n",
    "    dummy['sel_mark'] = dummy.apply(lambda x : 0 if ((x['ViewData.Side0_UniqueIds']==0) | (x['ViewData.Side1_UniqueIds']==0)) else 1, axis =1 )\n",
    "    if dummy[dummy['sel_mark']==1].shape[0]!=0:\n",
    "    \n",
    "        dummy['len_amount'] = dummy[amount_column].apply(lambda x : len(x))\n",
    "    \n",
    "        dummy['zero_list'] = dummy[amount_column].apply(lambda x : subSum(x,0))\n",
    "        dummy['zero_list_len'] = dummy['zero_list'].apply(lambda x : len(x))\n",
    "\n",
    "        dummy['diff_len'] = dummy['len_amount'] - dummy['zero_list_len']\n",
    "        dummy['zero_list_sum'] = dummy['zero_list'].apply(lambda x : round(abs(sum(x)),2))\n",
    "    \n",
    "    #dummy = pd.merge(dummy, pos , on = ['Custodian Account','Currency','Ticker1'], how = 'left')\n",
    "        final_cols = filters + dummy_filter\n",
    "    \n",
    "        dummy['remove_mark'] = dummy.apply(lambda x :remove_mark(x['zero_list_len'],x['diff_len'],x['zero_list_sum']),axis = 1)\n",
    "\n",
    "        dummy = dummy[final_cols]\n",
    "        df3 = pd.merge(df, dummy, on = filters, how = 'left')\n",
    "        #print(df3.columns)\n",
    "    \n",
    "        df4 = df3[(df3['remove_mark']==1) & (df3['sel_mark']==1)]\n",
    "    #print(df4.columns)\n",
    "    \n",
    "   \n",
    "        if df4.shape[0]!=0:\n",
    "            k1 = df4.groupby(filters)['ViewData.Side0_UniqueIds'].apply(list).reset_index()\n",
    "            k2 = df4.groupby(filters)['ViewData.Side1_UniqueIds'].apply(list).reset_index()\n",
    "            k3 = df4.groupby(filters)['ViewData.BreakID'].apply(list).reset_index()\n",
    "            k4 = df4.groupby(filters)['ViewData.Status'].apply(list).reset_index()\n",
    "            k = pd.merge(k1, k2 , on = filters, how = 'left')\n",
    "            k = pd.merge(k, k3 , on = filters, how = 'left')\n",
    "            k = pd.merge(k, k4 , on = filters, how = 'left')\n",
    "        \n",
    "            k['predicted status'] = 'pair'\n",
    "            k['predicted action'] = 'UMR'\n",
    "            k['predicted category'] = 'match'\n",
    "            k['predicted comment'] = 'match'\n",
    "            k = k[columns_to_output]\n",
    "        \n",
    "        \n",
    "            string_name = 'p'+str(serial_num)\n",
    "            filename = 'Lombard/Lombard 249 ' + string_name + '.csv'\n",
    "            k.to_csv(filename)\n",
    "        \n",
    "            df5 = df3[~((df3['remove_mark']==1) & (df3['sel_mark']==1))]\n",
    "            #print(df5.columns)\n",
    "            df5.drop(dummy_filter, axis = 1, inplace = True)\n",
    "            df5 = df5.reset_index()\n",
    "            df5.drop('index', axis = 1, inplace = True)\n",
    "        else:    \n",
    "            df5 = df3[~((df3['remove_mark']==1) & (df3['sel_mark']==1))]\n",
    "            df5.drop(dummy_filter, axis = 1, inplace = True)\n",
    "            df5 = df5.reset_index()\n",
    "            df5.drop('index', axis = 1, inplace = True)\n",
    "            print(df5.columns)\n",
    "    else:\n",
    "        df5 = df.copy()\n",
    "        \n",
    "    return df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_filter = ['remove_mark','sel_mark']\n",
    "columns_to_output = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.BreakID','ViewData.Status','predicted status','predicted action','predicted category','predicted comment']\n",
    "amount_column = 'ViewData.Net Amount Difference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description']\n",
    "serial_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "double1 = common_matching_engine_double(uni3,filters,columns_to_output, amount_column, dummy_filter,serial_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Ticker']\n",
    "serial_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "double2 = common_matching_engine_double(double1,filters,columns_to_output, amount_column, dummy_filter,serial_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ['ViewData.Mapped Custodian Account','ViewData.Currency']\n",
    "serial_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "double3 = common_matching_engine_double(double2, filters, columns_to_output, amount_column, dummy_filter,serial_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start single side reconciliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Ticker','ViewData.Description']\n",
    "serial_num = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "double4 = common_matching_engine_single(double3, filters, columns_to_output, amount_column, dummy_filter,serial_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description']\n",
    "serial_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "double5 = common_matching_engine_single(double4, filters, columns_to_output, amount_column, dummy_filter,serial_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ['ViewData.Mapped Custodian Account','ViewData.Currency']\n",
    "serial_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "double6 = common_matching_engine_single(double5, filters, columns_to_output, amount_column, dummy_filter,serial_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation filters applied. Custodian account | Currency | Description\n",
    "\n",
    "# dfk = uni3.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'])['ViewData.Net Amount Difference'].apply(list).reset_index()\n",
    "# dfk1 = uni3.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'])['len_0'].sum().reset_index()\n",
    "# dfk2 = uni3.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'])['len_1'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merge = pd.merge(dfk,dfk1, on = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'], how = 'left')\n",
    "# df_merge = pd.merge(df_merge,dfk2, on = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merge['single_sided'] = df_merge.apply(lambda x : 1 if ((x['len_0']==0) | (x['len_1']==0)) else 0, axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merge['len_amount'] = df_merge['ViewData.Net Amount Difference'].apply(lambda x : len(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We seaparte single sided and double sided reconcialiation here. 1 being single sided. We reconcile single sided first\n",
    "# df_merge1 = df_merge[df_merge['single_sided']==1]\n",
    "# df_merge2 = df_merge[df_merge['single_sided']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if df_merge1.shape[0]!=0:\n",
    "#     df_merge1['zero_list'] = df_merge1['ViewData.Net Amount Difference'].apply(lambda x : subSum(x,0))\n",
    "#     df_merge1['len_zero_list'] = df_merge1['zero_list'].apply(lambda x : len(x))\n",
    "#     df_merge1 = df_merge1.drop(['len_0','len_1','single_sided','ViewData.Net Amount Difference'], axis = 1)\n",
    "#     uni4 = pd.merge(uni3, df_merge1, on = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'], how = 'left' )\n",
    "#     uni4['amt_marker'] = uni4.apply(lambda x : amt_marker(x['ViewData.Net Amount Difference'],x['zero_list'] ,x['len_zero_list']) , axis = 1)\n",
    "\n",
    "#     if uni4[uni4['amt_marker'] != 0].shape[0]!=0:\n",
    "#         k = uni4[(uni4['amt_marker'] == 1)]\n",
    "#         k['predicted status'] = 'UCB'\n",
    "#         k['predicted action'] = 'Close'\n",
    "#         k['predicted category'] = 'close'\n",
    "#         k['predicted comment'] = 'Match'\n",
    "#         sel_col_1 = ['ViewData.BreakID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted status','predicted action','predicted category','predicted comment']\n",
    "#         k = k[sel_col_1]\n",
    "#         k.to_csv('prediction result lombard 249 P1.csv')\n",
    "#         uni5 = uni4[(uni4['amt_marker'] == 0)]\n",
    "    \n",
    "        \n",
    "#         uni5 = uni5.drop(['len_0','len_1','len_amount', 'zero_list', 'len_zero_list','amt_marker'], axis = 1)\n",
    "#         dummy = uni5.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency'])['ViewData.Net Amount Difference'].apply(list).reset_index()\n",
    "#         dummy['len_amount'] = dummy['ViewData.Net Amount Difference'].apply(lambda x : len(x))\n",
    "#         dummy['zero_list'] = dummy['ViewData.Net Amount Difference'].apply(lambda x : subSum(x,0))\n",
    "#         dummy['len_zero_list'] = dummy['zero_list'].apply(lambda x : len(x))\n",
    "#         dummy['diff_len'] = dummy['len_amount'] - dummy['len_zero_list']\n",
    "#         dummy['zero_list_sum'] = dummy['zero_list'].apply(lambda x : round(abs(sum(x)),2))\n",
    "#     #dummy = pd.merge(dummy, pos , on = ['Custodian Account','Currency','Ticker1'], how = 'left')\n",
    "#         dummy['remove_mark'] = dummy.apply(lambda x :remove_mark(x['len_zero_list'],x['diff_len'],x['zero_list_sum']),axis = 1)\n",
    "#         dummy = dummy[['ViewData.Mapped Custodian Account','ViewData.Currency',  'zero_list', 'len_zero_list', 'remove_mark']]\n",
    "#         uni4 = pd.merge(uni3, dummy, on = ['ViewData.Mapped Custodian Account','ViewData.Currency'], how = 'left')\n",
    "#         uni4['amt_marker'] = uni4.apply(lambda x : amt_marker(x['ViewData.Net Amount Difference'],x['zero_list'] ,x['len_zero_list']) , axis = 1)\n",
    "        \n",
    "        \n",
    "#         if uni4[(uni4['amt_marker'] != 0) & (uni4['remove_mark'] != 0) ].shape[0]!=0:\n",
    "#             k = uni4[(uni4['amt_marker'] != 0) & (uni4['remove_mark'] != 0)]\n",
    "#             k['predicted status'] = 'UCB'\n",
    "#             k['predicted action'] = 'Close'\n",
    "#             k['predicted category'] = 'close'\n",
    "#             k['predicted comment'] = 'Match'\n",
    "#             sel_col_1 = ['ViewData.BreakID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted status','predicted action','predicted category','predicted comment']\n",
    "#             k = k[sel_col_1]\n",
    "#             k.to_csv('prediction result lombard 249 P2.csv')\n",
    "#             uni5 = uni4[(uni4['amt_marker'] == 0)]\n",
    "#             uni5 = uni5.drop(['zero_list', 'len_zero_list', 'remove_mark','amt_marker'], axis = 1)\n",
    "#             remain_df1 = uni5.copy()\n",
    "#         else:\n",
    "#             uni5 = uni4.copy()\n",
    "#             uni5 = uni5.drop(['zero_list', 'len_zero_list', 'remove_mark','amt_marker'], axis = 1)\n",
    "#             remain_df1 = uni5.copy()\n",
    "            \n",
    "        \n",
    "    \n",
    "#     else:\n",
    "#         uni5 = uni4.copy()\n",
    "#         uni5 = uni5.drop(['len_0','len_1','len_amount', 'zero_list', 'len_zero_list','amt_marker'], axis = 1)\n",
    "#         remain_df1 = uni5.copy()\n",
    "# else:\n",
    "#     m = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remain_df1 = remain_df1.drop(['len_0','len_1', 'MTM_mark'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if df_merge2.shape[0]!=0:\n",
    "#     df_merge2['zero_list'] = df_merge2['ViewData.Net Amount Difference'].apply(lambda x : subSum(x,0))\n",
    "#     df_merge2['len_zero_list'] = df_merge2['zero_list'].apply(lambda x : len(x))\n",
    "#     df_merge2 = df_merge2.drop(['len_0','len_1','single_sided','ViewData.Net Amount Difference'], axis = 1)\n",
    "#     uni4 = pd.merge(uni3, df_merge2, on = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'], how = 'left' )\n",
    "#     uni4['amt_marker'] = uni4.apply(lambda x : amt_marker(x['ViewData.Net Amount Difference'],x['zero_list'] ,x['len_zero_list']) , axis = 1)\n",
    "\n",
    "#     if uni4[uni4['amt_marker'] != 0].shape[0]!=0:\n",
    "#         k = uni4[(uni4['amt_marker'] == 1)]\n",
    "#         k1 = k.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'])['ViewData.Side0_UniqueIds'].apply(list).reset_index()\n",
    "#         k2 = k.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'])['ViewData.Side1_UniqueIds'].apply(list).reset_index()\n",
    "#         k3 = pd.merge(k1,k2, on = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'], how = 'left' )\n",
    "        \n",
    "        \n",
    "#         k3['predicted status'] = 'UMR'\n",
    "#         k3['predicted action'] = 'UMR'\n",
    "#         k3['predicted category'] = 'match'\n",
    "#         k3['predicted comment'] = 'match'\n",
    "#         sel_col_1 = ['ViewData.BreakID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted status','predicted action','predicted category','predicted comment']\n",
    "#         k3 = k3[sel_col_1]\n",
    "#         k3.to_csv('prediction result lombard 249 P3.csv')\n",
    "#         uni5 = uni4[(uni4['amt_marker'] == 0)]\n",
    "    \n",
    "        \n",
    "#         uni5 = uni5.drop(['len_amount', 'zero_list', 'len_zero_list','amt_marker'], axis = 1)\n",
    "#         dummy = uni5.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency'])['ViewData.Net Amount Difference'].apply(list).reset_index()\n",
    "#         dummy['len_amount'] = dummy['ViewData.Net Amount Difference'].apply(lambda x : len(x))\n",
    "#         dummy['zero_list'] = dummy['ViewData.Net Amount Difference'].apply(lambda x : subSum(x,0))\n",
    "#         dummy['len_zero_list'] = dummy['zero_list'].apply(lambda x : len(x))\n",
    "#         dummy['diff_len'] = dummy['len_amount'] - dummy['len_zero_list']\n",
    "#         dummy['zero_list_sum'] = dummy['zero_list'].apply(lambda x : round(abs(sum(x)),2))\n",
    "#     #dummy = pd.merge(dummy, pos , on = ['Custodian Account','Currency','Ticker1'], how = 'left')\n",
    "#         dummy['remove_mark'] = dummy.apply(lambda x :remove_mark(x['len_zero_list'],x['diff_len'],x['zero_list_sum']),axis = 1)\n",
    "#         dummy = dummy[['ViewData.Mapped Custodian Account','ViewData.Currency',  'zero_list', 'len_zero_list', 'remove_mark']]\n",
    "#         uni4 = pd.merge(uni3, dummy, on = ['ViewData.Mapped Custodian Account','ViewData.Currency'], how = 'left')\n",
    "#         uni4['amt_marker'] = uni4.apply(lambda x : amt_marker(x['ViewData.Net Amount Difference'],x['zero_list'] ,x['len_zero_list']) , axis = 1)\n",
    "        \n",
    "        \n",
    "#         if uni4[(uni4['amt_marker'] != 0) & (uni4['remove_mark'] != 0) ].shape[0]!=0:\n",
    "#             k = uni4[(uni4['amt_marker'] != 0) & (uni4['remove_mark'] != 0)]\n",
    "            \n",
    "#             k1 = k.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'])['ViewData.Side0_UniqueIds'].apply(list).reset_index()\n",
    "#             k2 = k.groupby(['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'])['ViewData.Side1_UniqueIds'].apply(list).reset_index()\n",
    "#             k3 = pd.merge(k1,k2, on = ['ViewData.Mapped Custodian Account','ViewData.Currency','ViewData.Description'], how = 'left' )\n",
    "        \n",
    "#             k3['predicted status'] = 'UMR'\n",
    "#             k3['predicted action'] = 'UMR'\n",
    "#             k3['predicted category'] = 'match'\n",
    "#             k3['predicted comment'] = 'match'\n",
    "#             sel_col_1 = ['ViewData.BreakID','ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','predicted status','predicted action','predicted category','predicted comment']\n",
    "#             k3 = k3[sel_col_1]\n",
    "#             k3.to_csv('prediction result lombard 249 P4.csv')\n",
    "#             uni5 = uni4[(uni4['amt_marker'] == 0)]\n",
    "#             uni5 = uni5.drop(['zero_list', 'len_zero_list', 'remove_mark','amt_marker'], axis = 1)\n",
    "#             remain_df1 = uni5.copy()\n",
    "#         else:\n",
    "#             uni5 = uni4.copy()\n",
    "#             uni5 = uni5.drop(['zero_list', 'len_zero_list', 'remove_mark','amt_marker'], axis = 1)\n",
    "#             remain_df2 = uni5.copy()\n",
    "            \n",
    "        \n",
    "    \n",
    "#     else:\n",
    "#         uni5 = uni4.copy()\n",
    "#         uni5 = uni5.drop(['len_amount', 'zero_list', 'len_zero_list','amt_marker'], axis = 1)\n",
    "#         remain_df2 = uni5.copy()\n",
    "# else:\n",
    "#     m = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M cross n architecture for UMB finding using desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_match(x,y):\n",
    "    if ((type(x) == str) & (type(y)==str)):\n",
    "        x = x.lower()\n",
    "        y = y.lower()\n",
    "        x1 =  re.split(\"[,/. \\- !?:]+\", x)\n",
    "        y1 =  re.split(\"[,/. \\- !?:]+\", y)\n",
    "        if len(x1)<len(y1):\n",
    "            lst3 = [value for value in x1 if value in y1]\n",
    "            \n",
    "            if len(lst3)>0:\n",
    "                score = len(lst3)/len(x1)\n",
    "            else:\n",
    "                score = 0\n",
    "        else:\n",
    "            lst3 = [value for value in y1 if value in x1]\n",
    "            \n",
    "            if len(lst3)>0:\n",
    "                \n",
    "                score = len(lst3)/len(y1)\n",
    "            else:\n",
    "                score = 0\n",
    "    else:\n",
    "        score = 2\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding OB for the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ViewData.Task Business Date', 'ViewData.Side0_UniqueIds',\n",
       "       'ViewData.Side1_UniqueIds', 'ViewData.Mapped Custodian Account',\n",
       "       'ViewData.BreakID', 'ViewData.Fund', 'ViewData.Task Business Date',\n",
       "       'ViewData.Currency', 'ViewData.Asset Type Category',\n",
       "       'ViewData.Transaction Type', 'ViewData.Investment Type',\n",
       "       'ViewData.Prime Broker', 'ViewData.Ticker', 'ViewData.Sec Fees',\n",
       "       'ViewData.Settle Date', 'ViewData.Trade Date', 'ViewData.Description',\n",
       "       'ViewData.CUSIP', 'ViewData.Call Put Indicator', 'ViewData.Cancel Flag',\n",
       "       'ViewData.Commission', 'ViewData.ISIN', 'ViewData.Investment ID',\n",
       "       'ViewData.Interest Amount', 'ViewData.InternalComment1',\n",
       "       'ViewData.InternalComment2', 'ViewData.InternalComment3',\n",
       "       'ViewData.Accounting Net Amount', 'ViewData.B-P Net Amount',\n",
       "       'ViewData.Net Amount Difference', 'ViewData.Status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double6.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this copying is done so that we can add as many filters we can. We start code for UMB here.\n",
    "uni5 = double6.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_col = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.Mapped Custodian Account','ViewData.Fund','ViewData.Task Business Date',\n",
    "#  'ViewData.Currency',\n",
    "#  'ViewData.Asset Type Category',\n",
    "#  'ViewData.Transaction Type',\n",
    "#  'ViewData.Investment Type',\n",
    "#  'ViewData.Prime Broker',\n",
    "#  'ViewData.Ticker',\n",
    "#  'ViewData.Sec Fees',\n",
    "#  'ViewData.Settle Date',\n",
    "#  'ViewData.Trade Date',\n",
    "#  'ViewData.Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uni5 = uni5[filter_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni5[['len_0','len_1']] = uni5.apply(lambda x : mtm(x['ViewData.Side0_UniqueIds'],x['ViewData.Side1_UniqueIds']),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni5['MTM_mark'] = uni5.apply(lambda x : mtm_mark(x['len_0'],x['len_1']),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OB     40830\n",
       "OTO     1640\n",
       "MTM      975\n",
       "OTM      578\n",
       "MTO      243\n",
       "Name: MTM_mark, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni5['MTM_mark'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "ob_df= uni5[uni5['MTM_mark'] == 'OB']\n",
    "side_0 = ob_df[ob_df['ViewData.Side1_UniqueIds'].isna()]\n",
    "side_0['final_id'] = side_0['ViewData.Side0_UniqueIds']\n",
    "side_1 = ob_df[ob_df['ViewData.Side0_UniqueIds'].isna()]\n",
    "side_1['final_id'] = side_1['ViewData.Side1_UniqueIds']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "side0_otm= uni5[uni5['MTM_mark'] != 'OB']\n",
    "side0_otm['final_id'] = side0_otm['ViewData.Side0_UniqueIds'].astype(str) + '|' + side0_otm['ViewData.Side1_UniqueIds'].astype(str) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_0 = pd.concat([side_0,side0_otm], axis = 0)\n",
    "umb_0 = umb_0.reset_index()\n",
    "umb_0.drop('index', axis = 1, inplace =True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_1 = side_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6954, 35)\n",
      "(37312, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:32<00:00, 32.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from pandas import merge\n",
    "from tqdm import tqdm\n",
    "\n",
    "pool =[]\n",
    "key_index =[]\n",
    "training_df =[]\n",
    "call1 = []\n",
    "\n",
    "appended_data = []\n",
    "\n",
    "no_pair_ids = []\n",
    "#max_rows = 5\n",
    "\n",
    "k = list(set(list(set(umb_0['ViewData.Task Business Date'])) + list(set(umb_1['ViewData.Task Business Date']))))\n",
    "k1 = k\n",
    "\n",
    "for d in tqdm(k1):\n",
    "    aa1 = umb_0.copy()#[umb_0['ViewData.Task Business Date']==d]\n",
    "    bb1 = umb_1.copy()#[umb_1['ViewData.Task Business Date']==d]\n",
    "#     aa1['marker'] = 1\n",
    "#     bb1['marker'] = 1\n",
    "    \n",
    "    aa1 = aa1.reset_index()\n",
    "    aa1 = aa1.drop('index',1)\n",
    "    bb1 = bb1.reset_index()\n",
    "    bb1 = bb1.drop('index', 1)\n",
    "    print(aa1.shape)\n",
    "    print(bb1.shape)\n",
    "    \n",
    "    aa1.columns = ['SideB.' + x  for x in aa1.columns] \n",
    "    bb1.columns = ['SideA.' + x  for x in bb1.columns]\n",
    "    \n",
    "    cc1 = pd.merge(aa1,bb1, left_on = ['SideB.ViewData.Mapped Custodian Account','SideB.ViewData.Currency'], right_on = ['SideA.ViewData.Mapped Custodian Account','SideA.ViewData.Currency'], how = 'outer')\n",
    "    appended_data.append(cc1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbmn = pd.concat(appended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbmn['desc_score'] =  umbmn.apply(lambda x : desc_match(x['SideA.ViewData.Description'],x['SideB.ViewData.Description']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input for UMB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now remove those pair where one side is absent\n",
    "ab_a = umbmn[(umbmn['SideB.final_id'].isna()) | (umbmn['SideA.final_id'].isna())]\n",
    "ab_b = umbmn[~((umbmn['SideB.final_id'].isna()) | (umbmn['SideA.final_id'].isna()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SideB.ViewData.Task Business Date', 'SideB.ViewData.Side0_UniqueIds',\n",
       "       'SideB.ViewData.Side1_UniqueIds',\n",
       "       'SideB.ViewData.Mapped Custodian Account', 'SideB.ViewData.BreakID',\n",
       "       'SideB.ViewData.Fund', 'SideB.ViewData.Task Business Date',\n",
       "       'SideB.ViewData.Currency', 'SideB.ViewData.Asset Type Category',\n",
       "       'SideB.ViewData.Transaction Type', 'SideB.ViewData.Investment Type',\n",
       "       'SideB.ViewData.Prime Broker', 'SideB.ViewData.Ticker',\n",
       "       'SideB.ViewData.Sec Fees', 'SideB.ViewData.Settle Date',\n",
       "       'SideB.ViewData.Trade Date', 'SideB.ViewData.Description',\n",
       "       'SideB.ViewData.CUSIP', 'SideB.ViewData.Call Put Indicator',\n",
       "       'SideB.ViewData.Cancel Flag', 'SideB.ViewData.Commission',\n",
       "       'SideB.ViewData.ISIN', 'SideB.ViewData.Investment ID',\n",
       "       'SideB.ViewData.Interest Amount', 'SideB.ViewData.InternalComment1',\n",
       "       'SideB.ViewData.InternalComment2', 'SideB.ViewData.InternalComment3',\n",
       "       'SideB.ViewData.Accounting Net Amount', 'SideB.ViewData.B-P Net Amount',\n",
       "       'SideB.ViewData.Net Amount Difference', 'SideB.ViewData.Status',\n",
       "       'SideB.len_0', 'SideB.len_1', 'SideB.MTM_mark', 'SideB.final_id',\n",
       "       'SideA.ViewData.Task Business Date', 'SideA.ViewData.Side0_UniqueIds',\n",
       "       'SideA.ViewData.Side1_UniqueIds',\n",
       "       'SideA.ViewData.Mapped Custodian Account', 'SideA.ViewData.BreakID',\n",
       "       'SideA.ViewData.Fund', 'SideA.ViewData.Task Business Date',\n",
       "       'SideA.ViewData.Currency', 'SideA.ViewData.Asset Type Category',\n",
       "       'SideA.ViewData.Transaction Type', 'SideA.ViewData.Investment Type',\n",
       "       'SideA.ViewData.Prime Broker', 'SideA.ViewData.Ticker',\n",
       "       'SideA.ViewData.Sec Fees', 'SideA.ViewData.Settle Date',\n",
       "       'SideA.ViewData.Trade Date', 'SideA.ViewData.Description',\n",
       "       'SideA.ViewData.CUSIP', 'SideA.ViewData.Call Put Indicator',\n",
       "       'SideA.ViewData.Cancel Flag', 'SideA.ViewData.Commission',\n",
       "       'SideA.ViewData.ISIN', 'SideA.ViewData.Investment ID',\n",
       "       'SideA.ViewData.Interest Amount', 'SideA.ViewData.InternalComment1',\n",
       "       'SideA.ViewData.InternalComment2', 'SideA.ViewData.InternalComment3',\n",
       "       'SideA.ViewData.Accounting Net Amount', 'SideA.ViewData.B-P Net Amount',\n",
       "       'SideA.ViewData.Net Amount Difference', 'SideA.ViewData.Status',\n",
       "       'SideA.len_0', 'SideA.len_1', 'SideA.MTM_mark', 'SideA.final_id',\n",
       "       'desc_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_b.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbk = ab_b[['SideB.ViewData.Side0_UniqueIds', 'SideB.ViewData.Side1_UniqueIds','SideB.final_id','SideB.ViewData.BreakID','SideB.ViewData.Status','SideA.ViewData.Side0_UniqueIds','SideA.ViewData.Side1_UniqueIds','SideA.final_id','SideA.ViewData.BreakID','SideA.ViewData.Status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_umb = ['SideB.ViewData.Asset Type Category',\n",
    "        'SideB.ViewData.Fund',\n",
    "       'SideB.ViewData.Investment Type',\n",
    "        'SideB.ViewData.Ticker','SideB.ViewData.Transaction Type',\n",
    "      'SideB.ViewData.Mapped Custodian Account','SideB.ViewData.Currency', 'SideA.ViewData.Asset Type Category',\n",
    "        'SideA.ViewData.Fund','SideA.ViewData.Investment Type',\n",
    "        'SideA.ViewData.Ticker','SideA.ViewData.Transaction Type',\n",
    "      'SideA.ViewData.Mapped Custodian Account','SideA.ViewData.Currency', 'desc_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_file = ab_b[col_umb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_vars = [ \n",
    "      \n",
    "#       'SideB.ViewData.Asset Type Category',\n",
    "#         'SideB.ViewData.Fund',\n",
    "#        'SideB.ViewData.Investment Type',\n",
    "#         'SideB.ViewData.Ticker','SideB.ViewData.Transaction Type',\n",
    "#       'SideB.ViewData.Currency', 'SideB.ViewData.Mapped Custodian Account','SideA.ViewData.Asset Type Category',\n",
    "#         'SideA.ViewData.Fund','SideA.ViewData.Investment Type',\n",
    "#         'SideA.ViewData.Ticker','SideA.ViewData.Transaction Type',\n",
    "#       'SideA.ViewData.Currency', 'SideA.ViewData.Mapped Custodian Account'\n",
    "#        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "umb_file['SideB.ViewData.Asset Type Category'] = umb_file['SideB.ViewData.Asset Type Category'].fillna('AA')\n",
    "umb_file['SideB.ViewData.Fund'] = umb_file['SideB.ViewData.Fund'].fillna('BB')\n",
    "umb_file['SideB.ViewData.Investment Type'] = umb_file['SideB.ViewData.Investment Type'].fillna('CC')\n",
    "umb_file['SideB.ViewData.Ticker'] = umb_file['SideB.ViewData.Ticker'].fillna('DD')\n",
    "umb_file['SideB.ViewData.Transaction Type'] = umb_file['SideB.ViewData.Transaction Type'].fillna('EE')\n",
    "umb_file['SideB.ViewData.Currency'] = umb_file['SideB.ViewData.Currency'].fillna('FF')\n",
    "umb_file['SideB.ViewData.Mapped Custodian Account'] = umb_file['SideB.ViewData.Mapped Custodian Account'].fillna('GG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "umb_file['SideA.ViewData.Asset Type Category'] = umb_file['SideA.ViewData.Asset Type Category'].fillna('aa')\n",
    "umb_file['SideA.ViewData.Fund'] = umb_file['SideA.ViewData.Fund'].fillna('bb')\n",
    "umb_file['SideA.ViewData.Investment Type'] = umb_file['SideA.ViewData.Investment Type'].fillna('cc')\n",
    "umb_file['SideA.ViewData.Ticker'] = umb_file['SideA.ViewData.Ticker'].fillna('dd')\n",
    "umb_file['SideA.ViewData.Transaction Type'] = umb_file['SideA.ViewData.Transaction Type'].fillna('ee')\n",
    "umb_file['SideA.ViewData.Currency'] = umb_file['SideA.ViewData.Currency'].fillna('ff')\n",
    "umb_file['SideA.ViewData.Mapped Custodian Account'] = umb_file['SideA.ViewData.Mapped Custodian Account'].fillna('gg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in list(umb_file.columns):\n",
    "    \n",
    "    x1 = item.split('.')\n",
    "    if 'desc_score' not in x1:\n",
    "    \n",
    "        if x1[0]=='SideB':\n",
    "            m = 'ViewData.' + 'Accounting'+ \" \" + x1[2]\n",
    "            umb_file = umb_file.rename(columns = {item:m})\n",
    "        else:\n",
    "            m = 'ViewData.' + 'B-P'+ \" \" + x1[2]\n",
    "            umb_file =umb_file.rename(columns = {item:m})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model_lombard_249_umb_v3.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_predictions = clf.predict(umb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = []\n",
    "for item in cb_predictions:\n",
    "    demo.append(item[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_file['predicted'] = pd.Series(demo)\n",
    "#result['Actual'] = pd.Series(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OB     3969186\n",
       "UMB      97939\n",
       "Name: predicted, dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umb_file['predicted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ViewData.Accounting Asset Type Category', 'ViewData.Accounting Fund',\n",
       "       'ViewData.Accounting Investment Type', 'ViewData.Accounting Ticker',\n",
       "       'ViewData.Accounting Transaction Type',\n",
       "       'ViewData.Accounting Mapped Custodian Account',\n",
       "       'ViewData.Accounting Currency', 'ViewData.B-P Asset Type Category',\n",
       "       'ViewData.B-P Fund', 'ViewData.B-P Investment Type',\n",
       "       'ViewData.B-P Ticker', 'ViewData.B-P Transaction Type',\n",
       "       'ViewData.B-P Mapped Custodian Account', 'ViewData.B-P Currency',\n",
       "       'desc_score', 'predicted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umb_file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbpred = pd.concat([umbk,umb_file], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbpred =umbpred.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first segragate ab_a, then next\n",
    "side0_ab_a = ab_a[ab_a['SideB.final_id'].isna()]\n",
    "side1_ab_a = ab_a[~ab_a['SideB.final_id'].isna()]\n",
    "\n",
    "if ((side0_ab_a.shape[0]!=0) & (side1_ab_a.shape[0]!=0)):\n",
    "    list_id_0_ab_a = list(set(ab_a['SideA.final_id']))\n",
    "    list_id_1_ab_a = list(set(ab_a['SideB.final_id']))\n",
    "    side0_ob = umb_0[umb_0['final_id'].isin(list_id_1_ab_a)]\n",
    "    side1_ob = umb_1[umb_1['final_id'].isin(list_id_0_ab_a)]\n",
    "    ob_1st_set = pd.concat([side0_ob,side1_ob], axis = 0)\n",
    "    ob_1st_set = ob_1st_set.reset_index()\n",
    "    ob_1st_set = ob_1st_set.drop('index', axis = 1)\n",
    "elif (side0_ab_a.shape[0]!=0):\n",
    "    list_id_0_ab_a = list(set(ab_a['SideA.final_id']))\n",
    "    \n",
    "    side0_ob = umb_0[umb_0['final_id'].isin(list_id_1_ab_a)]\n",
    "    \n",
    "    ob_1st_set = side0_ob.copy()\n",
    "    ob_1st_set = ob_1st_set.reset_index()\n",
    "    ob_1st_set = ob_1st_set.drop('index', axis = 1)\n",
    "else:\n",
    "    list_id_1_ab_a = list(set(ab_a['SideB.final_id']))\n",
    "    \n",
    "    side1_ob = umb_1[umb_1['final_id'].isin(list_id_0_ab_a)]\n",
    "    \n",
    "    ob_1st_set = side1_ob.copy()\n",
    "    ob_1st_set = ob_1st_set.reset_index()\n",
    "    ob_1st_set = ob_1st_set.drop('index', axis = 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will segragate IDs on both sides which were just OB.\n",
    "k1 = umbpred.groupby('SideB.final_id')['predicted'].apply(list).reset_index()\n",
    "k2 = umbpred.groupby('SideA.final_id')['predicted'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ob_umb(x):\n",
    "    x1 = list(set(x))\n",
    "    if ((len(x1)==1) & ('OB' in x1)):\n",
    "        return 'OB'\n",
    "    elif ((len(x1)==1) & ('UMB' in x1)):\n",
    "        return 'UMB'\n",
    "    else:\n",
    "        return 'Both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1['State'] = k1['predicted'].apply(lambda x : ob_umb(x) )\n",
    "k2['State'] = k2['predicted'].apply(lambda x : ob_umb(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_id_0_k1 = list(set(k1[k1['State']=='OB']['SideB.final_id']))\n",
    "list_id_1_k2 = list(set(k2[k2['State']=='OB']['SideA.final_id']))\n",
    "side0_ob = umb_0[umb_0['final_id'].isin(list_id_0_k1)]\n",
    "side1_ob = umb_1[umb_1['final_id'].isin(list_id_1_k2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_2nd_set = pd.concat([side0_ob,side1_ob], axis = 0)\n",
    "ob_2nd_set = ob_2nd_set.reset_index()\n",
    "ob_2nd_set = ob_2nd_set.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31298, 35)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob_2nd_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((ob_1st_set.shape[0]!=0) & (ob_2nd_set.shape[0]!=0)):\n",
    "    ob_for_comment = pd.concat([ob_1st_set,ob_2nd_set], axis = 0)\n",
    "    ob_for_comment = ob_for_comment.reset_index()\n",
    "    ob_for_comment = ob_for_comment.drop('index', axis = 1)\n",
    "elif ((ob_1st_set.shape[0]!=0)):\n",
    "    ob_for_comment = ob_1st_set.copy()\n",
    "    ob_for_comment = ob_for_comment.reset_index()\n",
    "    ob_for_comment = ob_for_comment.drop('index', axis = 1)\n",
    "else:\n",
    "    \n",
    "    ob_for_comment = ob_2nd_set.copy()\n",
    "    ob_for_comment = ob_for_comment.reset_index()\n",
    "    ob_for_comment = ob_for_comment.drop('index', axis = 1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_for_comment.to_csv('Ob for comment daily lombard 249.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbpred = umbpred[~umbpred['SideB.final_id'].isin(list_id_0_k1)]\n",
    "umbpred = umbpred[~umbpred['SideA.final_id'].isin(list_id_1_k2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difficult part segragation of UMBs in OTO, OTM, MTO and MTM\n",
    "# OTO\n",
    "\n",
    "import collections\n",
    "\n",
    "def umb_counter(x):\n",
    "    counter=collections.Counter(x)\n",
    "    if counter['UMB'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return counter['UMB']\n",
    "        \n",
    "k1['umb_counter'] = k1['predicted'].apply(lambda x : umb_counter(x) )\n",
    "k2['umb_counter'] = k2['predicted'].apply(lambda x : umb_counter(x) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_id_0_k1 = list(set(k1[k1['umb_counter']==1]['SideB.final_id']))\n",
    "list_id_1_k2 = list(set(k2[k2['umb_counter']==1]['SideA.final_id']))\n",
    "if ((len(list_id_0_k1)>0) & (len(list_id_1_k2)>0)):\n",
    "    umb_oto = umbpred[(umbpred['SideB.final_id'].isin(list_id_0_k1)) & (umbpred['SideA.final_id'].isin(list_id_1_k2))]\n",
    "    umbpred = umbpred[~umbpred['SideB.final_id'].isin(list_id_0_k1)]\n",
    "    umbpred = umbpred[~umbpred['SideA.final_id'].isin(list_id_1_k2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakid(x,y):\n",
    "    \n",
    "    brk = []\n",
    "    for item in x:\n",
    "        brk.append(item)\n",
    "    brk.append(y)\n",
    "    \n",
    "    return brk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now We write the hierarchy for many to one\n",
    "k3 = umbpred.groupby('SideB.final_id')['SideA.final_id'].apply(list).reset_index()\n",
    "k4 = umbpred.groupby('SideB.final_id')['SideA.ViewData.BreakID'].apply(list).reset_index()\n",
    "k3 = pd.merge(k3, k4, on = 'SideB.final_id', how = 'left')\n",
    "k3['id_len'] = k3['SideA.final_id'].apply(lambda x : len(x) )\n",
    "mn = umbpred[['SideB.final_id','SideB.ViewData.BreakID']]\n",
    "mn = mn.drop_duplicates()\n",
    "k3 = pd.merge(k3, mn, on = 'SideB.final_id', how = 'left')\n",
    "k3['ViewData.BreakID'] = k3.apply(lambda x : breakid(x['SideA.ViewData.BreakID'],x['SideB.ViewData.BreakID']), axis = 1 )\n",
    "stat = umbpred.groupby('SideB.final_id')['SideA.ViewData.Status'].apply(list).reset_index()\n",
    "k3 = pd.merge(k3, stat, on = 'SideB.final_id', how = 'left')\n",
    "\n",
    "mn1 = umbpred[['SideB.final_id','SideB.ViewData.Status']]\n",
    "mn1 = mn1.drop_duplicates()\n",
    "k3 = pd.merge(k3, mn1, on = 'SideB.final_id', how = 'left')\n",
    "k3['ViewData.Status'] = k3.apply(lambda x : breakid(x['SideA.ViewData.Status'],x['SideB.ViewData.Status']), axis = 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2472, 9)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return round((len(lst3)/len(lst1)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(li1, li2):\n",
    "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2]\n",
    "    return li_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "ob_stage_df = pd.DataFrame()\n",
    "umb_mtm = pd.DataFrame()\n",
    "umb_mtm_list = []\n",
    "umb_otm_list = []\n",
    "\n",
    "for i, row in k3.iterrows():\n",
    "    if k3.shape[0]!=0:\n",
    "    \n",
    "        k5 = k3.copy()\n",
    "        mid = row['SideB.final_id']\n",
    "        midlist = row['SideA.final_id']\n",
    "        midlen = row['id_len']\n",
    "        all_brk = row['ViewData.BreakID']\n",
    "        all_sts = row['ViewData.Status']\n",
    "    \n",
    "        k6 = k5[(k5['id_len']<midlen+3) & (k5['id_len']>midlen-4)]\n",
    "        k6['match'] = k6['SideA.final_id'].apply(lambda x:intersection_(x,midlist) )\n",
    "    \n",
    "    \n",
    "        k7 =list(set(k6[k6['match']>0.9]['SideB.final_id']))\n",
    "        if len(k7)>0:\n",
    "        \n",
    "            set_for_int = list((k6[k6['match']>0.7]['SideA.final_id']))\n",
    "            k8 = list(reduce(set.intersection, [set(item) for item in set_for_int]))\n",
    "        \n",
    "            int1 = umbpred[umbpred['SideB.final_id'].isin(k7)]\n",
    "            br7 =list((int1['SideB.ViewData.BreakID']))\n",
    "            br8 =list((umbpred[umbpred['SideA.final_id'].isin(k8)]['SideA.ViewData.BreakID']))\n",
    "            br7_8 = br7 + br8\n",
    "            vi7 =list(set(int1['SideB.ViewData.Status']))\n",
    "            vi8 =list(set(umbpred[umbpred['SideA.final_id'].isin(k8)]['SideA.ViewData.Status']))\n",
    "            vi7_8 = vi7 + vi8\n",
    "            k9 =list(set(int1['SideA.final_id']))\n",
    "            if ((len(k8)>0) & (len(k9)>0)):\n",
    "                umb_mtm_list_temp = []\n",
    "                umb_mtm_list_temp.append(k7)\n",
    "                umb_mtm_list_temp.append(k8)\n",
    "                umb_mtm_list_temp.append(br7_8)\n",
    "                umb_mtm_list_temp.append(vi7_8)\n",
    "                umb_mtm_list.append(umb_mtm_list_temp)\n",
    "            \n",
    "                k10 = Diff(k9,k8)\n",
    "                umbpred = umbpred[~umbpred['SideB.final_id'].isin(k7)]\n",
    "                k11 = list(set(umbpred['SideA.final_id']))\n",
    "                k12 = Diff(k10,k11)\n",
    "                ob_3rd_set = umb_1[umb_1['final_id'].isin(k12)]\n",
    "                ob_stage_df = pd.concat([ob_stage_df,ob_3rd_set], axis = 0)\n",
    "                ob_stage_df = ob_stage_df.reset_index()\n",
    "                ob_stage_df = ob_stage_df.drop('index', axis = 1)\n",
    "            \n",
    "                int1 = umbpred[umbpred['SideA.final_id'].isin(k8)]\n",
    "            \n",
    "#             k15 =list(set(int1['SideB.final_id']))\n",
    "#             k16 = Diff(k15,k7)\n",
    "#             umbpred = umbpred[~umbpred['SideA.final_id'].isin(k8)]\n",
    "#             k17 = list(set(umbpred['SideB.final_id']))\n",
    "#             k18 = Diff(k16,k17)\n",
    "#             ob_4th_set = umb_0[umb_0['final_id'].isin(k18)]\n",
    "#             ob_stage_df = pd.concat([ob_stage_df,ob_4th_set], axis = 0)\n",
    "#             ob_stage_df = ob_stage_df.reset_index()\n",
    "#             ob_stage_df = ob_stage_df.drop('index', axis = 1)\n",
    "            if umbpred.shape[0]!=0:\n",
    "                k3 = umbpred.groupby('SideB.final_id')['SideA.final_id'].apply(list).reset_index()\n",
    "                k4 = umbpred.groupby('SideB.final_id')['SideA.ViewData.BreakID'].apply(list).reset_index()\n",
    "                k3 = pd.merge(k3, k4, on = 'SideB.final_id', how = 'left')\n",
    "                k3['id_len'] = k3['SideA.final_id'].apply(lambda x : len(x) )\n",
    "                mn = umbpred[['SideB.final_id','SideB.ViewData.BreakID']]\n",
    "                mn = mn.drop_duplicates()\n",
    "                k3 = pd.merge(k3, mn, on = 'SideB.final_id', how = 'left')\n",
    "                k3['ViewData.BreakID'] = k3.apply(lambda x : breakid(x['SideA.ViewData.BreakID'],x['SideB.ViewData.BreakID']), axis = 1 )\n",
    "                stat = umbpred.groupby('SideB.final_id')['SideA.ViewData.Status'].apply(list).reset_index()\n",
    "                k3 = pd.merge(k3, stat, on = 'SideB.final_id', how = 'left')\n",
    "\n",
    "                mn1 = umbpred[['SideB.final_id','SideB.ViewData.Status']]\n",
    "                mn1 = mn1.drop_duplicates()\n",
    "                k3 = pd.merge(k3, mn1, on = 'SideB.final_id', how = 'left')\n",
    "                k3['ViewData.Status'] = k3.apply(lambda x : breakid(x['SideA.ViewData.Status'],x['SideB.ViewData.Status']), axis = 1 )\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        else:\n",
    "        #k8 = list(reduce(set.intersection, [set(item) for item in set_for_int]))\n",
    "#         int1 = k3[k3['SideB.final_id']==mid]\n",
    "#         print(int1.shape[0])\n",
    "            mk = list(mid)\n",
    "  \n",
    "            if (len(midlist)>0):\n",
    "            \n",
    "#             int1 = umbpred[umbpred['SideB.final_id']==mid]\n",
    "#             print(int1.shape)\n",
    "#             br7 = list(umbpred[umbpred['SideB.final_id']==mid]['SideB.ViewData.BreakID'])\n",
    "#             print(br7)\n",
    "#             br8 =list((umbpred[umbpred['SideA.final_id'].isin(midlist)]['SideA.ViewData.BreakID']))\n",
    "#             #print(br8)\n",
    "#             br7_8 = br7 + br8\n",
    "#             vi7 =list(umbpred[umbpred['SideB.final_id']==mid]['SideB.ViewData.Status'])\n",
    "#             vi8 =list(set(umbpred[umbpred['SideA.final_id'].isin(midlist)]['SideA.ViewData.Status']))\n",
    "#             vi7_8 = vi7 + vi8\n",
    "                umb_otm_list_temp = []\n",
    "                umb_otm_list_temp.append(mid)\n",
    "                umb_otm_list_temp.append(midlist)\n",
    "                umb_otm_list_temp.append(all_brk)\n",
    "                umb_otm_list_temp.append(all_sts)\n",
    "            \n",
    "                umb_otm_list.append(umb_otm_list_temp)\n",
    "            #k10 = Diff(midlist,mk) \n",
    "                knn = umbpred[umbpred['SideA.final_id'].isin(midlist)]\n",
    "                k11 = list(set(knn['SideB.final_id']))\n",
    "                k12 = Diff(k11,mk)\n",
    "            \n",
    "                umbpred = umbpred[~umbpred['SideA.final_id'].isin(midlist)]\n",
    "                k13 = list(set(umbpred['SideB.final_id']))\n",
    "                k14 = Diff(k12,k13)\n",
    "            \n",
    "                ob_4th_set = umb_0[umb_0['final_id'].isin(k14)]\n",
    "                ob_stage_df = pd.concat([ob_stage_df,ob_4th_set], axis = 0)\n",
    "                ob_stage_df = ob_stage_df.reset_index()\n",
    "                ob_stage_df = ob_stage_df.drop('index', axis = 1)\n",
    "            if umbpred.shape[0]!=0:\n",
    "                k3 = umbpred.groupby('SideB.final_id')['SideA.final_id'].apply(list).reset_index()\n",
    "                k4 = umbpred.groupby('SideB.final_id')['SideA.ViewData.BreakID'].apply(list).reset_index()\n",
    "                k3 = pd.merge(k3, k4, on = 'SideB.final_id', how = 'left')\n",
    "                k3['id_len'] = k3['SideA.final_id'].apply(lambda x : len(x) )\n",
    "                mn = umbpred[['SideB.final_id','SideB.ViewData.BreakID']]\n",
    "                mn = mn.drop_duplicates()\n",
    "                k3 = pd.merge(k3, mn, on = 'SideB.final_id', how = 'left')\n",
    "                k3['ViewData.BreakID'] = k3.apply(lambda x : breakid(x['SideA.ViewData.BreakID'],x['SideB.ViewData.BreakID']), axis = 1 )\n",
    "                stat = umbpred.groupby('SideB.final_id')['SideA.ViewData.Status'].apply(list).reset_index()\n",
    "                k3 = pd.merge(k3, stat, on = 'SideB.final_id', how = 'left')\n",
    "\n",
    "                mn1 = umbpred[['SideB.final_id','SideB.ViewData.Status']]\n",
    "                mn1 = mn1.drop_duplicates()\n",
    "                k3 = pd.merge(k3, mn1, on = 'SideB.final_id', how = 'left')\n",
    "                k3['ViewData.Status'] = k3.apply(lambda x : breakid(x['SideA.ViewData.Status'],x['SideB.ViewData.Status']), axis = 1 )\n",
    "            else:\n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 26)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umbpred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_mtm = pd.DataFrame(umb_mtm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_otm = pd.DataFrame(umb_otm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_otm.columns = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.BreakID','ViewData.Status']\n",
    "umb_mtm.columns = ['ViewData.Side0_UniqueIds','ViewData.Side1_UniqueIds','ViewData.BreakID','ViewData.Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_otm['predicted status'] = 'UMB'\n",
    "umb_otm['predicted action'] = 'UMB one to many'\n",
    "umb_otm['predicted category'] = 'UMB'\n",
    "umb_otm['predicted comment'] = 'Difference in amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_mtm['predicted status'] = 'UMB'\n",
    "umb_mtm['predicted action'] = 'UMB many to many'\n",
    "umb_mtm['predicted category'] = 'UMB'\n",
    "umb_mtm['predicted comment'] = 'Difference in amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_mtm = umb_mtm.reset_index()\n",
    "umb_mtm = umb_mtm.drop('index', axis = 1)\n",
    "\n",
    "umb_otm = umb_otm.reset_index()\n",
    "umb_otm = umb_otm.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_oto1 = umb_oto[['SideB.final_id','SideB.ViewData.BreakID','SideB.ViewData.Status','SideA.final_id','SideA.ViewData.BreakID','SideA.ViewData.Status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_oto1 = umb_oto1.rename(columns = {'SideB.final_id':'ViewData.Side0_UniqueIds',\n",
    "                                     'SideA.final_id':'ViewData.Side1_UniqueIds'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combining(x,y):\n",
    "    blank_list = []\n",
    "    blank_list.append(x)\n",
    "    blank_list.append(y)\n",
    "    \n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_oto1['ViewData.BreakID'] = umb_oto1.apply(lambda x : combining(x['SideB.ViewData.BreakID'], x['SideA.ViewData.BreakID']), axis = 1)\n",
    "umb_oto1['ViewData.Status'] = umb_oto1.apply(lambda x : combining(x['SideB.ViewData.Status'], x['SideA.ViewData.Status']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_oto1 = umb_oto1.reset_index()\n",
    "umb_oto1.drop(['index','SideB.ViewData.BreakID','SideA.ViewData.BreakID','SideB.ViewData.Status','SideA.ViewData.Status'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_oto1['predicted status'] = 'UMB'\n",
    "umb_oto1['predicted action'] = 'UMB one to one'\n",
    "umb_oto1['predicted category'] = 'UMB'\n",
    "umb_oto1['predicted comment'] = 'Difference in amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb_oto1.to_csv('Lombard/249/umb lombard 249 oto.csv')\n",
    "umb_mtm.to_csv('Lombard/249/umb lombard 249 mtm.csv')\n",
    "umb_otm.to_csv('Lombard/249/umb lombard 249 otm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = ob_stage_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5679, 35)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ob_for_comment = pd.read_csv('Ob for comment daily lombard 249.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_for_comment = ob_for_comment.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36264, 35)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob_for_comment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_for_comment.drop('ViewData.Task Business Date', axis =1 , inplace = True)\n",
    "k.drop('ViewData.Task Business Date', axis =1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ViewData.Side0_UniqueIds', 'ViewData.Side1_UniqueIds',\n",
       "       'ViewData.Mapped Custodian Account', 'ViewData.BreakID',\n",
       "       'ViewData.Fund', 'ViewData.Currency', 'ViewData.Asset Type Category',\n",
       "       'ViewData.Transaction Type', 'ViewData.Investment Type',\n",
       "       'ViewData.Prime Broker', 'ViewData.Ticker', 'ViewData.Sec Fees',\n",
       "       'ViewData.Settle Date', 'ViewData.Trade Date', 'ViewData.Description',\n",
       "       'ViewData.CUSIP', 'ViewData.Call Put Indicator', 'ViewData.Cancel Flag',\n",
       "       'ViewData.Commission', 'ViewData.ISIN', 'ViewData.Investment ID',\n",
       "       'ViewData.Interest Amount', 'ViewData.InternalComment1',\n",
       "       'ViewData.InternalComment2', 'ViewData.InternalComment3',\n",
       "       'ViewData.Accounting Net Amount', 'ViewData.B-P Net Amount',\n",
       "       'ViewData.Net Amount Difference', 'ViewData.Status', 'len_0', 'len_1',\n",
       "       'MTM_mark', 'final_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ob_for_comment_model = pd.concat([ob_for_comment,k], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_for_comment_model = ob_for_comment_model.reset_index()\n",
    "ob_for_comment_model = ob_for_comment_model.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we take all OBs to single side model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from dateutil.parser import parse\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = ob_for_comment_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MTM_mark', 'ViewData.Accounting Net Amount',\n",
       "       'ViewData.Asset Type Category', 'ViewData.B-P Net Amount',\n",
       "       'ViewData.BreakID', 'ViewData.CUSIP', 'ViewData.Call Put Indicator',\n",
       "       'ViewData.Cancel Flag', 'ViewData.Commission', 'ViewData.Currency',\n",
       "       'ViewData.Description', 'ViewData.Fund', 'ViewData.ISIN',\n",
       "       'ViewData.Interest Amount', 'ViewData.InternalComment1',\n",
       "       'ViewData.InternalComment2', 'ViewData.InternalComment3',\n",
       "       'ViewData.Investment ID', 'ViewData.Investment Type',\n",
       "       'ViewData.Mapped Custodian Account', 'ViewData.Net Amount Difference',\n",
       "       'ViewData.Prime Broker', 'ViewData.Sec Fees', 'ViewData.Settle Date',\n",
       "       'ViewData.Side0_UniqueIds', 'ViewData.Side1_UniqueIds',\n",
       "       'ViewData.Status', 'ViewData.Task Business Date.1', 'ViewData.Ticker',\n",
       "       'ViewData.Trade Date', 'ViewData.Transaction Type', 'final_id', 'len_0',\n",
       "       'len_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = pd.read_excel('Mapping variables for variable cleaning.xlsx', sheet_name='General')\n",
    "def make_dict(row):\n",
    "    keys_l = str(row['Keys']).lower()\n",
    "    keys_s = keys_l.split(', ')\n",
    "    keys = tuple(keys_s)\n",
    "    return keys\n",
    "df['tuple'] = df.apply(make_dict, axis=1)\n",
    "clean_map_dict = df.set_index('tuple')['Value'].to_dict()\n",
    "\n",
    "df3['ViewData.Transaction Type'] = df3['ViewData.Transaction Type'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df3['ViewData.Asset Type Category'] = df3['ViewData.Asset Type Category'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df3['ViewData.Investment Type'] = df3['ViewData.Investment Type'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "df3['ViewData.Prime Broker'] = df3['ViewData.Prime Broker'].apply(lambda x : x.lower() if type(x)==str else x)\n",
    "\n",
    "def clean_mapping(item):\n",
    "    item1 = item.split()\n",
    "    \n",
    "    \n",
    "    ttype = []\n",
    "    \n",
    "    \n",
    "    for x in item1:\n",
    "        ttype1 = []\n",
    "        for key, value in clean_map_dict.items():\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "            if x in key:\n",
    "                a = value\n",
    "                ttype1.append(a)\n",
    "           \n",
    "        if len(ttype1)==0:\n",
    "            ttype1.append(x)\n",
    "        ttype = ttype + ttype1\n",
    "        \n",
    "    return ' '.join(ttype)\n",
    "\n",
    "df3['ViewData.Transaction Type1'] = df3['ViewData.Transaction Type'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df3['ViewData.Asset Type Category1'] = df3['ViewData.Asset Type Category'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df3['ViewData.Investment Type1'] = df3['ViewData.Investment Type'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "df3['ViewData.Prime Broker1'] = df3['ViewData.Prime Broker'].apply(lambda x : clean_mapping(x) if type(x)==str else x)\n",
    "\n",
    "def is_num(item):\n",
    "    try:\n",
    "        float(item)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_date_format(item):\n",
    "    try:\n",
    "        parse(item, fuzzy=False)\n",
    "        return True\n",
    "    \n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def date_edge_cases(item):\n",
    "    if len(item) == 5 and item[2] =='/' and is_num(item[:2]) and is_num(item[3:]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def comb_clean(x):\n",
    "    k = []\n",
    "    for item in x.split():\n",
    "        if ((is_num(item)==False) and (is_date_format(item)==False) and (date_edge_cases(item)==False)):\n",
    "            k.append(item)\n",
    "    return ' '.join(k)\n",
    "\n",
    "df3['ViewData.Transaction Type1'] = df3['ViewData.Transaction Type1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "df3['ViewData.Asset Type Category1'] = df3['ViewData.Asset Type Category1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "df3['ViewData.Investment Type1'] = df3['ViewData.Investment Type1'].apply(lambda x : comb_clean(x) if type(x)==str else x)\n",
    "df3['ViewData.Prime Broker1'] = df3['ViewData.Prime Broker1'].apply(lambda x : comb_clean(x) if type(x)==str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "com = pd.read_csv('desc cat with naveen oaktree.csv')\n",
    "cat_list = list(set(com['Pairing']))\n",
    "\n",
    "def descclean(com,cat_list):\n",
    "    cat_all1 = []\n",
    "    list1 = cat_list\n",
    "    m = 0\n",
    "    if (type(com) == str):\n",
    "        com = com.lower()\n",
    "        com1 =  re.split(\"[,/. \\-!?:]+\", com)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for item in list1:\n",
    "            if (type(item) == str):\n",
    "                item = item.lower()\n",
    "                item1 = item.split(' ')\n",
    "                lst3 = [value for value in item1 if value in com1] \n",
    "                if len(lst3) == len(item1):\n",
    "                    cat_all1.append(item)\n",
    "                    m = m+1\n",
    "            \n",
    "                else:\n",
    "                    m = m\n",
    "            else:\n",
    "                    m = 0\n",
    "    else:\n",
    "        m = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    if m >0 :\n",
    "        return list(set(cat_all1))\n",
    "    else:\n",
    "        if ((type(com)==str)):\n",
    "            if (len(com1)<4):\n",
    "                if ((len(com1)==1) & com1[0].startswith('20')== True):\n",
    "                    return 'swap id'\n",
    "                else:\n",
    "                    return com\n",
    "            else:\n",
    "                return 'NA'\n",
    "        else:\n",
    "            return 'NA'\n",
    "\n",
    "df3['desc_cat'] = df3['ViewData.Description'].apply(lambda x : descclean(x,cat_list))\n",
    "\n",
    "def currcln(x):\n",
    "    if (type(x)==list):\n",
    "        return x\n",
    "      \n",
    "    else:\n",
    "       \n",
    "        \n",
    "        if x == 'NA':\n",
    "            return \"NA\"\n",
    "        elif (('dollar' in x) | ('dollars' in x )):\n",
    "            return 'dollar'\n",
    "        elif (('pound' in x) | ('pounds' in x)):\n",
    "            return 'pound'\n",
    "        elif ('yen' in x):\n",
    "            return 'yen'\n",
    "        elif ('euro' in x) :\n",
    "            return 'euro'\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['desc_cat'] = df3['desc_cat'].apply(lambda x : currcln(x))\n",
    "com = com.drop(['var','Catogery'], axis = 1)\n",
    "com['Pairing'] = com['Pairing'].apply(lambda x : x.lower())\n",
    "com['replace'] = com['replace'].apply(lambda x : x.lower())\n",
    "\n",
    "def catcln1(cat,df):\n",
    "    ret = []\n",
    "    if (type(cat)==list):\n",
    "        \n",
    "        if 'equity swap settlement' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'equity swap' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'swap settlement' in cat:\n",
    "            ret.append('equity swap settlement')\n",
    "        #return 'equity swap settlement'\n",
    "        elif 'swap unwind' in cat:\n",
    "            ret.append('swap unwind')\n",
    "        #return 'swap unwind'\n",
    "   \n",
    "    \n",
    "        else:\n",
    "        \n",
    "       \n",
    "            for item in cat:\n",
    "            \n",
    "                a = df[df['Pairing']==item]['replace'].values[0]\n",
    "                if a not in ret:\n",
    "                    ret.append(a)\n",
    "        return list(set(ret))\n",
    "      \n",
    "    else:\n",
    "        return cat\n",
    "    \n",
    "df3['new_desc_cat'] = df3['desc_cat'].apply(lambda x : catcln1(x,com))\n",
    "\n",
    "comp = ['inc','stk','corp ','llc','pvt','plc']\n",
    "df3['new_desc_cat'] = df3['new_desc_cat'].apply(lambda x : 'Company' if x in comp else x)\n",
    "def desccat(x):\n",
    "    if isinstance(x, list):\n",
    "        \n",
    "        if 'equity swap settlement' in x:\n",
    "            return 'swap settlement'\n",
    "        elif 'collateral transfer' in x:\n",
    "            return 'collateral transfer'\n",
    "        elif 'dividend' in x:\n",
    "            return 'dividend'\n",
    "        elif (('loan' in x) & ('option' in x)):\n",
    "            return 'option loan'\n",
    "        \n",
    "        elif (('interest' in x) & ('corp' in x) ):\n",
    "            return 'corp loan'\n",
    "        elif (('interest' in x) & ('loan' in x) ):\n",
    "            return 'interest'\n",
    "        else:\n",
    "            return x[0]\n",
    "    else:\n",
    "        if x == 'db_int':\n",
    "            return 'interest'\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "df3['new_desc_cat'] = df3['new_desc_cat'].apply(lambda x : desccat(x))\n",
    "\n",
    "df3['new_pb'] = df3['ViewData.Mapped Custodian Account'].apply(lambda x : x.split('_')[0] if type(x)==str else x)\n",
    "new_pb_mapping = {'GSIL':'GS','CITIGM':'CITI','JPMNA':'JPM'}\n",
    "def new_pf_mapping(x):\n",
    "    if x=='GSIL':\n",
    "        return 'GS'\n",
    "    elif x == 'CITIGM':\n",
    "        return 'CITI'\n",
    "    elif x == 'JPMNA':\n",
    "        return 'JPM'\n",
    "    else:\n",
    "        return x\n",
    "df3['new_pb'] = df3['new_pb'].apply(lambda x : new_pf_mapping(x))\n",
    "df3['ViewData.Prime Broker1'] = df3['ViewData.Prime Broker1'].fillna('kkk')\n",
    "df3['new_pb1'] = df3.apply(lambda x : x['new_pb'] if x['ViewData.Prime Broker1']=='kkk' else x['ViewData.Prime Broker1'],axis = 1)\n",
    "df3['new_pb1'] = df3['new_pb1'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['ViewData.Settle Date'] = pd.to_datetime(df3['ViewData.Settle Date'])\n",
    "days = [1,30,31,29]\n",
    "df3['monthend marker'] = df3['ViewData.Settle Date'].apply(lambda x : 1 if x.day in days else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['comm_marker'] = 'zero'\n",
    "df3['new_pb2'] = df3.apply(lambda x : 'Geneva' if x['ViewData.Side0_UniqueIds'] != 'AA' else x['new_pb1'], axis = 1)\n",
    "df3['new_pb2'] = df3['new_pb2'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MTM_mark', 'ViewData.Accounting Net Amount',\n",
       "       'ViewData.Asset Type Category', 'ViewData.B-P Net Amount',\n",
       "       'ViewData.BreakID', 'ViewData.CUSIP', 'ViewData.Call Put Indicator',\n",
       "       'ViewData.Cancel Flag', 'ViewData.Commission', 'ViewData.Currency',\n",
       "       'ViewData.Description', 'ViewData.Fund', 'ViewData.ISIN',\n",
       "       'ViewData.Interest Amount', 'ViewData.InternalComment1',\n",
       "       'ViewData.InternalComment2', 'ViewData.InternalComment3',\n",
       "       'ViewData.Investment ID', 'ViewData.Investment Type',\n",
       "       'ViewData.Mapped Custodian Account', 'ViewData.Net Amount Difference',\n",
       "       'ViewData.Prime Broker', 'ViewData.Sec Fees', 'ViewData.Settle Date',\n",
       "       'ViewData.Side0_UniqueIds', 'ViewData.Side1_UniqueIds',\n",
       "       'ViewData.Status', 'ViewData.Task Business Date.1', 'ViewData.Ticker',\n",
       "       'ViewData.Trade Date', 'ViewData.Transaction Type', 'final_id', 'len_0',\n",
       "       'len_1', 'ViewData.Transaction Type1', 'ViewData.Asset Type Category1',\n",
       "       'ViewData.Investment Type1', 'ViewData.Prime Broker1', 'desc_cat',\n",
       "       'new_desc_cat', 'new_pb', 'new_pb1', 'monthend marker', 'comm_marker',\n",
       "       'new_pb2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ViewData.Transaction Type1',\n",
    " 'ViewData.Asset Type Category1',\n",
    " 'ViewData.Investment Type1',\n",
    " 'new_desc_cat',\n",
    " 'new_pb2',\n",
    " 'new_pb1',\n",
    " 'comm_marker',\n",
    " 'monthend marker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\consultant137\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df4['ViewData.Transaction Type1'] = df4['ViewData.Transaction Type1'].fillna('aa')\n",
    "df4['ViewData.Asset Type Category1'] = df4['ViewData.Asset Type Category1'].fillna('aa')\n",
    "df4['ViewData.Investment Type1'] = df4['ViewData.Investment Type1'].fillna('aa')\n",
    "df4['new_desc_cat'] = df4['new_desc_cat'].fillna('aa')\n",
    "df4['new_pb2'] = df4['new_pb2'].fillna('aa')\n",
    "df4['new_pb1'] = df4['new_pb1'].fillna('aa')\n",
    "df4['comm_marker'] = df4['comm_marker'].fillna('aa')\n",
    "df4['monthend marker'] = df4['monthend marker'].fillna('aa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model_lombard_249_v1.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViewData.Transaction Type1       41943\n",
       "ViewData.Asset Type Category1    41943\n",
       "ViewData.Investment Type1        41943\n",
       "new_desc_cat                     41943\n",
       "new_pb2                          41943\n",
       "new_pb1                          41943\n",
       "comm_marker                      41943\n",
       "monthend marker                  41943\n",
       "dtype: int64"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_predictions = clf.predict(df4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = []\n",
    "for item in cb_predictions:\n",
    "    demo.append(item[0])\n",
    "df3['predicted category'] = pd.Series(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_temp = pd.read_csv('lobard 249 comment template for delivery.csv')\n",
    "com_temp = com_temp.rename(columns = {'Category':'predicted category','template':'predicted template'})\n",
    "result_non_trade = df3.copy()\n",
    "result_non_trade = pd.merge(result_non_trade,com_temp,on = 'predicted category',how = 'left')\n",
    "def comgen(x,y,z,k):\n",
    "    if x == 'Geneva':\n",
    "        \n",
    "        com = k + ' ' +y + ' ' + str(z)\n",
    "    else:\n",
    "        com = \"Geneva\" + ' ' +y + ' ' + str(z)\n",
    "        \n",
    "    return com\n",
    "\n",
    "result_non_trade['predicted comment'] = result_non_trade.apply(lambda x : comgen(x['new_pb2'],x['predicted template'],x['ViewData.Settle Date'],x['new_pb1']), axis = 1)\n",
    "result_non_trade['predicted status'] = 'comment'\n",
    "result_non_trade['predicted action'] = 'OB'\n",
    "result_non_trade = result_non_trade[['ViewData.Side0_UniqueIds','ViewData.Side0_UniqueIds','ViewData.BreakID','ViewData.Status','predicted status','predicted action','predicted category','predicted comment']]\n",
    "result_non_trade.to_csv('Lombard/249/Comment file for lombard 249.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
